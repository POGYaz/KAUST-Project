{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee120523",
   "metadata": {},
   "source": [
    "# 02: Data Embeddings & Baseline Models\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- **Configuration & Imports**: define data paths and evaluation settings\n",
    "- **Load Processed Data**: sequences, maps, and interaction tables\n",
    "- **Data Inspection**: peek at samples\n",
    "- **Create Interaction Matrices**:\n",
    "  - Build sparse user–item matrices from sequences\n",
    "  - Generate train/test splits for evaluation\n",
    "- **Define Baseline Models**:\n",
    "  - Popularity\n",
    "  - Item-based KNN\n",
    "  - User-based KNN\n",
    "  - Matrix Factorization (NMF)\n",
    "- **Evaluation Metrics**: Recall@K\n",
    "- **Train & Evaluate** each baseline\n",
    "- **Summarize & Save** results to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa8c9b-1815-4995-9cbe-b928798a1d3f",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports\n",
    "\n",
    "- Set file paths for processed data\n",
    "- Define evaluation parameters (K values, sample sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded760d1-c2c8-4443-b1c3-53f42b98ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global reproducibility\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths to processed data\n",
    "out_dir = Path('../data/processed/jarir/')\n",
    "seq_train_path = out_dir / 'sequences_train.parquet'\n",
    "seq_val_path   = out_dir / 'sequences_val.parquet'\n",
    "seq_test_path  = out_dir / 'sequences_test.parquet'\n",
    "item_map_path  = out_dir / 'item_id_map.parquet'\n",
    "cust_map_path  = out_dir / 'customer_id_map.parquet'\n",
    "\n",
    "# Evaluation config\n",
    "K_VALUES = [5, 10, 20]\n",
    "EVAL_SAMPLE_SIZE = 1000  # sample users for faster evaluation (None = all)\n",
    "\n",
    "# Placeholder for baseline results\n",
    "baseline_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862bc289-038b-419c-aa95-9840483ca49d",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data\n",
    "\n",
    "- Read train/val/test sequence tables\n",
    "- Read user & item mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe0a0d0-011f-4ce0-992e-25c8ccf687c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences and maps...\n",
      "Train seq: 1108 rows\n",
      "Val seq:   169 rows\n",
      "Test seq:  160 rows\n",
      "Items:     1735\n",
      "Users:     929\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading sequences and maps...\")\n",
    "seq_train = pd.read_parquet(seq_train_path, engine='fastparquet')\n",
    "seq_val   = pd.read_parquet(seq_val_path,   engine='fastparquet')\n",
    "seq_test  = pd.read_parquet(seq_test_path,  engine='fastparquet')\n",
    "item_map  = pd.read_parquet(item_map_path, engine='fastparquet')\n",
    "cust_map  = pd.read_parquet(cust_map_path, engine='fastparquet')\n",
    "print(f\"Train seq: {len(seq_train)} rows\")\n",
    "print(f\"Val seq:   {len(seq_val)} rows\")\n",
    "print(f\"Test seq:  {len(seq_test)} rows\")\n",
    "print(f\"Items:     {len(item_map)}\")\n",
    "print(f\"Users:     {len(cust_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf62513-fcd2-4f21-b522-891523675762",
   "metadata": {},
   "source": [
    "## 3. Data Inspection\n",
    "\n",
    "- View a few example rows from sequences, item_map, and cust_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47526814-146d-4ed2-a20f-d2f6c8b2f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence row:\n",
      "   customer_id  user_idx         ts history_idx  pos_item_idx     country\n",
      "0     10018322         5 2024-03-07        9 10            11  0103-PLAZA\n",
      "1     10018322         5 2024-03-24     9 10 11            12  0103-PLAZA\n",
      "2     10018322         5 2024-04-30  9 10 11 12            13  0103-PLAZA\n",
      "\n",
      "Sample item map:\n",
      "      stock_code  item_idx\n",
      "0      RQ-CHB002         0\n",
      "1  ZQ-F27318BGLD         1\n",
      "2     NE-0230059         2\n",
      "\n",
      "Sample customer map:\n",
      "   customer_id  user_idx\n",
      "0        11949         0\n",
      "1        24811         1\n",
      "2        33097         2\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample sequence row:\")\n",
    "print(seq_train.head(3))\n",
    "print(\"\\nSample item map:\")\n",
    "print(item_map.head(3))\n",
    "print(\"\\nSample customer map:\")\n",
    "print(cust_map.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e2650-9f04-4223-8266-733a1c918f26",
   "metadata": {},
   "source": [
    "## 4. Create Sparse Interaction Matrix\n",
    "\n",
    "- Build user–item matrix: history items weighted lower than positive event\n",
    "- Also prepare full interactions matrix for non-sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d099c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc771185-e637-41fb-9b04-d7f2e53ecab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix: (929, 1735), nz=1623\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def build_matrix_from_sequences(seq_df, n_users, n_items):\n",
    "    rows, cols, vals = [], [], []\n",
    "    for _, r in seq_df.iterrows():\n",
    "        u = int(r['user_idx'])\n",
    "        p = int(r['pos_item_idx'])\n",
    "        # positive event\n",
    "        rows.append(u); cols.append(p); vals.append(1.0)\n",
    "        # history events\n",
    "        if pd.notna(r['history_idx']) and r['history_idx']:\n",
    "            h = [int(x) for x in r['history_idx'].split()]\n",
    "            for item in h:\n",
    "                rows.append(u); cols.append(item); vals.append(0.5)\n",
    "    return csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "n_users = len(cust_map)\n",
    "n_items = len(item_map)\n",
    "train_mat = build_matrix_from_sequences(seq_train, n_users, n_items)\n",
    "full_mat  = build_matrix_from_sequences(pd.concat([seq_train, seq_val, seq_test]), n_users, n_items)\n",
    "print(f\"Train matrix: {train_mat.shape}, nz={train_mat.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ecf054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN source matrix (from interactions): (929, 1735), nz=2230\n"
     ]
    }
   ],
   "source": [
    "# Build a denser KNN source matrix from full interactions (cut at train cutoff) to help KNN\n",
    "from pathlib import Path\n",
    "try:\n",
    "    interactions_path = out_dir / 'interactions_clean.parquet'\n",
    "    if interactions_path.exists():\n",
    "        interactions = pd.read_parquet(interactions_path, engine='fastparquet')\n",
    "        interactions = interactions.merge(item_map, on='stock_code', how='inner')\n",
    "        interactions = interactions.merge(cust_map, on='customer_id', how='inner')\n",
    "        cutoff = pd.to_datetime(seq_train['ts'].max()) if 'ts' in seq_train.columns else None\n",
    "        if cutoff is not None and 'invoice_date' in interactions.columns:\n",
    "            interactions = interactions[interactions['invoice_date'] <= cutoff]\n",
    "        rows = interactions['user_idx'].astype(int).to_numpy()\n",
    "        cols = interactions['item_idx'].astype(int).to_numpy()\n",
    "        vals = np.ones_like(rows, dtype='float32')\n",
    "        knn_mat = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "        print(f\"KNN source matrix (from interactions): {knn_mat.shape}, nz={knn_mat.nnz}\")\n",
    "    else:\n",
    "        knn_mat = train_mat\n",
    "        print(\"KNN source matrix fallback to train_mat\")\n",
    "except Exception as e:\n",
    "    knn_mat = train_mat\n",
    "    print(\"KNN source matrix build failed, fallback to train_mat:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34104978-121a-4619-8cea-909463aae8f4",
   "metadata": {},
   "source": [
    "## 5. Prepare Train/Test Split\n",
    "\n",
    "- Remove validation interactions from training matrix for proper held-out evaluation\n",
    "- Collect test interactions list [(user, item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b5e3a0-d20a-474d-b7c1-a8096737a891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 169 held-out test cases\n"
     ]
    }
   ],
   "source": [
    "test_interactions = []\n",
    "for _, r in seq_val.iterrows():\n",
    "    u, p = int(r['user_idx']), int(r['pos_item_idx'])\n",
    "    test_interactions.append((u, p))\n",
    "    train_mat[u, p] = 0\n",
    "print(f\"Prepared {len(test_interactions)} held-out test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a0fd8-93c7-4175-be54-14cae239bc43",
   "metadata": {},
   "source": [
    "## 6. Define Baseline Models\n",
    "\n",
    "- **Popularity**: rank by total interactions\n",
    "- **ItemKNN**: cosine similarity on item columns\n",
    "- **UserKNN**: cosine similarity on user rows\n",
    "- **Matrix Factorization**: NMF on dense matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b45d35-d287-460f-b5f0-bb8f9bf7cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Popularity:\n",
    "    def fit(self, mat):\n",
    "        self.pop = np.array(mat.sum(axis=0)).flatten()\n",
    "        return self\n",
    "    def recommend(self, u, k=10, allow_item=None):\n",
    "        seen = mat[u].nonzero()[1]\n",
    "        if allow_item is not None:\n",
    "            seen = np.setdiff1d(seen, np.array([allow_item]))\n",
    "        scores = self.pop.copy()\n",
    "        scores[seen] = -1e12\n",
    "        return np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "class ItemKNN:\n",
    "    def __init__(self, k=50, pop_alpha=0.01):\n",
    "        self.k = k\n",
    "        self.pop_alpha = pop_alpha\n",
    "    def fit(self, mat):\n",
    "        # Prefer a denser matrix from interactions if available (pre-cutoff) to improve co-occurrence\n",
    "        base = globals().get('knn_mat', mat)\n",
    "        self.base = base\n",
    "        # Item-item cosine similarity\n",
    "        self.sim = cosine_similarity(base.T)\n",
    "        # Popularity fallback\n",
    "        self.pop = np.array(base.sum(axis=0)).flatten().astype(np.float32)\n",
    "        if self.pop.max() > 0:\n",
    "            self.pop /= self.pop.max()\n",
    "        return self\n",
    "    def recommend(self, u, k=10, allow_item=None):\n",
    "        # User profile in the same base used for similarity\n",
    "        user_vec = self.base[u].toarray().flatten()\n",
    "        scores = user_vec @ self.sim\n",
    "        # Add a small popularity prior to break ties in sparse regimes\n",
    "        scores = scores + self.pop_alpha * self.pop\n",
    "        seen = np.where(user_vec > 0)[0]\n",
    "        if allow_item is not None:\n",
    "            seen = np.setdiff1d(seen, np.array([allow_item]))\n",
    "        scores[seen] = -1e12\n",
    "        return np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "class UserKNN:\n",
    "    def __init__(self, k=50):\n",
    "        self.k = k\n",
    "    def fit(self, mat):\n",
    "        self.sim = cosine_similarity(mat)\n",
    "        return self\n",
    "    def recommend(self, u, k=10, allow_item=None):\n",
    "        # Top similar users excluding self\n",
    "        sim_u = self.sim[u].copy()\n",
    "        sim_u[u] = -1e12\n",
    "        top_users = np.argsort(sim_u)[-self.k:][::-1]\n",
    "        scores = np.zeros(mat.shape[1])\n",
    "        for v in top_users:\n",
    "            scores += mat[v].toarray().flatten() * max(sim_u[v], 0)\n",
    "        seen = mat[u].nonzero()[1]\n",
    "        if allow_item is not None:\n",
    "            seen = np.setdiff1d(seen, np.array([allow_item]))\n",
    "        scores[seen] = -1e12\n",
    "        return np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "class MFBaseline:\n",
    "    def __init__(self, n_f=50):\n",
    "        self.n_f = n_f\n",
    "    def fit(self, mat):\n",
    "        dense = mat.toarray()\n",
    "        self.model = NMF(n_components=self.n_f, random_state=42)\n",
    "        self.W = self.model.fit_transform(dense)\n",
    "        self.H = self.model.components_\n",
    "        return self\n",
    "    def recommend(self, u, k=10, allow_item=None):\n",
    "        scores = self.W[u].dot(self.H)\n",
    "        seen = train_mat[u].nonzero()[1]\n",
    "        if allow_item is not None:\n",
    "            seen = np.setdiff1d(seen, np.array([allow_item]))\n",
    "        scores[seen] = -1e12\n",
    "        return np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "# bind mat for popularity\n",
    "mat = train_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07e87a-b683-4cf8-802b-2665027f3d5f",
   "metadata": {},
   "source": [
    "## 7. Evaluation Function (Recall@K)\n",
    "\n",
    "- Compute Recall@K over held-out test_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379621e1-d569-4a2a-bfdf-d86649869fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(recs, true_item):\n",
    "    return int(true_item in recs)\n",
    "\n",
    "def ndcg_at_k(recs, true_item):\n",
    "    for rank, item in enumerate(recs, start=1):\n",
    "        if item == true_item:\n",
    "            return 1.0 / np.log2(rank + 1)\n",
    "    return 0.0\n",
    "\n",
    "def mrr_at_k(recs, true_item):\n",
    "    for rank, item in enumerate(recs, start=1):\n",
    "        if item == true_item:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def evaluate(model, k=10):\n",
    "    recalls, ndcgs, mrrs = [], [], []\n",
    "    for u, true in test_interactions:\n",
    "        # allow masking all seen items EXCEPT the held-out positive to avoid leakage\n",
    "        recs = model.recommend(u, k, allow_item=true)\n",
    "        recalls.append(recall_at_k(recs, true))\n",
    "        ndcgs.append(ndcg_at_k(recs, true))\n",
    "        mrrs.append(mrr_at_k(recs, true))\n",
    "    return {\n",
    "        'Recall': float(np.mean(recalls)),\n",
    "        'NDCG': float(np.mean(ndcgs)),\n",
    "        'MRR': float(np.mean(mrrs)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7cf03-c2cb-46ee-9ea5-22456b80c833",
   "metadata": {},
   "source": [
    "## 8. Train & Evaluate Baselines\n",
    "\n",
    "- Fit each model on `train_mat`\n",
    "- Evaluate Recall@K for K in `K_VALUES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc777afe-5593-42b4-8911-c7667ff18115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results:\n",
      "  Popularity K=5: Recall=0.0651, NDCG=0.0391, MRR=0.0303\n",
      "  Popularity K=10: Recall=0.0947, NDCG=0.0488, MRR=0.0344\n",
      "  Popularity K=20: Recall=0.1243, NDCG=0.0568, MRR=0.0369\n",
      "  ItemKNN K=5: Recall=0.0947, NDCG=0.0947, MRR=0.0947\n",
      "  ItemKNN K=10: Recall=0.1124, NDCG=0.1003, MRR=0.0969\n",
      "  ItemKNN K=20: Recall=0.1479, NDCG=0.1095, MRR=0.0996\n",
      "  UserKNN K=5: Recall=0.0059, NDCG=0.0059, MRR=0.0059\n",
      "  UserKNN K=10: Recall=0.0118, NDCG=0.0079, MRR=0.0068\n",
      "  UserKNN K=20: Recall=0.0178, NDCG=0.0093, MRR=0.0071\n",
      "  MatrixFactorization K=5: Recall=0.0000, NDCG=0.0000, MRR=0.0000\n",
      "  MatrixFactorization K=10: Recall=0.0059, NDCG=0.0017, MRR=0.0006\n",
      "  MatrixFactorization K=20: Recall=0.0059, NDCG=0.0017, MRR=0.0006\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Popularity\n",
    "pop = Popularity().fit(train_mat)\n",
    "results['Popularity'] = {f'K={k}': evaluate(pop, k) for k in K_VALUES}\n",
    "\n",
    "# ItemKNN\n",
    "itemknn = ItemKNN(k=50).fit(train_mat)\n",
    "results['ItemKNN'] = {f'K={k}': evaluate(itemknn, k) for k in K_VALUES}\n",
    "\n",
    "# UserKNN\n",
    "userknn = UserKNN(k=50).fit(train_mat)\n",
    "results['UserKNN'] = {f'K={k}': evaluate(userknn, k) for k in K_VALUES}\n",
    "\n",
    "# Matrix Factorization\n",
    "mf = MFBaseline(n_f=50).fit(train_mat)\n",
    "results['MatrixFactorization'] = {f'K={k}': evaluate(mf, k) for k in K_VALUES}\n",
    "\n",
    "print(\"Baseline Results:\")\n",
    "for model_name, metrics_by_k in results.items():\n",
    "    for k, m in metrics_by_k.items():\n",
    "        print(f\"  {model_name} {k}: Recall={m['Recall']:.4f}, NDCG={m['NDCG']:.4f}, MRR={m['MRR']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e407f-bf6f-40f5-801d-dddc8f8e85cf",
   "metadata": {},
   "source": [
    "## 9. Summarize & Save Results\n",
    "\n",
    "- Choose best model at highest K\n",
    "- Write `baseline_results.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc329a3-d55f-4318-9f01-01ba23d92686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline results, best=ItemKNN Recall@20=0.1479\n"
     ]
    }
   ],
   "source": [
    "# Persist detailed metrics; choose best by Recall@K (highest K)\n",
    "final_k = K_VALUES[-1]\n",
    "get_recall = lambda m: m[f'K={final_k}']['Recall']\n",
    "best = max(results.items(), key=lambda x: get_recall(x[1]))\n",
    "\n",
    "baseline_summary = {\n",
    "    'held_out_interactions': results,\n",
    "    'best_model': best[0],\n",
    "    'best_recall': get_recall(best[1]),\n",
    "    'k': final_k\n",
    "}\n",
    "with open(out_dir / 'baseline_results.json', 'w') as f:\n",
    "    json.dump(baseline_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved baseline results, best={best[0]} Recall@{final_k}={get_recall(best[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f41853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN source matrix (from interactions): (929, 1735), nz=2230\n"
     ]
    }
   ],
   "source": [
    "# Optional: build denser KNN matrix from full interactions to improve KNN baselines\n",
    "# Uses interactions before validation cutoff to avoid leakage\n",
    "try:\n",
    "    interactions_path = out_dir / 'interactions_clean.parquet'\n",
    "    if interactions_path.exists():\n",
    "        interactions = pd.read_parquet(interactions_path, engine='fastparquet')\n",
    "        # Map to indices\n",
    "        interactions = interactions.merge(item_map, on='stock_code', how='inner')\n",
    "        interactions = interactions.merge(cust_map, on='customer_id', how='inner')\n",
    "        # Determine cutoff from train sequences\n",
    "        cutoff = pd.to_datetime(seq_train['ts'].max()) if 'ts' in seq_train.columns else None\n",
    "        if cutoff is not None and 'invoice_date' in interactions.columns:\n",
    "            interactions = interactions[interactions['invoice_date'] <= cutoff]\n",
    "        # Build binary interactions matrix\n",
    "        rows = interactions['user_idx'].astype(int).to_numpy()\n",
    "        cols = interactions['item_idx'].astype(int).to_numpy()\n",
    "        vals = np.ones_like(rows, dtype='float32')\n",
    "        knn_mat = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "        print(f\"KNN source matrix (from interactions): {knn_mat.shape}, nz={knn_mat.nnz}\")\n",
    "    else:\n",
    "        knn_mat = train_mat\n",
    "        print(\"KNN source matrix fallback to train_mat\")\n",
    "except Exception as e:\n",
    "    knn_mat = train_mat\n",
    "    print(\"KNN source matrix build failed, fallback to train_mat:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363860f-512c-4c61-b28d-ec56a3893938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
