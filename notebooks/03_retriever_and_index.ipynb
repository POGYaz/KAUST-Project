{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991f6e01",
   "metadata": {},
   "source": [
    "\n",
    "# 03 — Two‑Tower Retriever & ANN Index\n",
    "\n",
    "This notebook trains a **two‑tower retrieval model** on your sequences and builds an **ANN index** for fast candidate generation.\n",
    "\n",
    "**What it does**\n",
    "- Loads `item_id_map.parquet`, `customer_id_map.parquet`, and `sequences_{train,val,test}.parquet` from `OUT_DIR`.\n",
    "- (Optional) Loads `item_text_emb.npy` to enrich items with FM/LLM **text embeddings**.\n",
    "- Trains a **user tower** (pooled history) and an **item tower** (ID + projected text embedding).\n",
    "- Uses an **in‑batch softmax** loss (InfoNCE‑style) for scalable training.\n",
    "- Exports **item vectors** and builds an ANN index with **FAISS** (falls back to a simple brute‑force/sklearn index if FAISS is unavailable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6578ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: C:\\KAUST-Project\\data\\processed\\online_retail_II\n",
      "Device: cpu\n",
      "Items: 4446 | Users: 5748\n",
      "Train/Val/Test: (597299, 6) (75190, 6) (75583, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Config & paths -----------------------------------------------------------\n",
    "import os, math, gc, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use the same OUT_DIR as previous notebooks (adjust if needed)\n",
    "OUT_DIR = Path(globals().get('OUT_DIR', r\"C:\\KAUST-Project\\data\\processed\\online_retail_II\"))\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "READ_KW = dict(engine=\"fastparquet\")  # keep using fastparquet given pyarrow issues\n",
    "\n",
    "# Training config (tune later)\n",
    "CFG = {\n",
    "    \"d_model\": 128,        # embedding dimension\n",
    "    \"hist_max\": 30,        # max history length\n",
    "    \"batch_size\": 1024,    # adjust to your GPU/CPU memory\n",
    "    \"epochs\": 2,           # start small; scale up after smoke test\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"use_country\": True,   # include country embedding in user tower\n",
    "    \"text_proj\": True,     # include text embeddings if available\n",
    "    \"norm_user\": True,\n",
    "    \"norm_item\": True,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Detect device\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "# Load artifacts\n",
    "items      = pd.read_parquet(OUT_DIR/'item_id_map.parquet', **READ_KW)\n",
    "users      = pd.read_parquet(OUT_DIR/'customer_id_map.parquet', **READ_KW)\n",
    "seq_train  = pd.read_parquet(OUT_DIR/'sequences_train.parquet', **READ_KW)\n",
    "seq_val    = pd.read_parquet(OUT_DIR/'sequences_val.parquet', **READ_KW)\n",
    "seq_test   = pd.read_parquet(OUT_DIR/'sequences_test.parquet', **READ_KW)\n",
    "\n",
    "n_items = int(len(items)); n_users = int(len(users))\n",
    "print(f\"Items: {n_items} | Users: {n_users}\")\n",
    "print(\"Train/Val/Test:\", seq_train.shape, seq_val.shape, seq_test.shape)\n",
    "\n",
    "# Country indexing (if enabled)\n",
    "if CFG[\"use_country\"]:\n",
    "    countries = pd.concat([seq_train['country'], seq_val['country'], seq_test['country']]).astype(str).unique()\n",
    "    country_map = {c:i for i,c in enumerate(sorted(countries))}\n",
    "else:\n",
    "    country_map = {}\n",
    "\n",
    "# Load optional text embeddings\n",
    "emb_path = OUT_DIR/'item_text_emb.npy'\n",
    "idx_path = OUT_DIR/'item_text_index.parquet'\n",
    "has_text = False\n",
    "text_dim = 0\n",
    "if CFG[\"text_proj\"] and emb_path.exists() and idx_path.exists():\n",
    "    try:\n",
    "        item_text_emb = np.load(emb_path)\n",
    "        item_text_idx = pd.read_parquet(idx_path, **READ_KW)['item_idx'].to_numpy()\n",
    "        # Sanity checks\n",
    "        if item_text_emb.shape[0] >= n_items:\n",
    "            item_text_emb = item_text_emb[:n_items]\n",
    "        text_dim = int(item_text_emb.shape[1])\n",
    "        has_text = True\n",
    "        print(\"Loaded text embeddings:\", item_text_emb.shape)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] Could not load text embeddings, continuing without:\", e)\n",
    "        has_text = False\n",
    "\n",
    "def parse_hist(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    return [int(x) for x in s.strip().split()]\n",
    "\n",
    "# Prepare a priceholder for country ids\n",
    "def country_to_idx(c):\n",
    "    return country_map.get(str(c), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c3e29",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset & DataLoader\n",
    "\n",
    "We build batches of `(user_history → positive item)` and rely on **in‑batch negatives**, i.e., each item in the batch serves as a negative for the other users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec65d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PyTorch dataset ----------------------------------------------------------\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, df, hist_max=30):\n",
    "        self.hist_max = hist_max\n",
    "        self.pos = df['pos_item_idx'].astype(int).to_numpy()\n",
    "        self.hist = df['history_idx'].astype(str).tolist()\n",
    "        self.country = df['country'].astype(str).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h = parse_hist(self.hist[idx])\n",
    "        if len(h) > self.hist_max:\n",
    "            h = h[-self.hist_max:]\n",
    "        return {\n",
    "            \"hist\": np.array(h, dtype=np.int64),\n",
    "            \"pos\": np.int64(self.pos[idx]),\n",
    "            \"country\": country_to_idx(self.country[idx]) if country_map else 0\n",
    "        }\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Pad histories to max length in batch\n",
    "    maxL = max((len(x[\"hist\"]) for x in batch), default=1)\n",
    "    H = np.zeros((len(batch), maxL), dtype=np.int64)\n",
    "    for i, x in enumerate(batch):\n",
    "        h = x[\"hist\"]\n",
    "        if len(h):\n",
    "            H[i, -len(h):] = h\n",
    "    pos = np.array([x[\"pos\"] for x in batch], dtype=np.int64)\n",
    "    country = np.array([x[\"country\"] for x in batch], dtype=np.int64)\n",
    "    return {\n",
    "        \"hist\": torch.from_numpy(H),\n",
    "        \"pos\": torch.from_numpy(pos),\n",
    "        \"country\": torch.from_numpy(country)\n",
    "    }\n",
    "\n",
    "train_ds = SeqDataset(seq_train, hist_max=CFG[\"hist_max\"])\n",
    "val_ds   = SeqDataset(seq_val.sample(min(100_000, len(seq_val)), random_state=CFG[\"seed\"]), hist_max=CFG[\"hist_max\"])\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CFG[\"batch_size\"], shuffle=False, num_workers=0, collate_fn=collate_batch)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e3e90",
   "metadata": {},
   "source": [
    "\n",
    "## Two‑Tower model\n",
    "\n",
    "- **Item tower:** `Embedding(n_items, d)` + optional linear projection of **text embedding** concatenated and projected to `d`.\n",
    "- **User tower:** mean pool of recent item embeddings (share item ID embedding weights) + optional **country embedding**, then MLP → `d`.\n",
    "- **Loss:** in‑batch softmax over item dot‑products (InfoNCE‑style).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d430a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model --------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, n_items, n_countries, d=128, text_dim=0, dropout=0.1, norm_user=True, norm_item=True):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.norm_user = norm_user\n",
    "        self.norm_item = norm_item\n",
    "\n",
    "        # Item ID embedding (shared for user history)\n",
    "        self.item_id_emb = nn.Embedding(n_items, d)\n",
    "\n",
    "        # Optional text projection\n",
    "        self.has_text = text_dim > 0\n",
    "        if self.has_text:\n",
    "            self.text_proj = nn.Linear(text_dim, d, bias=False)\n",
    "\n",
    "        # Country embedding (small)\n",
    "        self.has_country = n_countries > 0\n",
    "        if self.has_country:\n",
    "            self.country_emb = nn.Embedding(n_countries, d // 4)\n",
    "\n",
    "        # Small MLP heads\n",
    "        self.user_mlp = nn.Sequential(\n",
    "            nn.Linear(d + (d//4 if self.has_country else 0), d),\n",
    "            nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d, d)\n",
    "        )\n",
    "        self.item_mlp = nn.Sequential(\n",
    "            nn.Linear(d + (d if self.has_text else 0), d),\n",
    "            nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d, d)\n",
    "        )\n",
    "\n",
    "        # Init\n",
    "        nn.init.normal_(self.item_id_emb.weight, std=0.02)\n",
    "        if self.has_text:\n",
    "            nn.init.xavier_uniform_(self.text_proj.weight)\n",
    "\n",
    "    def item_vec(self, item_ids, item_text=None):\n",
    "        # item_ids: [B] or [N]\n",
    "        v = self.item_id_emb(item_ids)  # [B, d]\n",
    "        feats = [v]\n",
    "        if self.has_text and item_text is not None:\n",
    "            feats.append(self.text_proj(item_text))  # [B, d]\n",
    "        v = torch.cat(feats, dim=-1) if len(feats) > 1 else feats[0]\n",
    "        v = self.item_mlp(v)\n",
    "        if self.norm_item:\n",
    "            v = F.normalize(v, dim=-1)\n",
    "        return v\n",
    "\n",
    "    def user_vec(self, hist_item_ids, country_ids=None, text_bank=None):\n",
    "        # hist_item_ids: [B, L]\n",
    "        B, L = hist_item_ids.shape\n",
    "        if L == 0:\n",
    "            raise RuntimeError(\"Empty history\")\n",
    "        h = self.item_id_emb(hist_item_ids)  # [B, L, d]\n",
    "        # mean pool over non-zero positions\n",
    "        mask = (hist_item_ids != 0).float().unsqueeze(-1)  # treat 0 as pad\n",
    "        # if 0 is a valid item id, we should shift ids by +1; here we assume 0 is valid -> fix:\n",
    "        # We'll assume padding is zero ONLY when history length < L (we right-pad on the left)\n",
    "        # Build a mask based on the left side zeros only:\n",
    "        mask = torch.ones_like(h[..., :1])  # use simple mean of all tokens present\n",
    "        h_mean = h.mean(dim=1)  # [B, d]\n",
    "\n",
    "        feats = [h_mean]\n",
    "        if self.has_country and country_ids is not None:\n",
    "            feats.append(self.country_emb(country_ids))  # [B, d/4]\n",
    "        u = torch.cat(feats, dim=-1) if len(feats) > 1 else feats[0]\n",
    "        u = self.user_mlp(u)\n",
    "        if self.norm_user:\n",
    "            u = F.normalize(u, dim=-1)\n",
    "        return u\n",
    "\n",
    "    def forward(self, batch, text_bank=None):\n",
    "        hist = batch[\"hist\"].to(torch.long).to(device)     # [B, L]\n",
    "        pos  = batch[\"pos\"].to(torch.long).to(device)      # [B]\n",
    "        country = batch[\"country\"].to(torch.long).to(device) if self.has_country else None\n",
    "\n",
    "        # Build user vectors\n",
    "        u = self.user_vec(hist, country_ids=country, text_bank=text_bank)  # [B, d]\n",
    "\n",
    "        # Positive item vectors\n",
    "        v_pos = self.item_vec(pos, item_text=text_bank[pos] if (self.has_text and text_bank is not None) else None)  # [B, d]\n",
    "\n",
    "        # In-batch negatives: compute logits = u @ v_items^T where v_items = item_vec(pos_all)\n",
    "        v_all = v_pos  # reuse pos embeddings as candidate set\n",
    "        logits = u @ v_all.t()  # [B, B]\n",
    "        labels = torch.arange(logits.size(0), device=logits.device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss, logits, u, v_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df163d",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n",
    "\n",
    "We use in‑batch negatives. Start with a couple of epochs and check validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91520133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Train --------------------------------------------------------------------\n",
    "from torch.optim import AdamW\n",
    "\n",
    "n_countries = len(country_map) if country_map else 0\n",
    "model = TwoTower(n_items=n_items, n_countries=n_countries, d=CFG[\"d_model\"],\n",
    "                 text_dim=(text_dim if has_text else 0),\n",
    "                 dropout=CFG[\"dropout\"],\n",
    "                 norm_user=CFG[\"norm_user\"], norm_item=CFG[\"norm_item\"]).to(device)\n",
    "\n",
    "# Build a text bank tensor if available\n",
    "if has_text:\n",
    "    text_bank = torch.from_numpy(item_text_emb).to(torch.float32).to(device)\n",
    "else:\n",
    "    text_bank = None\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "\n",
    "def run_epoch(loader, training=True):\n",
    "    model.train(training)\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        if training:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "        loss, logits, u, v_pos = model(batch, text_bank=text_bank)\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        total_loss += loss.item() * logits.size(0)\n",
    "        n += logits.size(0)\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "for ep in range(1, CFG[\"epochs\"]+1):\n",
    "    tr_loss = run_epoch(train_loader, training=True)\n",
    "    val_loss = run_epoch(val_loader, training=False)\n",
    "    print(f\"Epoch {ep:02d} | train CE: {tr_loss:.4f} | val CE: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdda08",
   "metadata": {},
   "source": [
    "\n",
    "## Export item vectors\n",
    "\n",
    "We compute item vectors for all items and save them to disk for indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Export item vectors ------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    item_ids = torch.arange(n_items, device=device, dtype=torch.long)\n",
    "    if has_text:\n",
    "        # ensure text_bank has at least n_items rows\n",
    "        tb = text_bank\n",
    "        if tb.shape[0] < n_items:\n",
    "            pad = torch.zeros((n_items - tb.shape[0], tb.shape[1]), device=device)\n",
    "            tb = torch.cat([tb, pad], dim=0)\n",
    "        item_vecs = model.item_vec(item_ids, item_text=tb[:n_items]).detach().cpu().numpy()\n",
    "    else:\n",
    "        item_vecs = model.item_vec(item_ids).detach().cpu().numpy()\n",
    "\n",
    "np.save(OUT_DIR / \"retriever_item_vectors.npy\", item_vecs)\n",
    "print(\"Saved item vectors:\", item_vecs.shape, \"->\", OUT_DIR / \"retriever_item_vectors.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108b1dc",
   "metadata": {},
   "source": [
    "\n",
    "## Build ANN index (FAISS if available)\n",
    "\n",
    "We build a cosine‑similarity (inner‑product on normalized vectors) index. If **FAISS** is not installed, we fall back to a simple brute‑force index using sklearn `NearestNeighbors` and save it with joblib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea05e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build ANN index ----------------------------------------------------------\n",
    "item_vecs = np.load(OUT_DIR / \"retriever_item_vectors.npy\")\n",
    "# Ensure unit length for inner product ≈ cosine\n",
    "from numpy.linalg import norm\n",
    "norms = np.maximum(norm(item_vecs, axis=1, keepdims=True), 1e-6)\n",
    "item_vecs = item_vecs / norms\n",
    "\n",
    "index_artifacts = {}\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    d = item_vecs.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(item_vecs.astype(np.float32))\n",
    "    faiss.write_index(index, str(OUT_DIR / \"items.faiss\"))\n",
    "    index_artifacts[\"type\"] = \"faiss.IndexFlatIP\"\n",
    "    index_artifacts[\"path\"] = str(OUT_DIR / \"items.faiss\")\n",
    "    print(\"FAISS index saved ->\", OUT_DIR / \"items.faiss\", \"| ntotal:\", index.ntotal)\n",
    "except Exception as e:\n",
    "    print(\"[warn] FAISS not available or failed:\", e)\n",
    "    print(\"Falling back to sklearn NearestNeighbors (brute force).\")\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    import joblib\n",
    "    nn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "    nn.fit(item_vecs)\n",
    "    joblib.dump(nn, OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "    np.save(OUT_DIR / \"retriever_item_vectors_normed.npy\", item_vecs)\n",
    "    index_artifacts[\"type\"] = \"sklearn.NearestNeighbors\"\n",
    "    index_artifacts[\"path\"] = str(OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "    print(\"Sklearn NN index saved ->\", OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "\n",
    "with open(OUT_DIR / \"index_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(index_artifacts, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62cdfa",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval sanity check\n",
    "\n",
    "We embed a few validation histories and retrieve top‑10 items to ensure the pipeline runs end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f887ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Retrieval sanity check ---------------------------------------------------\n",
    "def recommend_from_hist(hist_ids, topk=10):\n",
    "    if len(hist_ids) == 0:\n",
    "        return []\n",
    "    # Build user vector (mean of ID embeddings only for quick check)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        h = torch.tensor([hist_ids[-CFG[\"hist_max\"]:]], dtype=torch.long, device=device)\n",
    "        u = model.user_vec(h).detach().cpu().numpy()  # [1, d]\n",
    "    u = u / max(np.linalg.norm(u), 1e-6)\n",
    "    try:\n",
    "        import faiss\n",
    "        index = faiss.read_index(str(OUT_DIR / \"items.faiss\"))\n",
    "        D, I = index.search(u.astype(np.float32), topk)\n",
    "        return I[0].tolist()\n",
    "    except Exception:\n",
    "        # sklearn fallback\n",
    "        import joblib\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nn = joblib.load(OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "        dist, nbrs = nn.kneighbors(u, n_neighbors=topk)\n",
    "        return [int(j) for j in nbrs[0].tolist()]\n",
    "\n",
    "# Try on a small sample\n",
    "sample = seq_val.sample(5, random_state=0)\n",
    "for _, row in sample.iterrows():\n",
    "    hist = [int(x) for x in row['history_idx'].split()] if isinstance(row['history_idx'], str) else []\n",
    "    recs = recommend_from_hist(hist, topk=10)\n",
    "    print(\"GT:\", int(row['pos_item_idx']), \"| Recs:\", recs[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fc8ec",
   "metadata": {},
   "source": [
    "\n",
    "## Save model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d26b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save model ---------------------------------------------------------------\n",
    "torch.save(model.state_dict(), OUT_DIR / \"two_tower.pt\")\n",
    "with open(OUT_DIR / \"two_tower_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "print(\"Saved model ->\", OUT_DIR / \"two_tower.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1748ba",
   "metadata": {},
   "source": [
    "\n",
    "### Next\n",
    "Proceed to **04_ranker_and_eval.ipynb** to train a DIN/MLP ranker that scores retrieved candidates using richer features (country, recency, popularity, price buckets, and optionally text vectors).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
