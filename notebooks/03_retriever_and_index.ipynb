{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991f6e01",
   "metadata": {},
   "source": [
    "\n",
    "# 03 — Two‑Tower Retriever & ANN Index\n",
    "\n",
    "This notebook trains a **two‑tower retrieval model** on your sequences and builds an **ANN index** for fast candidate generation.\n",
    "\n",
    "**What it does**\n",
    "- Loads `item_id_map.parquet`, `customer_id_map.parquet`, and `sequences_{train,val,test}.parquet` from `OUT_DIR`.\n",
    "- (Optional) Loads `item_text_emb.npy` to enrich items with FM/LLM **text embeddings**.\n",
    "- Trains a **user tower** (pooled history) and an **item tower** (ID + projected text embedding).\n",
    "- Uses an **in‑batch softmax** loss (InfoNCE‑style) for scalable training.\n",
    "- Exports **item vectors** and builds an ANN index with **FAISS** (falls back to a simple brute‑force/sklearn index if FAISS is unavailable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6578ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Items: 4446 | Users: 5748\n",
      "Train/Val/Test: (592183, 6) (74819, 6) (75377, 6)\n",
      "Loaded text embeddings: (4446, 384)\n",
      "OUT_DIR: /home/kamalyy/KAUST-Project/data/processed/online_retail_II\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0) Config & paths --------------------------------------------------------\n",
    "import os, math, gc, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set your processed data path\n",
    "PROJECT_ROOT = Path.home() / \"KAUST-Project\"   # /home/kamalyy/KAUST-Project\n",
    "OUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"online_retail_II\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "READ_KW = dict(engine=\"fastparquet\")\n",
    "\n",
    "CFG = {\n",
    "    \"d_model\": 512,          # embedding dim (reduced for interactions)\n",
    "    \"hist_max\": 10,           # not used for interactions\n",
    "    \"batch_size\": 2048,      # larger batches for interactions\n",
    "    \"accum_steps\": 1,        # no need for accumulation with larger batches\n",
    "    \"epochs\": 30,            # interactions train faster\n",
    "    \"patience\": 8,           # slightly higher patience\n",
    "    \"lr\": 1e-3,              # higher LR for interactions\n",
    "    \"weight_decay\": 0.0,    # lower weight decay\n",
    "    \"dropout\": 0.2,          # lower dropout\n",
    "    \"use_country\": True,    # not needed for interactions\n",
    "    \"text_proj\": True,      # simplify for interactions\n",
    "    \"eval_topk\": 10,         # Recall@10\n",
    "    \"eval_sample\": None,     # sample for faster evaluation\n",
    "    \"seed\": 42,\n",
    "    \"k_neg\": 20,             # fewer negatives for interactions\n",
    "    \"fixed_logit_scale\": 10.0 # lower temperature\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "# Load artifacts from 01/02\n",
    "items      = pd.read_parquet(OUT_DIR/'item_id_map.parquet', **READ_KW)\n",
    "users      = pd.read_parquet(OUT_DIR/'customer_id_map.parquet', **READ_KW)\n",
    "seq_train  = pd.read_parquet(OUT_DIR/'sequences_train.parquet', **READ_KW)\n",
    "seq_val    = pd.read_parquet(OUT_DIR/'sequences_val.parquet', **READ_KW)\n",
    "seq_test   = pd.read_parquet(OUT_DIR/'sequences_test.parquet', **READ_KW)\n",
    "\n",
    "n_items = int(len(items)); n_users = int(len(users))\n",
    "print(f\"Items: {n_items} | Users: {n_users}\")\n",
    "print(\"Train/Val/Test:\", seq_train.shape, seq_val.shape, seq_test.shape)\n",
    "\n",
    "# Country indexing\n",
    "if CFG[\"use_country\"]:\n",
    "    countries = pd.concat([seq_train['country'], seq_val['country'], seq_test['country']]).astype(str).unique()\n",
    "    country_map = {c:i for i,c in enumerate(sorted(countries))}\n",
    "else:\n",
    "    country_map = {}\n",
    "\n",
    "# Optional text embeddings\n",
    "emb_path = OUT_DIR/'item_text_emb.npy'\n",
    "has_text = False\n",
    "text_dim = 0\n",
    "if CFG[\"text_proj\"] and emb_path.exists():\n",
    "    try:\n",
    "        item_text_emb = np.load(emb_path)\n",
    "        if item_text_emb.shape[0] >= n_items:\n",
    "            item_text_emb = item_text_emb[:n_items]\n",
    "        text_dim = int(item_text_emb.shape[1])\n",
    "        has_text = True\n",
    "        print(\"Loaded text embeddings:\", item_text_emb.shape)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] Text embeddings not loaded:\", e)\n",
    "        has_text = False\n",
    "\n",
    "def parse_hist(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    return [int(x) for x in s.strip().split()]\n",
    "\n",
    "def country_to_idx(c):\n",
    "    return country_map.get(str(c), 0)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "READ_KW  = dict(engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c3e29",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset & DataLoader\n",
    "\n",
    "We build batches of `(user_history → positive item)` and rely on **in‑batch negatives**, i.e., each item in the batch serves as a negative for the other users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec65d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, df, hist_max=30, n_items=0, k_neg=50):\n",
    "        self.hist_max = hist_max\n",
    "        self.pos = df['pos_item_idx'].astype(int).to_numpy()\n",
    "        self.hist = df['history_idx'].astype(str).tolist()\n",
    "        self.country = df['country'].astype(str).tolist()\n",
    "        self.n_items = n_items\n",
    "        self.k_neg = k_neg\n",
    "\n",
    "    def __len__(self): return len(self.pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h = parse_hist(self.hist[idx])\n",
    "        if len(h) > self.hist_max:\n",
    "            h = h[-self.hist_max:]\n",
    "        # +1 shift so 0=PAD\n",
    "        h = [x+1 for x in h]\n",
    "        pos = int(self.pos[idx]) + 1\n",
    "        cidx = country_to_idx(self.country[idx]) if country_map else 0\n",
    "\n",
    "        # Sample K negatives uniformly (1..n_items), avoiding pos; allow seen in history to keep it simple\n",
    "        negs = []\n",
    "        if self.k_neg > 0 and self.n_items > 1:\n",
    "            import random\n",
    "            for _ in range(self.k_neg):\n",
    "                r = random.randint(1, self.n_items)  # inclusive\n",
    "                while r == pos:\n",
    "                    r = random.randint(1, self.n_items)\n",
    "                negs.append(r)\n",
    "        return {\"hist\": np.array(h, dtype=np.int64),\n",
    "                \"pos\": np.int64(pos),\n",
    "                \"negs\": np.array(negs, dtype=np.int64) if negs else np.zeros((0,), dtype=np.int64),\n",
    "                \"country\": np.int64(cidx)}\n",
    "\n",
    "def collate_batch(batch):\n",
    "    maxL = max((len(x[\"hist\"]) for x in batch), default=1)\n",
    "    B = len(batch)\n",
    "    H = np.zeros((B, maxL), dtype=np.int64)  # 0 = PAD\n",
    "    for i, x in enumerate(batch):\n",
    "        h = x[\"hist\"]\n",
    "        if len(h):\n",
    "            H[i, -len(h):] = h\n",
    "    pos = np.array([x[\"pos\"] for x in batch], dtype=np.int64)\n",
    "    country = np.array([x[\"country\"] for x in batch], dtype=np.int64)\n",
    "    # Negatives as [B, K]\n",
    "    if len(batch[0][\"negs\"]) > 0:\n",
    "        K = len(batch[0][\"negs\"])\n",
    "        NE = np.stack([x[\"negs\"] for x in batch], axis=0)\n",
    "    else:\n",
    "        K = 0\n",
    "        NE = np.zeros((B, 0), dtype=np.int64)\n",
    "    return {\"hist\": torch.from_numpy(H),\n",
    "            \"pos\": torch.from_numpy(pos),\n",
    "            \"negs\": torch.from_numpy(NE),\n",
    "            \"country\": torch.from_numpy(country)}\n",
    "\n",
    "train_ds = SeqDataset(seq_train, hist_max=CFG[\"hist_max\"], n_items=n_items, k_neg=CFG[\"k_neg\"])\n",
    "val_src = seq_val if CFG[\"eval_sample\"] is None else seq_val.sample(min(CFG[\"eval_sample\"], len(seq_val)), random_state=CFG[\"seed\"])\n",
    "val_ds  = SeqDataset(val_src, hist_max=CFG[\"hist_max\"], n_items=n_items, k_neg=0)  # no negatives needed for eval\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,  num_workers=0, collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_ds,  batch_size=CFG[\"batch_size\"], shuffle=False, num_workers=0, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e3e90",
   "metadata": {},
   "source": [
    "\n",
    "## Two‑Tower model\n",
    "\n",
    "- **Item tower:** `Embedding(n_items, d)` + optional linear projection of **text embedding** concatenated and projected to `d`.\n",
    "- **User tower:** mean pool of recent item embeddings (share item ID embedding weights) + optional **country embedding**, then MLP → `d`.\n",
    "- **Loss:** in‑batch softmax over item dot‑products (InfoNCE‑style).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d430a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_out, d_out),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, n_items, n_countries, d=256, text_dim=0, dropout=0.2, norm_user=True, norm_item=True):\n",
    "        super().__init__()\n",
    "        self.norm_user = norm_user\n",
    "        self.norm_item = norm_item\n",
    "\n",
    "        # ID embeddings (+1 because 0 is PAD)\n",
    "        self.item_id_emb = nn.Embedding(n_items + 1, d, padding_idx=0)\n",
    "\n",
    "        # Optional projected text embedding concatenated to ID emb\n",
    "        self.has_text = text_dim > 0\n",
    "        if self.has_text:\n",
    "            self.text_proj = nn.Linear(text_dim, d, bias=False)\n",
    "\n",
    "        # Country embedding (small) as user context\n",
    "        self.has_country = n_countries > 0\n",
    "        if self.has_country:\n",
    "            self.country_emb = nn.Embedding(n_countries, d // 4)\n",
    "\n",
    "        # Towers as shallow MLPs (to mirror your Keras Dense towers)\n",
    "        self.user_tower = MLP(d + (d//4 if self.has_country else 0), d, dropout=dropout)\n",
    "        self.item_tower = MLP(d + (d if self.has_text else 0), d, dropout=dropout)\n",
    "\n",
    "        # Fixed/bounded temperature scale for stability\n",
    "        self.register_buffer(\"logit_scale\", torch.tensor(float(math.log(CFG[\"fixed_logit_scale\"]))))\n",
    "\n",
    "        nn.init.normal_(self.item_id_emb.weight, std=0.02)\n",
    "        if self.has_text:\n",
    "            nn.init.xavier_uniform_(self.text_proj.weight)\n",
    "\n",
    "    def user_vec(self, hist_item_ids, country_ids=None):\n",
    "        # hist_item_ids: [B, L], 0 = PAD\n",
    "        h = self.item_id_emb(hist_item_ids)            # [B,L,d]\n",
    "        mask = (hist_item_ids != 0).float().unsqueeze(-1)\n",
    "        h_sum = (h * mask).sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp_min(1.0)\n",
    "        h_mean = h_sum / lengths                       # [B,d]\n",
    "    \n",
    "        feats = [h_mean]\n",
    "        if self.has_country:\n",
    "            if country_ids is None:\n",
    "                # use zeros when country is not provided\n",
    "                cemb = torch.zeros(hist_item_ids.size(0),\n",
    "                                   self.country_emb.embedding_dim,\n",
    "                                   device=hist_item_ids.device, dtype=h_mean.dtype)\n",
    "            else:\n",
    "                cemb = self.country_emb(country_ids)\n",
    "            feats.append(cemb)\n",
    "    \n",
    "        u = self.user_tower(torch.cat(feats, dim=-1))\n",
    "        if self.norm_user:\n",
    "            u = F.normalize(u, dim=-1)\n",
    "        return u\n",
    "\n",
    "\n",
    "    def item_vec(self, item_ids, item_text=None):\n",
    "        v = self.item_id_emb(item_ids)                               # [*,d]\n",
    "        feats = [v]\n",
    "        if self.has_text and item_text is not None:\n",
    "            feats.append(self.text_proj(item_text))\n",
    "        v = torch.cat(feats, dim=-1) if len(feats) > 1 else feats[0]\n",
    "        v = self.item_tower(v)\n",
    "        if self.norm_item: v = F.normalize(v, dim=-1)\n",
    "        return v\n",
    "\n",
    "    def forward(self, batch, text_bank=None, device=None):\n",
    "        hist = batch[\"hist\"].to(torch.long).to(device)\n",
    "        pos  = batch[\"pos\"].to(torch.long).to(device)\n",
    "        country = batch[\"country\"].to(torch.long).to(device) if self.has_country else None\n",
    "        negs = batch[\"negs\"].to(torch.long).to(device)               # [B,K] or [B,0]\n",
    "\n",
    "        u = self.user_vec(hist, country_ids=country)                  # [B,d]\n",
    "\n",
    "        # Gather positive and negative item vectors (with optional text)\n",
    "        pos_txt = text_bank[(pos-1).clamp_min(0)] if (self.has_text and text_bank is not None) else None\n",
    "        v_pos = self.item_vec(pos, item_text=pos_txt)                 # [B,d]\n",
    "\n",
    "        if negs.numel() > 0:\n",
    "            # flatten negatives and embed\n",
    "            negs_flat = negs.reshape(-1)                              # [B*K]\n",
    "            neg_txt = text_bank[(negs_flat-1).clamp_min(0)] if (self.has_text and text_bank is not None) else None\n",
    "            v_neg = self.item_vec(negs_flat, item_text=neg_txt).view(negs.size(0), negs.size(1), -1)  # [B,K,d]\n",
    "        else:\n",
    "            v_neg = None\n",
    "\n",
    "        return u, v_pos, v_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df163d",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n",
    "\n",
    "We use in‑batch negatives. Start with a couple of epochs and check validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91520133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train CE 1.9301 | val CE 0.0000 | val Recall@10: 0.0818\n",
      "Epoch 02 | train CE 1.2600 | val CE 0.0000 | val Recall@10: 0.1234\n",
      "Epoch 03 | train CE 1.0809 | val CE 0.0000 | val Recall@10: 0.1493\n",
      "Epoch 04 | train CE 0.9884 | val CE 0.0000 | val Recall@10: 0.1713\n",
      "Epoch 05 | train CE 0.9293 | val CE 0.0000 | val Recall@10: 0.1807\n",
      "Epoch 06 | train CE 0.8886 | val CE 0.0000 | val Recall@10: 0.1940\n",
      "Epoch 07 | train CE 0.8568 | val CE 0.0000 | val Recall@10: 0.1963\n",
      "Epoch 08 | train CE 0.8307 | val CE 0.0000 | val Recall@10: 0.2030\n",
      "Epoch 09 | train CE 0.8078 | val CE 0.0000 | val Recall@10: 0.2118\n",
      "Epoch 10 | train CE 0.7899 | val CE 0.0000 | val Recall@10: 0.2159\n",
      "Epoch 11 | train CE 0.7721 | val CE 0.0000 | val Recall@10: 0.2178\n",
      "Epoch 12 | train CE 0.7581 | val CE 0.0000 | val Recall@10: 0.2233\n",
      "Epoch 13 | train CE 0.7427 | val CE 0.0000 | val Recall@10: 0.2262\n",
      "Epoch 14 | train CE 0.7290 | val CE 0.0000 | val Recall@10: 0.2292\n",
      "Epoch 15 | train CE 0.7163 | val CE 0.0000 | val Recall@10: 0.2315\n",
      "Epoch 16 | train CE 0.7065 | val CE 0.0000 | val Recall@10: 0.2337\n",
      "Epoch 17 | train CE 0.6953 | val CE 0.0000 | val Recall@10: 0.2330\n",
      "Epoch 18 | train CE 0.6851 | val CE 0.0000 | val Recall@10: 0.2403\n",
      "Epoch 19 | train CE 0.6757 | val CE 0.0000 | val Recall@10: 0.2390\n",
      "Epoch 20 | train CE 0.6685 | val CE 0.0000 | val Recall@10: 0.2384\n",
      "Epoch 21 | train CE 0.6604 | val CE 0.0000 | val Recall@10: 0.2401\n",
      "Epoch 22 | train CE 0.6542 | val CE 0.0000 | val Recall@10: 0.2421\n",
      "Epoch 23 | train CE 0.6472 | val CE 0.0000 | val Recall@10: 0.2447\n",
      "Epoch 24 | train CE 0.6417 | val CE 0.0000 | val Recall@10: 0.2449\n",
      "Epoch 25 | train CE 0.6370 | val CE 0.0000 | val Recall@10: 0.2432\n",
      "Epoch 26 | train CE 0.6333 | val CE 0.0000 | val Recall@10: 0.2452\n",
      "Epoch 27 | train CE 0.6299 | val CE 0.0000 | val Recall@10: 0.2454\n",
      "Epoch 28 | train CE 0.6287 | val CE 0.0000 | val Recall@10: 0.2461\n",
      "Epoch 29 | train CE 0.6264 | val CE 0.0000 | val Recall@10: 0.2463\n",
      "Epoch 30 | train CE 0.6257 | val CE 0.0000 | val Recall@10: 0.2467\n",
      "Best val Recall@10: 0.2467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_countries = len(country_map) if country_map else 0\n",
    "model = TwoTower(n_items=n_items, n_countries=n_countries, d=CFG[\"d_model\"],\n",
    "                 text_dim=(text_dim if has_text else 0),\n",
    "                 dropout=CFG[\"dropout\"]).to(device)\n",
    "\n",
    "text_bank = torch.from_numpy(item_text_emb).to(torch.float32).to(device) if has_text else None\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "sched = CosineAnnealingLR(opt, T_max=CFG[\"epochs\"])\n",
    "use_amp = (device.type == \"cuda\")\n",
    "amp_device = \"cuda\" if use_amp else \"cpu\"\n",
    "\n",
    "# New GradScaler (old: torch.cuda.amp.GradScaler)\n",
    "scaler = torch.amp.GradScaler(amp_device, enabled=use_amp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_item_matrix(m):\n",
    "    ids = torch.arange(1, n_items+1, device=device, dtype=torch.long)\n",
    "    if has_text and text_bank is not None:\n",
    "        vecs = m.item_vec(ids, item_text=text_bank)\n",
    "    else:\n",
    "        vecs = m.item_vec(ids)\n",
    "    return F.normalize(vecs, dim=-1).detach()\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_at_k_val(m, K=10, loader=val_loader):\n",
    "    m.eval()\n",
    "    item_mat = compute_item_matrix(m)                 # [n_items,d]\n",
    "    hits = 0; n = 0\n",
    "    for batch in loader:\n",
    "        hist = batch[\"hist\"].to(torch.long).to(device)\n",
    "        pos  = batch[\"pos\"].to(torch.long).to(device)\n",
    "        country = batch[\"country\"].to(torch.long).to(device) if m.has_country else None\n",
    "        u = m.user_vec(hist, country_ids=country)     # [B,d]\n",
    "        scores = u @ item_mat.t()                     # [B,n_items]\n",
    "        topk = scores.topk(k=min(K, scores.size(1)), dim=1).indices\n",
    "        pos0 = (pos - 1).clamp_min(0).unsqueeze(1)\n",
    "        hit = (topk == pos0).any(dim=1).float()\n",
    "        hits += hit.sum().item(); n += hit.numel()\n",
    "    return hits / max(1, n)\n",
    "\n",
    "def nce_loss(u, v_pos, v_neg, scale):\n",
    "    # u: [B,d], v_pos: [B,d], v_neg: [B,K,d] or None; scale: float\n",
    "    # logits per example: [1 + K]\n",
    "    pos_logit = (u * v_pos).sum(dim=-1, keepdim=True)         # [B,1]\n",
    "    if v_neg is not None and v_neg.numel() > 0:\n",
    "        # [B,K] = u · v_neg_k\n",
    "        neg_logit = torch.bmm(v_neg, u.unsqueeze(-1)).squeeze(-1)  # [B,K]\n",
    "        logits = torch.cat([pos_logit, neg_logit], dim=1)      # [B,1+K]\n",
    "    else:\n",
    "        logits = pos_logit                                     # [B,1]\n",
    "    logits = logits * scale\n",
    "    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)  # pos at index 0\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def run_epoch(loader, training=True):\n",
    "    model.train(training)\n",
    "    total = 0.0; nobs = 0; step = 0\n",
    "    if training: opt.zero_grad(set_to_none=True)\n",
    "    for batch in loader:\n",
    "        with torch.amp.autocast(amp_device, enabled=use_amp):\n",
    "            u, v_pos, v_neg = model(batch, text_bank=text_bank, device=device)\n",
    "            loss = nce_loss(u, v_pos, v_neg, scale=CFG[\"fixed_logit_scale\"]) / CFG[\"accum_steps\"]\n",
    "\n",
    "        if training:\n",
    "            scaler.scale(loss).backward()\n",
    "            step += 1\n",
    "            if step % CFG[\"accum_steps\"] == 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        total += loss.item() * u.size(0) * CFG[\"accum_steps\"]\n",
    "        nobs  += u.size(0)\n",
    "    return total / max(1, nobs)\n",
    "\n",
    "best_recall = -1.0\n",
    "bad = 0\n",
    "for ep in range(1, CFG[\"epochs\"]+1):\n",
    "    tr_ce = run_epoch(train_loader, training=True)\n",
    "    sched.step()\n",
    "    val_ce = run_epoch(val_loader,   training=False)  # CE on val (proxy)\n",
    "    val_rec = recall_at_k_val(model, K=CFG[\"eval_topk\"], loader=val_loader)\n",
    "    print(f\"Epoch {ep:02d} | train CE {tr_ce:.4f} | val CE {val_ce:.4f} | val Recall@{CFG['eval_topk']}: {val_rec:.4f}\")\n",
    "    if val_rec > best_recall + 1e-4:\n",
    "        best_recall = val_rec; bad = 0\n",
    "        torch.save(model.state_dict(), OUT_DIR / \"two_tower_best.pt\")\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= CFG[\"patience\"]:\n",
    "            print(\"Early stopping on Recall@K.\"); break\n",
    "print(\"Best val Recall@{}: {:.4f}\".format(CFG[\"eval_topk\"], best_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdda08",
   "metadata": {},
   "source": [
    "\n",
    "## Export item vectors\n",
    "\n",
    "We compute item vectors for all items and save them to disk for indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036e2d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved item vectors: (4446, 512)\n"
     ]
    }
   ],
   "source": [
    "# --- Export item vectors from best checkpoint ---------------------------------\n",
    "ckpt_path = OUT_DIR / \"two_tower_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ids = torch.arange(1, n_items+1, device=device, dtype=torch.long)\n",
    "    if has_text:\n",
    "        vecs = model.item_vec(ids, item_text=text_bank).detach().cpu().numpy()\n",
    "    else:\n",
    "        vecs = model.item_vec(ids).detach().cpu().numpy()\n",
    "\n",
    "# Normalize and save\n",
    "from numpy.linalg import norm\n",
    "vecs = vecs / np.maximum(norm(vecs, axis=1, keepdims=True), 1e-6)\n",
    "np.save(OUT_DIR / \"retriever_item_vectors.npy\", vecs)\n",
    "print(\"Saved item vectors:\", vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108b1dc",
   "metadata": {},
   "source": [
    "\n",
    "## Build ANN index (FAISS if available)\n",
    "\n",
    "We build a cosine‑similarity (inner‑product on normalized vectors) index. If **FAISS** is not installed, we fall back to a simple brute‑force index using sklearn `NearestNeighbors` and save it with joblib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea05e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved -> /home/kamalyy/KAUST-Project/data/processed/online_retail_II/items.faiss | ntotal: 4446\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Build ANN index ----------------------------------------------------------\n",
    "item_vecs = np.load(OUT_DIR / \"retriever_item_vectors.npy\")\n",
    "# Ensure unit length for inner product ≈ cosine\n",
    "from numpy.linalg import norm\n",
    "norms = np.maximum(norm(item_vecs, axis=1, keepdims=True), 1e-6)\n",
    "item_vecs = item_vecs / norms\n",
    "\n",
    "index_artifacts = {}\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    d = item_vecs.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(item_vecs.astype(np.float32))\n",
    "    faiss.write_index(index, str(OUT_DIR / \"items.faiss\"))\n",
    "    index_artifacts[\"type\"] = \"faiss.IndexFlatIP\"\n",
    "    index_artifacts[\"path\"] = str(OUT_DIR / \"items.faiss\")\n",
    "    print(\"FAISS index saved ->\", OUT_DIR / \"items.faiss\", \"| ntotal:\", index.ntotal)\n",
    "except Exception as e:\n",
    "    print(\"[warn] FAISS not available or failed:\", e)\n",
    "    print(\"Falling back to sklearn NearestNeighbors (brute force).\")\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    import joblib\n",
    "    nn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "    nn.fit(item_vecs)\n",
    "    joblib.dump(nn, OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "    np.save(OUT_DIR / \"retriever_item_vectors_normed.npy\", item_vecs)\n",
    "    index_artifacts[\"type\"] = \"sklearn.NearestNeighbors\"\n",
    "    index_artifacts[\"path\"] = str(OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "    print(\"Sklearn NN index saved ->\", OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "\n",
    "with open(OUT_DIR / \"index_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(index_artifacts, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62cdfa",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval sanity check\n",
    "\n",
    "We embed a few validation histories and retrieve top‑10 items to ensure the pipeline runs end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f887ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: 1673 | Recs: [390, 1136, 1312, 389, 1038, 1135, 1358, 1039, 236, 1555]\n",
      "GT: 1387 | Recs: [1444, 1394, 1377, 1423, 1446, 1421, 1445, 1461, 1359, 1393]\n",
      "GT: 722 | Recs: [1038, 1242, 1039, 1012, 1248, 1135, 1139, 1255, 1137, 814]\n",
      "GT: 3848 | Recs: [3931, 3884, 3879, 3858, 3880, 3870, 3868, 3869, 3783, 3797]\n",
      "GT: 2115 | Recs: [3884, 3797, 3866, 3783, 3870, 3931, 3796, 3808, 3807, 3910]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Retrieval sanity check ---------------------------------------------------\n",
    "def recommend_from_hist(hist_ids, topk=10, default_country=\"United Kingdom\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # +1 shift so 0 = PAD\n",
    "        h = torch.tensor([[x+1 for x in hist_ids[-CFG[\"hist_max\"]:]]],\n",
    "                         dtype=torch.long, device=device)\n",
    "\n",
    "        # provide a country id if the model uses country\n",
    "        if getattr(model, \"has_country\", False):\n",
    "            cidx = torch.tensor([country_map.get(default_country, 0)],\n",
    "                                dtype=torch.long, device=device)\n",
    "            u = model.user_vec(h, country_ids=cidx).detach().cpu().numpy()\n",
    "        else:\n",
    "            u = model.user_vec(h).detach().cpu().numpy()\n",
    "\n",
    "    u = u / max(np.linalg.norm(u), 1e-6)\n",
    "    try:\n",
    "        import faiss\n",
    "        index = faiss.read_index(str(OUT_DIR / \"items.faiss\"))\n",
    "        D, I = index.search(u.astype(np.float32), topk)\n",
    "        return I[0].tolist()\n",
    "    except Exception:\n",
    "        import joblib\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nn = joblib.load(OUT_DIR / \"items_sklearn_nn.joblib\")\n",
    "        dist, nbrs = nn.kneighbors(u, n_neighbors=topk)\n",
    "        return [int(j) for j in nbrs[0].tolist()]\n",
    "\n",
    "\n",
    "# Try on a small sample\n",
    "sample = seq_val.sample(5, random_state=0)\n",
    "for _, row in sample.iterrows():\n",
    "    hist = [int(x) for x in row['history_idx'].split()] if isinstance(row['history_idx'], str) else []\n",
    "    recs = recommend_from_hist(hist, topk=10)\n",
    "    print(\"GT:\", int(row['pos_item_idx']), \"| Recs:\", recs[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fc8ec",
   "metadata": {},
   "source": [
    "\n",
    "## Save model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d26b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model -> /home/kamalyy/KAUST-Project/data/processed/online_retail_II/two_tower.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Save model ---------------------------------------------------------------\n",
    "torch.save(model.state_dict(), OUT_DIR / \"two_tower.pt\")\n",
    "with open(OUT_DIR / \"two_tower_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "print(\"Saved model ->\", OUT_DIR / \"two_tower.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30580cc1-5f8b-4c28-8b9f-1fe06056f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embeddings for notebook 04...\n",
      "Saved user embeddings: (5748, 512)\n",
      "Saved item embeddings: (4446, 512)\n",
      "✅ Embeddings ready for notebook 04!\n"
     ]
    }
   ],
   "source": [
    "# --- Save Embeddings for Notebook 04 ---\n",
    "ITEM_MAP_PATH = OUT_DIR / 'item_id_map.parquet'\n",
    "CUSTOMER_MAP_PATH = OUT_DIR / 'customer_id_map.parquet'\n",
    "customer_map = pd.read_parquet(OUT_DIR / 'customer_id_map.parquet', engine=\"fastparquet\")\n",
    "item_map = pd.read_parquet(OUT_DIR / 'item_id_map.parquet', engine=\"fastparquet\")\n",
    "\n",
    "print(\"\\nSaving embeddings for notebook 04...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Save item embeddings with text features\n",
    "    all_item_ids = torch.arange(1, len(item_map)+1, device=device, dtype=torch.long)\n",
    "    \n",
    "    if has_text and item_text_emb is not None:\n",
    "        # Use actual text features\n",
    "        text_bank = torch.from_numpy(item_text_emb).to(device)\n",
    "        item_embeddings = model.item_vec(all_item_ids, item_text=text_bank).cpu().numpy()\n",
    "    else:\n",
    "        # Create dummy text features\n",
    "        dummy_text_dim = 512  # or whatever dimension the model expects\n",
    "        text_bank = torch.zeros(len(item_map), dummy_text_dim, device=device)\n",
    "        item_embeddings = model.item_vec(all_item_ids, item_text=text_bank).cpu().numpy()\n",
    "    \n",
    "    np.save(OUT_DIR / 'item_embeddings.npy', item_embeddings)\n",
    "    \n",
    "    # Save user embeddings by computing them from sequences\n",
    "    user_embeddings = []\n",
    "    user_ids = []\n",
    "    \n",
    "    # Get all unique users from sequences\n",
    "    all_users = pd.concat([seq_train, seq_val, seq_test])['user_idx'].unique()\n",
    "    \n",
    "    for user_idx in all_users:\n",
    "        # Get user's history from sequences\n",
    "        user_seqs = seq_train[seq_train['user_idx'] == user_idx]\n",
    "        if len(user_seqs) == 0:\n",
    "            user_seqs = seq_val[seq_val['user_idx'] == user_idx]\n",
    "        if len(user_seqs) == 0:\n",
    "            user_seqs = seq_test[seq_test['user_idx'] == user_idx]\n",
    "        \n",
    "        if len(user_seqs) > 0:\n",
    "            # Use the most recent history\n",
    "            latest_seq = user_seqs.iloc[-1]\n",
    "            hist_str = latest_seq['history_idx']\n",
    "            hist_items = parse_hist(hist_str)\n",
    "            \n",
    "            if len(hist_items) > 0:\n",
    "                # Convert to tensor format (+1 shift, 0=PAD)\n",
    "                hist_tensor = torch.tensor([[x+1 for x in hist_items[-CFG[\"hist_max\"]:]]], \n",
    "                                         dtype=torch.long, device=device)\n",
    "                \n",
    "                # Compute user embedding\n",
    "                user_emb = model.user_vec(hist_tensor).cpu().numpy()\n",
    "                user_embeddings.append(user_emb[0])  # Remove batch dimension\n",
    "                user_ids.append(user_idx)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    user_embeddings = np.array(user_embeddings)\n",
    "    \n",
    "    # Create full user embedding matrix (fill missing users with zeros)\n",
    "    full_user_embeddings = np.zeros((len(customer_map), user_embeddings.shape[1]))\n",
    "    for i, user_idx in enumerate(user_ids):\n",
    "        full_user_embeddings[user_idx] = user_embeddings[i]\n",
    "    \n",
    "    np.save(OUT_DIR / 'user_embeddings.npy', full_user_embeddings)\n",
    "\n",
    "print(f\"Saved user embeddings: {full_user_embeddings.shape}\")\n",
    "print(f\"Saved item embeddings: {item_embeddings.shape}\")\n",
    "print(\"✅ Embeddings ready for notebook 04!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1748ba",
   "metadata": {},
   "source": [
    "\n",
    "### Next\n",
    "Proceed to **04_ranker_and_eval.ipynb** to train a DIN/MLP ranker that scores retrieved candidates using richer features (country, recency, popularity, price buckets, and optionally text vectors).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
