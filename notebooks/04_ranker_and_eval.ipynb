{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1433b1d-84c7-41c1-9815-a0d00d3de4c0",
   "metadata": {},
   "source": [
    "# 04: Two-Stage Ranker Notebook\n",
    "\n",
    "This notebook trains and evaluates an MLP ranker on two-stage candidate lists for Jarir:\n",
    "\n",
    "- **Config & Imports**  \n",
    "- **Load Sequences & Maps**  \n",
    "- **Load Embeddings & Feature Tensors**  \n",
    "- **Candidate Generation**  \n",
    "- **Feature Engineering (sharded)**  \n",
    "- **Ranker Model Definition**  \n",
    "- **Training & Evaluation**  \n",
    "- **Comparison with Baselines**  \n",
    "- **Save Results**  \n",
    "- **Final Summary**  \n",
    "- **Feature Importance Analysis**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e818ed-68c6-498a-9bc6-ad58ecacd0d4",
   "metadata": {},
   "source": [
    "## 0. Config & Imports\n",
    "\n",
    "- Import standard libraries and PyTorch  \n",
    "- Set up data paths and I/O settings  \n",
    "- Define configuration dictionary `CFG`  \n",
    "- Initialize device and random seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea847173-dfef-4bfb-bdea-811588eb6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: ..\\data\\processed\\jarir\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 0) Config & Imports --------------------------------------------------------\n",
    "import os, json, math, time, gc, glob, bisect, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Global reproducibility\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "if hasattr(torch.backends, 'cudnn'):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Data paths\n",
    "OUT_DIR = Path('../data/processed/jarir/')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "READ_KW  = dict(engine=\"fastparquet\")\n",
    "WRITE_KW = dict(engine=\"fastparquet\", index=False)\n",
    "\n",
    "CFG = {\n",
    "    # Retrieval\n",
    "    \"cand_topk\": 100,\n",
    "    \"cand_batch\": 4096,\n",
    "    # Histories & features\n",
    "    \"hist_max\": 15,\n",
    "    # Feature building\n",
    "    \"feat_batch_q\": 1024,\n",
    "    \"shard_rows\": 1_000_000,\n",
    "    \"neg_per_query\": 20,\n",
    "    \"hard_negatives\": True,\n",
    "    # Ranker\n",
    "    \"batch_size\": 2048,\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 5,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.3,\n",
    "    \"hidden\": 384,\n",
    "    \"eval_topk\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"use_text\": False,\n",
    "    # Ranker regularization / splits\n",
    "    \"feature_drop_prob_dot_uv\": 0.3,\n",
    "    \"dev_split_q\": 0.90,\n",
    "    # Two-Tower embeddings\n",
    "    \"embedding_dim\": 256,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd643b2b-fb2f-45c9-a2c8-1182363988c0",
   "metadata": {},
   "source": [
    "## 1. Load Sequences & Maps\n",
    "- Load precomputed train/val/test sequence tables\n",
    "- Load item and customer ID maps\n",
    "- Compute popularity counts for baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16f3f171-948e-434b-bd27-658102e18855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: (1108, 6) (169, 6) (160, 6)\n",
      "Items: 1735 Customers: 929\n"
     ]
    }
   ],
   "source": [
    "# Load sequences\n",
    "seq_train = pd.read_parquet(OUT_DIR/'sequences_train.parquet', **READ_KW)\n",
    "seq_val   = pd.read_parquet(OUT_DIR/'sequences_val.parquet', **READ_KW)\n",
    "seq_test  = pd.read_parquet(OUT_DIR/'sequences_test.parquet', **READ_KW)\n",
    "print(\"Train/Val/Test:\", seq_train.shape, seq_val.shape, seq_test.shape)\n",
    "\n",
    "# Load item and customer maps\n",
    "item_map     = pd.read_parquet(OUT_DIR/'item_id_map.parquet', **READ_KW)\n",
    "customer_map = pd.read_parquet(OUT_DIR/'customer_id_map.parquet', **READ_KW)\n",
    "print(\"Items:\", len(item_map), \"Customers:\", len(customer_map))\n",
    "\n",
    "# Popularity for features/fallback\n",
    "pop_counts = seq_train['pos_item_idx'].value_counts()\n",
    "pop_norm   = (pop_counts - pop_counts.min()) / (pop_counts.max() - pop_counts.min() + 1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fba0ca-e062-4c94-961e-49cc5ed6e1c1",
   "metadata": {},
   "source": [
    "## 2. Load Two-Tower Embeddings & Feature Tensors\n",
    "- Load or generate user/item embeddings from two‐tower model\n",
    "- Move embeddings to GPU tensors\n",
    "- Build popularity and price z‐score feature tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "963a4dba-aefc-4ebc-a251-f9a47c60e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Tower embeddings loaded: (929, 256) (1735, 256)\n",
      "Price features loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Two-Tower embeddings\n",
    "USER_EMB_PATH = OUT_DIR/'user_embeddings.npy'\n",
    "ITEM_EMB_PATH = OUT_DIR/'item_embeddings.npy'\n",
    "\n",
    "if USER_EMB_PATH.exists() and ITEM_EMB_PATH.exists():\n",
    "    USER_EMB = np.load(USER_EMB_PATH).astype('float32')\n",
    "    ITEM_EMB = np.load(ITEM_EMB_PATH).astype('float32')\n",
    "    print(\"Two-Tower embeddings loaded:\", USER_EMB.shape, ITEM_EMB.shape)\n",
    "else:\n",
    "    print(\"❌ Missing Two-Tower embeddings. Please run notebook 03 first.\")\n",
    "    n_users = len(customer_map)\n",
    "    n_items = len(item_map)\n",
    "    USER_EMB = np.random.randn(n_users, CFG[\"embedding_dim\"]).astype('float32')\n",
    "    ITEM_EMB = np.random.randn(n_items, CFG[\"embedding_dim\"]).astype('float32')\n",
    "    USER_EMB /= np.linalg.norm(USER_EMB, axis=1, keepdims=True)\n",
    "    ITEM_EMB /= np.linalg.norm(ITEM_EMB, axis=1, keepdims=True)\n",
    "    print(\"Using dummy embeddings for testing\")\n",
    "\n",
    "# GPU tensors for embeddings\n",
    "USER_EMB_T = torch.from_numpy(USER_EMB).to(device, non_blocking=True)\n",
    "ITEM_EMB_T = torch.from_numpy(ITEM_EMB).to(device, non_blocking=True)\n",
    "\n",
    "# Popularity tensor\n",
    "pop_vec = torch.zeros(len(item_map), dtype=torch.float32, device=device)\n",
    "pop_idx = torch.tensor(pop_counts.index.values, dtype=torch.long, device=device)\n",
    "pop_val = torch.tensor(pop_norm.loc[pop_counts.index].values, dtype=torch.float32, device=device)\n",
    "pop_vec[pop_idx] = pop_val\n",
    "\n",
    "# Load item metadata for additional features\n",
    "items_clean_path = OUT_DIR/'items_clean.parquet'\n",
    "price_z = None\n",
    "if items_clean_path.exists():\n",
    "    items_clean = pd.read_parquet(items_clean_path, **READ_KW)\n",
    "    if 'price_median' in items_clean.columns:\n",
    "        m = items_clean[['stock_code','price_median']].dropna()\n",
    "        m = m.merge(item_map, on='stock_code', how='inner')\n",
    "        if len(m) > 0:\n",
    "            mu, sigma = m['price_median'].mean(), m['price_median'].std() + 1e-6\n",
    "            z = ((m['price_median'] - mu) / sigma).astype(float)\n",
    "            price_z = torch.zeros(len(item_map), dtype=torch.float32, device=device)\n",
    "            ii = torch.tensor(m['item_idx'].astype(int).values, dtype=torch.long, device=device)\n",
    "            price_z[ii] = torch.tensor(z.values, dtype=torch.float32, device=device)\n",
    "            print(\"Price features loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395ea4b-af43-42c2-9035-0b76af8bce0e",
   "metadata": {},
   "source": [
    "## 3. Candidate Generation\n",
    " \n",
    "- Define helpers to parse history strings and compute user vectors\n",
    "- Build tensor of history indices\n",
    "- Generate top‐K candidates per user from two‐tower embeddings\n",
    "- Save and reload candidate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b814f49f-b74a-4ee8-812e-41f2f0d732bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: (1108, 4) (169, 4) (160, 4)\n"
     ]
    }
   ],
   "source": [
    "def parse_hist(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    return [int(x) for x in s.strip().split()]\n",
    "\n",
    "@torch.no_grad()\n",
    "def user_vecs_from_hist_batch(hist_tensor):\n",
    "    \"\"\"Compute user vectors from history using Two-Tower item embeddings\"\"\"\n",
    "    B, L = hist_tensor.shape\n",
    "    safe_idx = hist_tensor.clamp(min=0)\n",
    "    H = ITEM_EMB_T.index_select(0, safe_idx.view(-1)).view(B, L, -1)\n",
    "    mask = (hist_tensor >= 0).float().unsqueeze(-1)\n",
    "    U = (H * mask).sum(1) / mask.sum(1).clamp_min(1e-6)\n",
    "    return F.normalize(U, dim=-1)\n",
    "\n",
    "def build_hist_tensor(series, L):\n",
    "    \"\"\"Build history tensor from string series\"\"\"\n",
    "    B = len(series)\n",
    "    H = torch.full((B, L), -1, dtype=torch.long)\n",
    "    for i, s in enumerate(series):\n",
    "        h = parse_hist(s)\n",
    "        if len(h) > L: h = h[-L:]\n",
    "        if h:\n",
    "            H[i, -len(h):] = torch.tensor(h, dtype=torch.long)\n",
    "    return H\n",
    "\n",
    "def gen_candidates_gpu(df, topk=100, batch_q=4096):\n",
    "    \"\"\"Generate candidates using Two-Tower embeddings\"\"\"\n",
    "    hist_series = df['history_idx'].astype(str).tolist()\n",
    "    H = build_hist_tensor(hist_series, CFG[\"hist_max\"])\n",
    "    U_chunks = []\n",
    "    for i in range(0, H.size(0), batch_q):\n",
    "        Ub = user_vecs_from_hist_batch(H[i:i+batch_q].to(device))\n",
    "        U_chunks.append(Ub.cpu())\n",
    "    U = torch.cat(U_chunks, 0).numpy().astype('float32')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        U_t = torch.from_numpy(U).to(device)\n",
    "        sims = U_t @ ITEM_EMB_T.t()\n",
    "        _, top_k_indices = torch.topk(sims, k=topk, dim=1)\n",
    "        I_full = top_k_indices.cpu().numpy().astype('int32')\n",
    "\n",
    "    pos_list = df['pos_item_idx'].astype(int).tolist()\n",
    "    ts_list  = df['ts'].astype(str).tolist() if 'ts' in df.columns else ['']*len(pos_list)\n",
    "    rows = []\n",
    "    for pos, cand_idx, ts, h_s in zip(pos_list, I_full, ts_list, hist_series):\n",
    "        cand = cand_idx.tolist()\n",
    "        if pos not in cand:\n",
    "            cand[-1] = int(pos)\n",
    "        rows.append((h_s, int(pos), \" \".join(map(str,cand)), ts))\n",
    "    return pd.DataFrame(rows, columns=['history_idx','pos_item_idx','cands','ts'])\n",
    "\n",
    "# Build & save candidates if missing\n",
    "CAND_TRAIN_PATH = OUT_DIR/'candidates_train.parquet'\n",
    "CAND_VAL_PATH   = OUT_DIR/'candidates_val.parquet'\n",
    "CAND_TEST_PATH  = OUT_DIR/'candidates_test.parquet'\n",
    "\n",
    "if not (CAND_TRAIN_PATH.exists() and CAND_VAL_PATH.exists() and CAND_TEST_PATH.exists()):\n",
    "    print(\"Generating training candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_train, topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TRAIN_PATH, **WRITE_KW)\n",
    "    print(\"Generating validation candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_val,   topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_VAL_PATH,   **WRITE_KW)\n",
    "    print(\"Generating test candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_test,  topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TEST_PATH,  **WRITE_KW)\n",
    "\n",
    "cand_train = pd.read_parquet(CAND_TRAIN_PATH, **READ_KW)\n",
    "cand_val   = pd.read_parquet(CAND_VAL_PATH,   **READ_KW)\n",
    "cand_test  = pd.read_parquet(CAND_TEST_PATH,  **READ_KW)\n",
    "print(\"Candidates:\", cand_train.shape, cand_val.shape, cand_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8539d4-863a-42ce-9bf5-d0d248deaeb2",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering on GPU (Sharded)\n",
    "- Pack features (dot product, max-sim, popularity, hist-length, price z)\n",
    "- Subsample hard negatives per query\n",
    "- Shard features into Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0952832-47d6-4020-a5bc-f6029d3330ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard dirs: ..\\data\\processed\\jarir\\ranker_feats_train_shards ..\\data\\processed\\jarir\\ranker_feats_val_shards ..\\data\\processed\\jarir\\ranker_feats_test_shards\n"
     ]
    }
   ],
   "source": [
    "def _pack_batch_features(Ub, Hb, Cb, Pb):\n",
    "    \"\"\"Pack features for a batch of candidates\"\"\"\n",
    "    B, d = Ub.shape\n",
    "    K = Cb.size(1)\n",
    "    L = Hb.size(1)\n",
    "\n",
    "    Vc = ITEM_EMB_T.index_select(0, Cb.view(-1)).view(B, K, d)\n",
    "    dot_uv = (Ub.unsqueeze(1) * Vc).sum(-1)\n",
    "\n",
    "    safe_hist = Hb.clamp(min=0)\n",
    "    Hvec = ITEM_EMB_T.index_select(0, safe_hist.view(-1)).view(B, L, d)\n",
    "    Hvec = F.normalize(Hvec, dim=-1); Vc_n = F.normalize(Vc, dim=-1)\n",
    "    sims = torch.matmul(Hvec, Vc_n.transpose(1,2))\n",
    "    maskL = (Hb >= 0).unsqueeze(-1).float()\n",
    "    sims = sims + (maskL - 1.0) * 1e9\n",
    "    max_sim_recent = sims.max(dim=1).values\n",
    "\n",
    "    pop    = pop_vec.index_select(0, Cb.view(-1)).view(B, K)\n",
    "    hlen   = (Hb >= 0).float().sum(1)/float(CFG[\"hist_max\"])\n",
    "    hlen   = hlen.unsqueeze(1).expand(B, K)\n",
    "    price  = price_z.index_select(0, Cb.view(-1)).view(B, K) if isinstance(price_z, torch.Tensor) else torch.zeros((B,K), device=Ub.device)\n",
    "    labels = (Cb == Pb.view(-1,1)).float()\n",
    "\n",
    "    return {\"dot_uv\": dot_uv, \"max_sim_recent\": max_sim_recent, \"pop\": pop,\n",
    "            \"hist_len\": hlen, \"price_z\": price, \"label\": labels, \"item_idx\": Cb.float()}\n",
    "\n",
    "def _select_negatives(feats):\n",
    "    \"\"\"Keep 1 positive + N negatives per query if configured\"\"\"\n",
    "    if CFG[\"neg_per_query\"] is None:\n",
    "        return feats\n",
    "    B, K = feats[\"label\"].shape\n",
    "    pos_col = torch.argmax(feats[\"label\"], dim=1, keepdim=True)\n",
    "    if CFG[\"hard_negatives\"]:\n",
    "        neg_scores = feats[\"dot_uv\"].clone()\n",
    "        neg_scores.scatter_(1, pos_col, -1e9)\n",
    "        _, neg_idx = torch.topk(neg_scores, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    else:\n",
    "        rnd = torch.rand_like(feats[\"dot_uv\"])\n",
    "        rnd.scatter_(1, pos_col, 1e9)\n",
    "        _, neg_idx = torch.topk(-rnd, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    keep_cols = torch.cat([pos_col, neg_idx], dim=1)\n",
    "    for k in [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\",\"label\",\"item_idx\"]:\n",
    "        feats[k] = torch.gather(feats[k], 1, keep_cols)\n",
    "    return feats\n",
    "\n",
    "def build_feats_gpu_sharded(cand_df, split_name):\n",
    "    \"\"\"Build features on GPU and save to sharded Parquet files\"\"\"\n",
    "    N = len(cand_df)\n",
    "    L = CFG[\"hist_max\"]\n",
    "    bq = CFG[\"feat_batch_q\"]\n",
    "    out_dir = OUT_DIR / f\"ranker_feats_{split_name}_shards\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def write_shard(idx, feats_dict):\n",
    "        cpu = {k: v.detach().float().view(-1).cpu().numpy() for k,v in feats_dict.items()}\n",
    "        df = pd.DataFrame(cpu); df['item_idx'] = df['item_idx'].astype(np.int32)\n",
    "        df.to_parquet(out_dir / f\"part_{idx:03d}.parquet\", **WRITE_KW)\n",
    "\n",
    "    hist_series  = cand_df['history_idx'].astype(str).tolist()\n",
    "    pos_list     = cand_df['pos_item_idx'].astype(int).tolist()\n",
    "    cands_series = cand_df['cands'].astype(str).tolist()\n",
    "\n",
    "    rows_written = 0; shard_idx = 0; buf = None\n",
    "    for i in range(0, N, bq):\n",
    "        H = build_hist_tensor(hist_series[i:i+bq], L).to(device, non_blocking=True)\n",
    "        P = torch.tensor(pos_list[i:i+bq], dtype=torch.long, device=device)\n",
    "        C = torch.tensor([[int(x) for x in s.split()] for s in cands_series[i:i+bq]],\n",
    "                         dtype=torch.long, device=device)\n",
    "        U = user_vecs_from_hist_batch(H)\n",
    "        feats = _pack_batch_features(U, H, C, P)\n",
    "        feats = _select_negatives(feats)\n",
    "\n",
    "        if buf is None:\n",
    "            buf = {k: v.detach().clone() for k,v in feats.items()}\n",
    "        else:\n",
    "            for k in buf:\n",
    "                buf[k] = torch.cat([buf[k], feats[k]], dim=0)\n",
    "\n",
    "        rows_in_buf = int(buf[\"label\"].numel())\n",
    "        if rows_in_buf >= CFG[\"shard_rows\"]:\n",
    "            write_shard(shard_idx, buf); shard_idx += 1\n",
    "            buf = None; torch.cuda.empty_cache()\n",
    "\n",
    "        rows_written += int(feats[\"label\"].numel())\n",
    "        if (i//bq) % 10 == 0:\n",
    "            print(f\"[{split_name}] Built ~{rows_written/1e6:.2f}M rows...\")\n",
    "\n",
    "    if buf is not None:\n",
    "        write_shard(shard_idx, buf)\n",
    "\n",
    "    print(f\"[{split_name}] Done. Rows ~{rows_written:,}. Shards -> {out_dir}\")\n",
    "    return out_dir\n",
    "\n",
    "# Build shards if missing\n",
    "train_shards_dir = OUT_DIR/\"ranker_feats_train_shards\"\n",
    "val_shards_dir   = OUT_DIR/\"ranker_feats_val_shards\"\n",
    "test_shards_dir  = OUT_DIR/\"ranker_feats_test_shards\"\n",
    "\n",
    "if not train_shards_dir.exists():\n",
    "    train_shards_dir = build_feats_gpu_sharded(cand_train, \"train\")\n",
    "if not val_shards_dir.exists():\n",
    "    val_shards_dir   = build_feats_gpu_sharded(cand_val,   \"val\")\n",
    "if not test_shards_dir.exists():\n",
    "    test_shards_dir  = build_feats_gpu_sharded(cand_test,  \"test\")\n",
    "\n",
    "print(\"Shard dirs:\", train_shards_dir, val_shards_dir, test_shards_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead27b8f-be16-4b1c-968c-3ca7a4e3baf7",
   "metadata": {},
   "source": [
    "## 5. Ranker Model Definition\n",
    "- Define feature columns and data loader for shard batches\n",
    "- Implement RankerMLP neural network class\n",
    "- Initialize model, optimizer, loss, and mixed‐precision scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42c51b8a-95bf-4286-bf90-eee46c3275d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKER_COLS = [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\"]\n",
    "\n",
    "class FeatureDropDotUV(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__(); self.p = p\n",
    "    def forward(self, X):\n",
    "        if not self.training or self.p <= 0: return X\n",
    "        # Zero-out dot_uv feature with prob p during training to reduce overfit\n",
    "        mask = (torch.rand(X.size(0), device=X.device) > self.p).float().unsqueeze(1)\n",
    "        X = X.clone(); X[:, 0:1] = X[:, 0:1] * mask  # dot_uv is col 0\n",
    "        return X\n",
    "\n",
    "def shard_batches(files, batch_size):\n",
    "    \"\"\"Generate batches from sharded Parquet files\"\"\"\n",
    "    for f in sorted(glob.glob(str(Path(files)/\"part_*.parquet\"))):\n",
    "        df = pd.read_parquet(f, engine=\"fastparquet\", columns=RANKER_COLS+[\"label\"])\n",
    "        X_np = df[RANKER_COLS].to_numpy(dtype='float32', copy=False)\n",
    "        y_np = df[\"label\"].to_numpy(dtype='float32', copy=False)\n",
    "        X = torch.from_numpy(X_np).to(device, non_blocking=True)\n",
    "        y = torch.from_numpy(y_np).to(device, non_blocking=True)\n",
    "        perm = torch.randperm(X.size(0), device=device)\n",
    "        for i in range(0, X.size(0), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            yield X[idx], y[idx]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class RankerMLP(nn.Module):\n",
    "    \"\"\"Wider MLP ranker with residual skip\"\"\"\n",
    "    def __init__(self, d_in, hidden=384, dropout=0.3, feature_drop_p=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_dot = FeatureDropDotUV(feature_drop_p)\n",
    "        self.fc1 = nn.Linear(d_in, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden//2)\n",
    "        self.out = nn.Linear(hidden//2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "        self.ln1 = nn.LayerNorm(hidden)\n",
    "        self.ln2 = nn.LayerNorm(hidden)\n",
    "    def forward(self, x):\n",
    "        x = self.drop_dot(x)\n",
    "        h1 = self.dropout(self.act(self.fc1(x)))\n",
    "        h1 = self.ln1(h1)\n",
    "        h2 = self.dropout(self.act(self.fc2(h1)))\n",
    "        h2 = self.ln2(h2)\n",
    "        h = h1 + h2  # residual\n",
    "        h = self.dropout(self.act(self.fc3(h)))\n",
    "        return self.out(h).squeeze(-1)\n",
    "\n",
    "model   = RankerMLP(len(RANKER_COLS), hidden=CFG[\"hidden\"], dropout=CFG[\"dropout\"], feature_drop_p=CFG[\"feature_drop_prob_dot_uv\"]).to(device)\n",
    "opt     = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "use_amp = (device.type == \"cuda\")\n",
    "amp_device = \"cuda\" if use_amp else \"cpu\"\n",
    "scaler  = torch.amp.GradScaler(amp_device, enabled=use_amp)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e915284-45a9-4d28-97f5-fb6f7ffb69da",
   "metadata": {},
   "source": [
    "## 6. Training & Evaluation\n",
    "- Define train_ranker() to run epoch loops with mixed‐precision\n",
    "- Evaluate on validation shards each epoch\n",
    "- Implement rerank_one_batch and eval_reranked for final metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f3a3617-7cb5-4291-a4fd-81c4698c13d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING RANKER\n",
      "==================================================\n",
      "Epoch 01 | train BCE 0.2589 | eval Recall@10 0.4513 | eval NDCG@10 0.3529\n",
      "Epoch 02 | train BCE 0.1946 | eval Recall@10 0.4867 | eval NDCG@10 0.3822\n",
      "Epoch 03 | train BCE 0.1856 | eval Recall@10 0.5221 | eval NDCG@10 0.4062\n",
      "Epoch 04 | train BCE 0.1824 | eval Recall@10 0.5221 | eval NDCG@10 0.4143\n",
      "Epoch 05 | train BCE 0.1790 | eval Recall@10 0.5664 | eval NDCG@10 0.4302\n",
      "Epoch 06 | train BCE 0.1755 | eval Recall@10 0.5841 | eval NDCG@10 0.4473\n",
      "Epoch 07 | train BCE 0.1729 | eval Recall@10 0.6195 | eval NDCG@10 0.4669\n",
      "Epoch 08 | train BCE 0.1705 | eval Recall@10 0.6283 | eval NDCG@10 0.4844\n",
      "Epoch 09 | train BCE 0.1706 | eval Recall@10 0.6460 | eval NDCG@10 0.4966\n",
      "Epoch 10 | train BCE 0.1672 | eval Recall@10 0.6195 | eval NDCG@10 0.4968\n",
      "Epoch 11 | train BCE 0.1660 | eval Recall@10 0.6637 | eval NDCG@10 0.5408\n",
      "Epoch 12 | train BCE 0.1626 | eval Recall@10 0.6372 | eval NDCG@10 0.5245\n",
      "Epoch 13 | train BCE 0.1617 | eval Recall@10 0.6726 | eval NDCG@10 0.5495\n",
      "Epoch 14 | train BCE 0.1598 | eval Recall@10 0.6460 | eval NDCG@10 0.5466\n",
      "Epoch 15 | train BCE 0.1609 | eval Recall@10 0.6726 | eval NDCG@10 0.5520\n",
      "Epoch 16 | train BCE 0.1594 | eval Recall@10 0.6903 | eval NDCG@10 0.5736\n",
      "Epoch 17 | train BCE 0.1582 | eval Recall@10 0.6637 | eval NDCG@10 0.5532\n",
      "Epoch 18 | train BCE 0.1546 | eval Recall@10 0.7168 | eval NDCG@10 0.5858\n",
      "Epoch 19 | train BCE 0.1506 | eval Recall@10 0.6814 | eval NDCG@10 0.5742\n",
      "Epoch 20 | train BCE 0.1498 | eval Recall@10 0.7257 | eval NDCG@10 0.6145\n",
      "Best eval Recall@10 = 0.7257\n",
      "\n",
      "==================================================\n",
      "FINAL EVALUATION\n",
      "==================================================\n",
      "VAL  — Recall@10: 0.6509, NDCG@10: 0.5328\n",
      "TEST — Recall@10: 0.6937, NDCG@10: 0.5541\n",
      "VAL  — Recall@10: 0.6509, NDCG@10: 0.5328, Coverage@10: 0.2686\n",
      "TEST — Recall@10: 0.6937, NDCG@10: 0.5541, Coverage@10: 0.2375\n",
      "Saved CSV to: ..\\data\\processed\\jarir\\summary_metrics.csv\n",
      "Saved TXT to: ..\\data\\processed\\jarir\\summary_notes.txt\n"
     ]
    }
   ],
   "source": [
    "def rerank_one_batch(cand_df_slice):\n",
    "    \"\"\"Re-rank candidates for one batch\"\"\"\n",
    "    H = build_hist_tensor(cand_df_slice['history_idx'].astype(str).tolist(), CFG[\"hist_max\"]).to(device)\n",
    "    P = torch.tensor(cand_df_slice['pos_item_idx'].astype(int).tolist(), dtype=torch.long, device=device)\n",
    "    C = torch.tensor([[int(x) for x in s.split()] for s in cand_df_slice['cands'].astype(str).tolist()],\n",
    "                     dtype=torch.long, device=device)\n",
    "    U = user_vecs_from_hist_batch(H)\n",
    "    feats = _pack_batch_features(U, H, C, P)\n",
    "    X = torch.stack([feats[c] for c in RANKER_COLS], dim=-1).view(-1, len(RANKER_COLS))\n",
    "    scores = model(X).view(len(feats[\"label\"]), -1)\n",
    "    topk = min(CFG[\"eval_topk\"], C.size(1))\n",
    "    _, idx = torch.topk(scores, k=topk, dim=1)\n",
    "    return [C[i][idx[i]].tolist() for i in range(C.size(0))]\n",
    "\n",
    "def eval_reranked(cand_df, split=\"val\"):\n",
    "    \"\"\"Evaluate re-ranked candidates\"\"\"\n",
    "    model.eval()\n",
    "    hits = 0; ndcgs = 0.0; tot = 0\n",
    "    B = 1024\n",
    "    for i in range(0, len(cand_df), B):\n",
    "        batch_df = cand_df.iloc[i:i+B]\n",
    "        reranked = rerank_one_batch(batch_df)\n",
    "        for pos, rr in zip(batch_df['pos_item_idx'].tolist(), reranked):\n",
    "            hits += float(int(pos) in rr)\n",
    "            if int(pos) in rr:\n",
    "                rank = rr.index(int(pos)) + 1\n",
    "                ndcgs += 1.0 / math.log2(rank + 1.0)\n",
    "            tot += 1\n",
    "    return hits/max(1,tot), ndcgs/max(1,tot)\n",
    "\n",
    "def split_train_dev_candidates(cand_train, q=0.90):\n",
    "    if 'ts' not in cand_train.columns or cand_train['ts'].isna().all():\n",
    "        return cand_train, None\n",
    "    ts = pd.to_datetime(cand_train['ts'])\n",
    "    cut = ts.quantile(q)\n",
    "    tr = cand_train[ts < cut].reset_index(drop=True)\n",
    "    dv = cand_train[ts >= cut].reset_index(drop=True)\n",
    "    return tr, dv\n",
    "\n",
    "cand_train_tr, cand_train_dev = split_train_dev_candidates(cand_train, CFG['dev_split_q'])\n",
    "\n",
    "# Rebuild shards if we created a dev split\n",
    "if cand_train_dev is not None and len(cand_train_dev) > 0:\n",
    "    train_shards_dir = OUT_DIR/\"ranker_feats_train_tr_shards\"\n",
    "    dev_shards_dir   = OUT_DIR/\"ranker_feats_train_dev_shards\"\n",
    "    if not train_shards_dir.exists():\n",
    "        train_shards_dir = build_feats_gpu_sharded(cand_train_tr, \"train_tr\")\n",
    "    if not dev_shards_dir.exists():\n",
    "        dev_shards_dir   = build_feats_gpu_sharded(cand_train_dev, \"train_dev\")\n",
    "else:\n",
    "    dev_shards_dir = None\n",
    "\n",
    "\n",
    "def train_ranker():\n",
    "    \"\"\"Train the ranker model with dev monitoring to avoid tuning on val\"\"\"\n",
    "    best_recall = -1.0; bad = 0\n",
    "    for ep in range(1, CFG[\"epochs\"]+1):\n",
    "        model.train(); total_loss=0.0; nobs=0\n",
    "        for Xb, yb in shard_batches(train_shards_dir, CFG[\"batch_size\"]):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(amp_device, enabled=use_amp, dtype=torch.float16):\n",
    "                logits = model(Xb)\n",
    "                loss   = loss_fn(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            total_loss += float(loss.item()) * yb.numel()\n",
    "            nobs       += yb.numel()\n",
    "\n",
    "        # Evaluate on dev if available, else on val\n",
    "        eval_df = cand_train_dev if cand_train_dev is not None and len(cand_train_dev)>0 else cand_val\n",
    "        val_recall, val_ndcg = eval_reranked(eval_df, split=\"dev\" if eval_df is cand_train_dev else \"val\")\n",
    "        print(f\"Epoch {ep:02d} | train BCE {total_loss/max(1,nobs):.4f} | \"\n",
    "              f\"eval Recall@{CFG['eval_topk']} {val_recall:.4f} | eval NDCG@{CFG['eval_topk']} {val_ndcg:.4f}\")\n",
    "        if val_recall > best_recall + 1e-4:\n",
    "            best_recall, bad = val_recall, 0\n",
    "            torch.save(model.state_dict(), OUT_DIR/'ranker_best.pt')\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= CFG[\"patience\"]:\n",
    "                print(\"Early stopping.\"); break\n",
    "    print(\"Best eval Recall@{} = {:.4f}\".format(CFG[\"eval_topk\"], best_recall))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING RANKER\")\n",
    "print(\"=\"*50)\n",
    "train_ranker()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "model.load_state_dict(torch.load(OUT_DIR/'ranker_best.pt', map_location=device, weights_only=True))\n",
    "# Final metrics strictly on validation, then test\n",
    "val_recall, val_ndcg = eval_reranked(cand_val, split=\"val\")\n",
    "print(f\"VAL  — Recall@{CFG['eval_topk']}: {val_recall:.4f}, NDCG@{CFG['eval_topk']}: {val_ndcg:.4f}\")\n",
    "\n",
    "test_recall, test_ndcg = eval_reranked(cand_test, split=\"test\")\n",
    "print(f\"TEST — Recall@{CFG['eval_topk']}: {test_recall:.4f}, NDCG@{CFG['eval_topk']}: {test_ndcg:.4f}\")\n",
    "\n",
    "# Inline Coverage@K and summary outputs right after training/evaluation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "K = int(CFG[\"eval_topk\"]) if isinstance(CFG.get(\"eval_topk\", 10), (int, np.integer)) else 10\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_lists(cand_df, topk=10, batch_size=1024):\n",
    "    lists = []\n",
    "    for i in range(0, len(cand_df), batch_size):\n",
    "        batch_df = cand_df.iloc[i:i+batch_size]\n",
    "        H = build_hist_tensor(batch_df['history_idx'].astype(str).tolist(), CFG[\"hist_max\"]).to(device)\n",
    "        P = torch.tensor(batch_df['pos_item_idx'].astype(int).tolist(), dtype=torch.long, device=device)\n",
    "        C = torch.tensor([[int(x) for x in s.split()] for s in batch_df['cands'].astype(str).tolist()], dtype=torch.long, device=device)\n",
    "        U = user_vecs_from_hist_batch(H)\n",
    "        feats = _pack_batch_features(U, H, C, P)\n",
    "        X = torch.stack([feats[c] for c in RANKER_COLS], dim=-1).view(-1, len(RANKER_COLS))\n",
    "        scores = model(X).view(len(feats[\"label\"]), -1)\n",
    "        t = min(topk, C.size(1))\n",
    "        _, idx = torch.topk(scores, k=t, dim=1)\n",
    "        for j in range(len(batch_df)):\n",
    "            lists.append(C[j][idx[j]].tolist())\n",
    "    return lists\n",
    "\n",
    "def coverage_at_k(rec_lists, n_items):\n",
    "    seen_items = set()\n",
    "    for rec in rec_lists:\n",
    "        seen_items.update(int(x) for x in rec[:K])\n",
    "    return len(seen_items) / float(n_items)\n",
    "\n",
    "# Reranker Coverage@K on TEST and VAL\n",
    "rr_lists_test = rerank_lists(cand_test, topk=K)\n",
    "rr_cov = coverage_at_k(rr_lists_test, len(item_map))\n",
    "rr_lists_val = rerank_lists(cand_val, topk=K)\n",
    "rr_cov_val = coverage_at_k(rr_lists_val, len(item_map))\n",
    "# Print combined metrics with coverage\n",
    "print(f\"VAL  — Recall@{K}: {val_recall:.4f}, NDCG@{K}: {val_ndcg:.4f}, Coverage@{K}: {rr_cov_val:.4f}\")\n",
    "print(f\"TEST — Recall@{K}: {test_recall:.4f}, NDCG@{K}: {test_ndcg:.4f}, Coverage@{K}: {rr_cov:.4f}\")\n",
    "\n",
    "# Best Baseline (ItemKNN) — minimal rebuild and evaluation on TEST\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def build_matrix_from_sequences(seq_df, n_users, n_items):\n",
    "    rows, cols, vals = [], [], []\n",
    "    for _, r in seq_df.iterrows():\n",
    "        u = int(r['user_idx']); p = int(r['pos_item_idx'])\n",
    "        rows.append(u); cols.append(p); vals.append(1.0)\n",
    "        if pd.notna(r['history_idx']) and r['history_idx']:\n",
    "            for it in map(int, str(r['history_idx']).split()):\n",
    "                rows.append(u); cols.append(it); vals.append(0.5)\n",
    "    return csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "n_users = len(customer_map); n_items = len(item_map)\n",
    "train_mat = build_matrix_from_sequences(seq_train, n_users, n_items)\n",
    "\n",
    "try:\n",
    "    interactions_path = OUT_DIR / 'interactions_clean.parquet'\n",
    "    if interactions_path.exists():\n",
    "        interactions = pd.read_parquet(interactions_path, engine='fastparquet')\n",
    "        interactions = interactions.merge(item_map, on='stock_code', how='inner')\n",
    "        interactions = interactions.merge(customer_map, on='customer_id', how='inner')\n",
    "        cutoff = pd.to_datetime(seq_train['ts'].max()) if 'ts' in seq_train.columns else None\n",
    "        if cutoff is not None and 'invoice_date' in interactions.columns:\n",
    "            interactions = interactions[interactions['invoice_date'] <= cutoff]\n",
    "        rows = interactions['user_idx'].astype(int).to_numpy()\n",
    "        cols = interactions['item_idx'].astype(int).to_numpy()\n",
    "        vals = np.ones_like(rows, dtype='float32')\n",
    "        knn_mat = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "    else:\n",
    "        knn_mat = train_mat\n",
    "except Exception:\n",
    "    knn_mat = train_mat\n",
    "\n",
    "item_sim = cosine_similarity(knn_mat.T)\n",
    "\n",
    "def itemknn_recommend(u_vec, k=10):\n",
    "    scores = u_vec @ item_sim\n",
    "    seen = np.where(u_vec > 0)[0]\n",
    "    scores[seen] = -1e12\n",
    "    return np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "user_profiles = train_mat.toarray()\n",
    "\n",
    "iknn_recs = []\n",
    "iknn_hits, iknn_dcg = 0, 0.0\n",
    "for _, r in seq_test.iterrows():\n",
    "    u = int(r['user_idx']); true = int(r['pos_item_idx'])\n",
    "    recs = itemknn_recommend(user_profiles[u], k=K)\n",
    "    iknn_recs.append(recs.tolist())\n",
    "    if true in recs:\n",
    "        iknn_hits += 1\n",
    "        rank = list(recs).index(true) + 1\n",
    "        iknn_dcg += 1.0 / np.log2(rank + 1.0)\n",
    "\n",
    "iknn_recall = iknn_hits / max(1, len(seq_test))\n",
    "iknn_ndcg = iknn_dcg / max(1, len(seq_test))\n",
    "\n",
    "def coverage_from_lists(rec_lists, n_items):\n",
    "    seen = set()\n",
    "    for rec in rec_lists:\n",
    "        seen.update(rec[:K])\n",
    "    return len(seen) / float(n_items)\n",
    "\n",
    "iknn_cov = coverage_from_lists(iknn_recs, n_items)\n",
    "\n",
    "# Persist single-row CSV and TXT summary next to other artifacts\n",
    "summary_dir = OUT_DIR\n",
    "csv_path = summary_dir / 'summary_metrics.csv'\n",
    "txt_path = summary_dir / 'summary_notes.txt'\n",
    "\n",
    "import csv\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\",\"Recall@10\",\"NDCG@10\",\"Coverage@10\",\"Notes\"])\n",
    "    delta_ndcg = float(test_ndcg) - float(iknn_ndcg)\n",
    "    notes = f\"Two-Stage Ranker vs ItemKNN: delta NDCG@{K}={delta_ndcg:+.4f}\"\n",
    "    writer.writerow([\"Two-Stage Ranker\", f\"{float(test_recall):.4f}\", f\"{float(test_ndcg):.4f}\", f\"{rr_cov:.4f}\", notes])\n",
    "\n",
    "with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "    # Use ASCII-friendly delta notation to avoid Windows charmaps issues\n",
    "    delta_str = f\"+{delta_ndcg:.4f}\" if delta_ndcg >= 0 else f\"{delta_ndcg:.4f}\"\n",
    "    f.write(f\"Two-Stage Ranker: best Recall@{K} ({float(test_recall):.4f}); NDCG@{K} ({float(test_ndcg):.4f}, delta vs ItemKNN {delta_str}).\\n\")\n",
    "    cov_delta = rr_cov - iknn_cov\n",
    "    sign = '+' if cov_delta >= 0 else ''\n",
    "    f.write(f\"        - Two-Stage Ranker: Coverage@{K} up to {rr_cov*100:.2f}%, delta vs ItemKNN {sign}{cov_delta*100:.2f} pp.\\n\")\n",
    "    if abs(float(test_recall) - float(iknn_recall)) < 1e-6:\n",
    "        cov_delta_str = f\"+{cov_delta:.4f}\" if cov_delta >= 0 else f\"{cov_delta:.4f}\"\n",
    "        f.write(f\"        - Two-Stage Ranker: ties Recall@{K}; improves NDCG@{K} ({delta_str}) and Coverage@{K} ({cov_delta_str}).\\n\")\n",
    "\n",
    "print(f\"Saved CSV to: {csv_path}\")\n",
    "print(f\"Saved TXT to: {txt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4839258-9000-4f96-9f17-bdd590c75882",
   "metadata": {},
   "source": [
    "## 7. Comparison with Baselines\n",
    "- Load precomputed baseline results\n",
    "- Print Recall@K for each baseline and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6401ed5-96bb-46b5-8c89-afe745558d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPARISON WITH BASELINES\n",
      "==================================================\n",
      "Baseline Results (Held-out Interactions):\n",
      "  Popularity: {'K=5': {'Recall': 0.0650887573964497, 'NDCG': 0.03907164430021704, 'MRR': 0.030276134122287968}, 'K=10': {'Recall': 0.09467455621301775, 'NDCG': 0.04883659939283832, 'MRR': 0.03442519019442097}, 'K=20': {'Recall': 0.1301775147928994, 'NDCG': 0.05818557686424207, 'MRR': 0.037193427873901244}}\n",
      "  ItemKNN: {'K=5': {'Recall': 0.09467455621301775, 'NDCG': 0.09467455621301775, 'MRR': 0.09467455621301775}, 'K=10': {'Recall': 0.11242603550295859, 'NDCG': 0.10029484145433752, 'MRR': 0.0969169719169719}, 'K=20': {'Recall': 0.15384615384615385, 'NDCG': 0.1107932700360795, 'MRR': 0.09981957253445888}}\n",
      "  UserKNN: {'K=5': {'Recall': 0.005917159763313609, 'NDCG': 0.005917159763313609, 'MRR': 0.005917159763313609}, 'K=10': {'Recall': 0.011834319526627219, 'NDCG': 0.007889546351084813, 'MRR': 0.006762468300929839}, 'K=20': {'Recall': 0.01775147928994083, 'NDCG': 0.00930855502900275, 'MRR': 0.007110536522301228}}\n",
      "  MatrixFactorization: {'K=5': {'Recall': 0.0, 'NDCG': 0.0, 'MRR': 0.0}, 'K=10': {'Recall': 0.005917159763313609, 'NDCG': 0.0017104427592774432, 'MRR': 0.000591715976331361}, 'K=20': {'Recall': 0.005917159763313609, 'NDCG': 0.0017104427592774432, 'MRR': 0.000591715976331361}}\n",
      "No baseline results found for comparison\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH BASELINES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "baseline_results_path = OUT_DIR/'baseline_results.json'\n",
    "if baseline_results_path.exists():\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    print(\"Baseline Results (Held-out Interactions):\")\n",
    "    for model_name, results in baseline_results['held_out_interactions'].items():\n",
    "        print(f\"  {model_name}: {results}\")\n",
    "    # Support both old and new baseline schema\n",
    "try:\n",
    "    k_key = f\"K={CFG['eval_topk']}\"\n",
    "    best_baseline = max(\n",
    "        [(m, r[k_key]['Recall']) for m,r in baseline_results['held_out_interactions'].items() if k_key in r],\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    best_name, best_val = best_baseline\n",
    "except Exception:\n",
    "    rk = f\"Recall@{CFG['eval_topk']}\"\n",
    "    best_baseline = max(\n",
    "        [(m, r.get(rk, -1.0)) for m,r in baseline_results['held_out_interactions'].items()],\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    best_name, best_val = best_baseline\n",
    "    print(f\"\\nBest baseline ({best_name}): Recall@{CFG['eval_topk']} = {best_val:.4f}\")\n",
    "    print(f\"Two-Stage Ranker: Recall@{CFG['eval_topk']} = {test_recall:.4f}\")\n",
    "    improvement = ((test_recall - best_val) / best_val) * 100 if best_val>0 else 0\n",
    "    print(f\"Improvement: {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(\"No baseline results found for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7160bea",
   "metadata": {},
   "source": [
    "## 9. Results Saving\n",
    "\n",
    "- Persist final test metrics and model configuration to JSON  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c73083c-539d-4579-9ba7-558da7f6721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results to ..\\data\\processed\\jarir\\ranker_results.json\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'model_config': CFG,\n",
    "    'evaluation_results': {\n",
    "        'test_recall': test_recall,\n",
    "        'test_ndcg': test_ndcg,\n",
    "        'eval_topk': CFG['eval_topk']\n",
    "    },\n",
    "    'data_stats': {\n",
    "        'n_users': len(customer_map),\n",
    "        'n_items': len(item_map),\n",
    "        'train_sequences': len(seq_train),\n",
    "        'val_sequences': len(seq_val),\n",
    "        'test_sequences': len(seq_test)\n",
    "    }\n",
    "}\n",
    "with open(OUT_DIR/'ranker_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\nSaved results to {OUT_DIR/'ranker_results.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425b020",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u0394' in position 67: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    119\u001b[39m     writer = csv.writer(f)\n\u001b[32m    120\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mRecall@10\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mNDCG@10\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mCoverage@10\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mNotes\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTwo-Stage Ranker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrr_recall\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrr_ndcg\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrr_cov\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m; \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Write TXT with formatted bullets\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(txt_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yazan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs.charmap_encode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m.errors,encoding_table)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode character '\\u0394' in position 67: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# 11. Coverage@10 reporting and summaries (TEST split)\n",
    "# - Compute Coverage@10 for Two-Stage Ranker (reranked candidates)\n",
    "# - Compute Coverage@10 for best baseline (ItemKNN, rebuilt locally)\n",
    "# - Write single-row CSV and a TXT summary with deltas vs baseline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "K = int(CFG[\"eval_topk\"]) if isinstance(CFG.get(\"eval_topk\", 10), (int, np.integer)) else 10\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_lists(cand_df, topk=10, batch_size=1024):\n",
    "    lists = []\n",
    "    for i in range(0, len(cand_df), batch_size):\n",
    "        batch_df = cand_df.iloc[i:i+batch_size]\n",
    "        H = build_hist_tensor(batch_df['history_idx'].astype(str).tolist(), CFG[\"hist_max\"]).to(device)\n",
    "        P = torch.tensor(batch_df['pos_item_idx'].astype(int).tolist(), dtype=torch.long, device=device)\n",
    "        C = torch.tensor([[int(x) for x in s.split()] for s in batch_df['cands'].astype(str).tolist()], dtype=torch.long, device=device)\n",
    "        U = user_vecs_from_hist_batch(H)\n",
    "        feats = _pack_batch_features(U, H, C, P)\n",
    "        X = torch.stack([feats[c] for c in RANKER_COLS], dim=-1).view(-1, len(RANKER_COLS))\n",
    "        scores = model(X).view(len(feats[\"label\"]), -1)\n",
    "        t = min(topk, C.size(1))\n",
    "        _, idx = torch.topk(scores, k=t, dim=1)\n",
    "        for j in range(len(batch_df)):\n",
    "            lists.append(C[j][idx[j]].tolist())\n",
    "    return lists\n",
    "\n",
    "def coverage_at_k(rec_lists, n_items):\n",
    "    seen_items = set()\n",
    "    for rec in rec_lists:\n",
    "        seen_items.update(int(x) for x in rec[:K])\n",
    "    return len(seen_items) / float(n_items)\n",
    "\n",
    "# 11.1 Reranker Coverage@10 on TEST\n",
    "rr_lists_test = rerank_lists(cand_test, topk=K)\n",
    "rr_cov = coverage_at_k(rr_lists_test, len(item_map))\n",
    "rr_recall = float(test_recall)\n",
    "rr_ndcg = float(test_ndcg)\n",
    "\n",
    "# 11.2 Best Baseline (ItemKNN) — rebuild minimal and evaluate on TEST\n",
    "# Build training matrix from sequences (history 0.5, positive 1.0)\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def build_matrix_from_sequences(seq_df, n_users, n_items):\n",
    "    rows, cols, vals = [], [], []\n",
    "    for _, r in seq_df.iterrows():\n",
    "        u = int(r['user_idx']); p = int(r['pos_item_idx'])\n",
    "        rows.append(u); cols.append(p); vals.append(1.0)\n",
    "        if pd.notna(r['history_idx']) and r['history_idx']:\n",
    "            for it in map(int, str(r['history_idx']).split()):\n",
    "                rows.append(u); cols.append(it); vals.append(0.5)\n",
    "    return csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "n_users = len(customer_map); n_items = len(item_map)\n",
    "train_mat = build_matrix_from_sequences(seq_train, n_users, n_items)\n",
    "\n",
    "# Compute item-item cosine similarity on a denser matrix using interactions before test cutoff if available\n",
    "try:\n",
    "    interactions_path = OUT_DIR / 'interactions_clean.parquet'\n",
    "    if interactions_path.exists():\n",
    "        interactions = pd.read_parquet(interactions_path, engine='fastparquet')\n",
    "        interactions = interactions.merge(item_map, on='stock_code', how='inner')\n",
    "        interactions = interactions.merge(customer_map, on='customer_id', how='inner')\n",
    "        cutoff = pd.to_datetime(seq_train['ts'].max()) if 'ts' in seq_train.columns else None\n",
    "        if cutoff is not None and 'invoice_date' in interactions.columns:\n",
    "            interactions = interactions[interactions['invoice_date'] <= cutoff]\n",
    "        rows = interactions['user_idx'].astype(int).to_numpy()\n",
    "        cols = interactions['item_idx'].astype(int).to_numpy()\n",
    "        vals = np.ones_like(rows, dtype='float32')\n",
    "        knn_mat = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "    else:\n",
    "        knn_mat = train_mat\n",
    "except Exception:\n",
    "    knn_mat = train_mat\n",
    "\n",
    "item_sim = cosine_similarity(knn_mat.T)\n",
    "item_pop = np.array(knn_mat.sum(axis=0)).flatten().astype(np.float32)\n",
    "if item_pop.max() > 0:\n",
    "    item_pop /= item_pop.max()\n",
    "\n",
    "def itemknn_recommend(u_vec, k=10):\n",
    "    scores = u_vec @ item_sim\n",
    "    seen = np.where(u_vec > 0)[0]\n",
    "    scores[seen] = -1e12\n",
    "    return np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "# Build user profiles from train_mat\n",
    "user_profiles = train_mat.toarray()\n",
    "\n",
    "# Evaluate ItemKNN on TEST interactions\n",
    "iknn_recs = []\n",
    "iknn_hits, iknn_dcg = 0, 0.0\n",
    "for _, r in seq_test.iterrows():\n",
    "    u = int(r['user_idx']); true = int(r['pos_item_idx'])\n",
    "    recs = itemknn_recommend(user_profiles[u], k=K)\n",
    "    iknn_recs.append(recs.tolist())\n",
    "    if true in recs:\n",
    "        iknn_hits += 1\n",
    "        rank = list(recs).index(true) + 1\n",
    "        iknn_dcg += 1.0 / np.log2(rank + 1.0)\n",
    "\n",
    "iknn_recall = iknn_hits / max(1, len(seq_test))\n",
    "iknn_ndcg = iknn_dcg / max(1, len(seq_test))\n",
    "iknn_cov = coverage_at_k(iknn_recs, n_items)\n",
    "\n",
    "# 11.3 Produce CSV (single row) and TXT summary\n",
    "summary_dir = OUT_DIR\n",
    "csv_path = summary_dir / 'summary_metrics.csv'\n",
    "txt_path = summary_dir / 'summary_notes.txt'\n",
    "\n",
    "notes = []\n",
    "# Delta vs baseline\n",
    "delta_ndcg = rr_ndcg - iknn_ndcg\n",
    "notes.append(f\"Two-Stage Ranker vs ItemKNN: delta NDCG@{K}={delta_ndcg:+.4f}\")\n",
    "\n",
    "# Write CSV\n",
    "import csv\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\",\"Recall@10\",\"NDCG@10\",\"Coverage@10\",\"Notes\"])\n",
    "    writer.writerow([\"Two-Stage Ranker\", f\"{rr_recall:.4f}\", f\"{rr_ndcg:.4f}\", f\"{rr_cov:.4f}\", \"; \".join(notes)])\n",
    "\n",
    "# Write TXT with formatted bullets\n",
    "with open(txt_path, 'w') as f:\n",
    "    f.write(f\"Two-Stage Ranker: best Recall@{K} ({rr_recall:.4f}); NDCG@{K} ({rr_ndcg:.4f}, Δ vs ItemKNN {delta_ndcg:+.4f}).\\n\")\n",
    "    cov_delta = rr_cov - iknn_cov\n",
    "    sign = '+' if cov_delta >= 0 else ''\n",
    "    f.write(f\"        - Two-Stage Ranker: Coverage@{K} up to {rr_cov*100:.2f}%, Δ vs ItemKNN {sign}{cov_delta*100:.2f} pp.\\n\")\n",
    "    # Optional tie/improve line\n",
    "    if abs(rr_recall - iknn_recall) < 1e-6:\n",
    "        f.write(f\"        - Two-Stage Ranker: ties Recall@{K}; improves NDCG@{K} ({delta_ndcg:+.4f}) and Coverage@{K} ({cov_delta:+.4f}).\\n\")\n",
    "\n",
    "print(f\"Saved CSV to: {csv_path}\")\n",
    "print(f\"Saved TXT to: {txt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
