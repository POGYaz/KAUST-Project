{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7ad8d3",
   "metadata": {},
   "source": [
    "\n",
    "# 04 — Ranker & End‑to‑End (**FAST GPU**) — Recall@10 / NDCG@10\n",
    "\n",
    "This version removes per‑sample I/O, uses **FAISS‑GPU batched searches**, **GPU feature building with sharded Parquet**, and **AMP** for ranker training.\n",
    "\n",
    "**Highlights**\n",
    "- Candidate generation on GPU (batched).\n",
    "- Feature engineering on GPU with **optional negative subsampling** (`neg_per_query`).\n",
    "- Ranker training per‑**shard** (load once → many batches), not per‑sample file reads.\n",
    "- Mixed precision (**torch.cuda.amp**), gradient clipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315665bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: /home/kamalyy/KAUST-Project/data/processed/online_retail_II\n",
      "Device: cuda\n",
      "Train/Val/Test: (597298, 6) (75190, 6) (75583, 6)\n",
      "Item vectors: (4446, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0) Config & paths --------------------------------------------------------\n",
    "import os, json, math, time, gc, glob, bisect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "PROJECT_ROOT = Path.home() / \"KAUST-Project\"   # /home/kamalyy/KAUST-Project\n",
    "OUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"online_retail_II\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "READ_KW  = dict(engine=\"fastparquet\")\n",
    "WRITE_KW = dict(engine=\"fastparquet\", index=False)\n",
    "\n",
    "CFG = {\n",
    "    # Retrieval\n",
    "    \"cand_topk\": 100,\n",
    "    \"cand_batch\": 8192,          # FAISS search batch size\n",
    "    # Histories & features\n",
    "    \"hist_max\": 50,\n",
    "    # Feature building\n",
    "    \"feat_batch_q\": 2048,        # queries per GPU batch when building features\n",
    "    \"shard_rows\": 2_000_000,     # approx rows per Parquet shard (features)\n",
    "    \"neg_per_query\": 20,         # keep 1 pos + N hard negatives per query (set None to keep all K)\n",
    "    \"hard_negatives\": True,      # choose hardest by dot_uv; False=random\n",
    "    # Ranker\n",
    "    \"batch_size\": 4096,          # larger thanks to AMP\n",
    "    \"epochs\": 15,\n",
    "    \"patience\": 3,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"hidden\": 512,\n",
    "    \"eval_topk\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"use_text\": True,\n",
    "    # FAISS GPU\n",
    "    \"faiss_use_all_gpus\": False, # set True to use all GPUs\n",
    "    \"faiss_device\": 0,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "# Sequences\n",
    "seq_train = pd.read_parquet(OUT_DIR/'sequences_train.parquet', **READ_KW)\n",
    "seq_val   = pd.read_parquet(OUT_DIR/'sequences_val.parquet', **READ_KW)\n",
    "seq_test  = pd.read_parquet(OUT_DIR/'sequences_test.parquet', **READ_KW)\n",
    "print(\"Train/Val/Test:\", seq_train.shape, seq_val.shape, seq_test.shape)\n",
    "\n",
    "# Popularity for features/fallback\n",
    "pop_counts = seq_train['pos_item_idx'].value_counts()\n",
    "pop_norm = (pop_counts - pop_counts.min()) / (pop_counts.max() - pop_counts.min() + 1e-9)\n",
    "\n",
    "# Item vectors from retriever (already normalized)\n",
    "V_PATH = OUT_DIR/'retriever_item_vectors.npy'\n",
    "assert V_PATH.exists(), \"Missing retriever_item_vectors.npy from notebook 03\"\n",
    "ITEM_VECS = np.load(V_PATH).astype('float32')\n",
    "n_items, d_vec = ITEM_VECS.shape\n",
    "print(\"Item vectors:\", ITEM_VECS.shape)\n",
    "\n",
    "# Optional text embeddings (normalize)\n",
    "TXT_PATH = OUT_DIR/'item_text_emb.npy'\n",
    "HAS_TEXT = (CFG[\"use_text\"] and TXT_PATH.exists())\n",
    "ITEM_TXT = np.load(TXT_PATH).astype('float32') if HAS_TEXT else None\n",
    "if HAS_TEXT:\n",
    "    ITEM_TXT /= np.maximum(np.linalg.norm(ITEM_TXT, axis=1, keepdims=True), 1e-6)\n",
    "    print(\"Item text emb:\", ITEM_TXT.shape)\n",
    "\n",
    "# GPU tensors for items\n",
    "ITEM_VECS_T = torch.from_numpy(ITEM_VECS).to(device, non_blocking=True)\n",
    "ITEM_TXT_T  = torch.from_numpy(ITEM_TXT).to(device, non_blocking=True) if HAS_TEXT else None\n",
    "\n",
    "# Popularity & optional price_z tensors\n",
    "pop_vec = torch.zeros(n_items, dtype=torch.float32, device=device)\n",
    "pop_idx = torch.tensor(pop_counts.index.values, dtype=torch.long, device=device)\n",
    "pop_val = torch.tensor(pop_norm.loc[pop_counts.index].values, dtype=torch.float32, device=device)\n",
    "pop_vec[pop_idx] = pop_val\n",
    "\n",
    "price_z = None\n",
    "items_clean_path = OUT_DIR/'items_clean.parquet'\n",
    "if items_clean_path.exists():\n",
    "    items_clean = pd.read_parquet(items_clean_path, **READ_KW)\n",
    "    if 'price_median' in items_clean.columns and 'item_idx' in items_clean.columns:\n",
    "        m = items_clean[['item_idx','price_median']].dropna()\n",
    "        mu, sigma = m['price_median'].mean(), m['price_median'].std() + 1e-6\n",
    "        z = ((m['price_median'] - mu) / sigma).astype(float)\n",
    "        price_z = torch.zeros(n_items, dtype=torch.float32, device=device)\n",
    "        ii = torch.tensor(m['item_idx'].astype(int).values, dtype=torch.long, device=device)\n",
    "        price_z[ii] = torch.tensor(z.values, dtype=torch.float32, device=device)\n",
    "\n",
    "def parse_hist(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    return [int(x) for x in s.strip().split()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c95688",
   "metadata": {},
   "source": [
    "## 1) FAISS‑GPU index & batched retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS GPU index ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_faiss_gpu_index():\n",
    "    import faiss\n",
    "    idx_meta_path = OUT_DIR/'index_meta.json'\n",
    "    cpu_index = None\n",
    "    if idx_meta_path.exists():\n",
    "        meta = json.load(open(idx_meta_path))\n",
    "        if meta.get(\"type\",\"\").startswith(\"faiss\"):\n",
    "            cpu_index = faiss.read_index(str(OUT_DIR/'items.faiss'))\n",
    "    if cpu_index is None:\n",
    "        d = ITEM_VECS.shape[1]\n",
    "        cpu_index = faiss.IndexFlatIP(d)\n",
    "        cpu_index.add(ITEM_VECS)\n",
    "    if CFG[\"faiss_use_all_gpus\"]:\n",
    "        return faiss.index_cpu_to_all_gpus(cpu_index)\n",
    "    else:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        return faiss.index_cpu_to_gpu(res, CFG[\"faiss_device\"], cpu_index)\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    ANN = load_faiss_gpu_index()\n",
    "    print(\"FAISS GPU index ready.\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] FAISS GPU init failed; CPU fallback:\", e)\n",
    "    ANN = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def user_vecs_from_hist_batch(hist_tensor):\n",
    "    # hist_tensor: LongTensor [B, L], with -1 as PAD → mean of valid item vectors\n",
    "    B, L = hist_tensor.shape\n",
    "    safe_idx = hist_tensor.clamp(min=0)  # replace -1 with 0 for gather\n",
    "    H = ITEM_VECS_T.index_select(0, safe_idx.view(-1)).view(B, L, -1)  # [B,L,d]\n",
    "    mask = (hist_tensor >= 0).float().unsqueeze(-1)                    # [B,L,1]\n",
    "    U = (H * mask).sum(1) / mask.sum(1).clamp_min(1e-6)                # [B,d]\n",
    "    U = F.normalize(U, dim=-1)\n",
    "    return U\n",
    "\n",
    "def build_hist_tensor(series, L):\n",
    "    B = len(series)\n",
    "    H = torch.full((B, L), -1, dtype=torch.long)\n",
    "    for i, s in enumerate(series):\n",
    "        h = parse_hist(s)\n",
    "        if len(h) > L: h = h[-L:]\n",
    "        if h:\n",
    "            H[i, -len(h):] = torch.tensor(h, dtype=torch.long)\n",
    "    return H\n",
    "\n",
    "def batched_faiss_search(U_np, topk, batch=8192):\n",
    "    if ANN is None:\n",
    "        with torch.no_grad():\n",
    "            U = torch.from_numpy(U_np).to(device)\n",
    "            sims = U @ ITEM_VECS_T.t()\n",
    "            return torch.topk(sims, k=topk, dim=1).indices.detach().cpu().numpy().astype('int32')\n",
    "    I_all = []\n",
    "    for i in range(0, U_np.shape[0], batch):\n",
    "        I = ANN.search(U_np[i:i+batch], topk)[1]\n",
    "        I_all.append(I)\n",
    "    return np.vstack(I_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4067e3",
   "metadata": {},
   "source": [
    "## 2) Candidate generation on GPU (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fedc25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: (597298, 4) (75190, 4) (75583, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen_candidates_gpu(df, topk=100, batch_q=8192):\n",
    "    hist_series = df['history_idx'].astype(str).tolist()\n",
    "    H = build_hist_tensor(hist_series, CFG[\"hist_max\"])  # CPU\n",
    "    U_chunks = []\n",
    "    for i in range(0, H.size(0), batch_q):\n",
    "        Ub = user_vecs_from_hist_batch(H[i:i+batch_q].to(device))\n",
    "        U_chunks.append(Ub.detach().cpu())\n",
    "    U = torch.cat(U_chunks, 0).numpy().astype('float32')\n",
    "\n",
    "    I_full = batched_faiss_search(U, topk=topk, batch=CFG[\"cand_batch\"])  # [N,topk]\n",
    "\n",
    "    pos_list = df['pos_item_idx'].astype(int).tolist()\n",
    "    ts_list  = df['ts'].astype(str).tolist() if 'ts' in df.columns else ['']*len(pos_list)\n",
    "    rows = []\n",
    "    for pos, cand_idx, ts, h_s in zip(pos_list, I_full, ts_list, hist_series):\n",
    "        cand = cand_idx.tolist()\n",
    "        if pos not in cand:\n",
    "            cand[-1] = int(pos)\n",
    "        rows.append((h_s, int(pos), \" \".join(map(str,cand)), ts))\n",
    "    return pd.DataFrame(rows, columns=['history_idx','pos_item_idx','cands','ts'])\n",
    "\n",
    "# Build & save if missing\n",
    "CAND_TRAIN_PATH = OUT_DIR/'candidates_train.parquet'\n",
    "CAND_VAL_PATH   = OUT_DIR/'candidates_val.parquet'\n",
    "CAND_TEST_PATH  = OUT_DIR/'candidates_test.parquet'\n",
    "\n",
    "if not (CAND_TRAIN_PATH.exists() and CAND_VAL_PATH.exists() and CAND_TEST_PATH.exists()):\n",
    "    print(\"Generating training candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_train, topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TRAIN_PATH, **WRITE_KW)\n",
    "    print(\"Generating validation candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_val,   topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_VAL_PATH, **WRITE_KW)\n",
    "    print(\"Generating test candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_test,  topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TEST_PATH, **WRITE_KW)\n",
    "\n",
    "cand_train = pd.read_parquet(CAND_TRAIN_PATH, **READ_KW)\n",
    "cand_val   = pd.read_parquet(CAND_VAL_PATH, **READ_KW)\n",
    "cand_test  = pd.read_parquet(CAND_TEST_PATH, **READ_KW)\n",
    "print(\"Candidates:\", cand_train.shape, cand_val.shape, cand_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b6bcf",
   "metadata": {},
   "source": [
    "## 3) Feature engineering on GPU (sharded) + optional negative subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3735e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard dirs: /home/kamalyy/KAUST-Project/data/processed/online_retail_II/ranker_feats_train_shards /home/kamalyy/KAUST-Project/data/processed/online_retail_II/ranker_feats_val_shards /home/kamalyy/KAUST-Project/data/processed/online_retail_II/ranker_feats_test_shards\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _pack_batch_features(Ub, Hb, Cb, Pb):\n",
    "    B, d = Ub.shape\n",
    "    K = Cb.size(1)\n",
    "    L = Hb.size(1)\n",
    "    # Candidate item vectors\n",
    "    Vc = ITEM_VECS_T.index_select(0, Cb.view(-1)).view(B, K, d)\n",
    "    # dot(u,v)\n",
    "    dot_uv = (Ub.unsqueeze(1) * Vc).sum(-1)                        # [B,K]\n",
    "    # Max sim to recent\n",
    "    safe_hist = Hb.clamp(min=0)\n",
    "    Hvec = ITEM_VECS_T.index_select(0, safe_hist.view(-1)).view(B, L, d)\n",
    "    Hvec = F.normalize(Hvec, dim=-1); Vc_n = F.normalize(Vc, dim=-1)\n",
    "    sims = torch.matmul(Hvec, Vc_n.transpose(1,2))                  # [B,L,K]\n",
    "    maskL = (Hb >= 0).unsqueeze(-1).float()\n",
    "    sims = sims + (maskL - 1.0) * 1e9\n",
    "    max_sim_recent = sims.max(dim=1).values                         # [B,K]\n",
    "    # pop & hist_len\n",
    "    pop = pop_vec.index_select(0, Cb.view(-1)).view(B, K)\n",
    "    hlen = (Hb >= 0).float().sum(1) / float(CFG[\"hist_max\"])\n",
    "    hlen = hlen.unsqueeze(1).expand(B, K)\n",
    "    # price_z\n",
    "    if isinstance(globals().get('price_z', None), torch.Tensor):\n",
    "        price = price_z.index_select(0, Cb.view(-1)).view(B, K)\n",
    "    else:\n",
    "        price = torch.zeros((B, K), device=Ub.device)\n",
    "    # text_sim\n",
    "    if ITEM_TXT_T is not None:\n",
    "        Th = ITEM_TXT_T.index_select(0, safe_hist.view(-1)).view(B, L, -1)\n",
    "        mask = (Hb >= 0).float().unsqueeze(-1)\n",
    "        Th_mean = (Th * mask).sum(1) / mask.sum(1).clamp_min(1e-6)\n",
    "        Tc = ITEM_TXT_T.index_select(0, Cb.view(-1)).view(B, K, -1)\n",
    "        text_sim = torch.matmul(Th_mean.unsqueeze(1), Tc.transpose(1,2)).squeeze(1)\n",
    "    else:\n",
    "        text_sim = torch.zeros((B, K), device=Ub.device)\n",
    "    # labels\n",
    "    labels = (Cb == Pb.view(-1,1)).float()\n",
    "    return {\"dot_uv\": dot_uv, \"max_sim_recent\": max_sim_recent, \"pop\": pop,\n",
    "            \"hist_len\": hlen, \"price_z\": price, \"text_sim\": text_sim,\n",
    "            \"label\": labels, \"item_idx\": Cb.float()}\n",
    "\n",
    "def _select_negatives(feats):\n",
    "    # Keep 1 positive + N negatives per query if configured\n",
    "    if CFG[\"neg_per_query\"] is None:\n",
    "        return feats\n",
    "    B, K = feats[\"label\"].shape\n",
    "    pos_col = torch.argmax(feats[\"label\"], dim=1, keepdim=True)     # [B,1]\n",
    "    if CFG[\"hard_negatives\"]:\n",
    "        neg_scores = feats[\"dot_uv\"].clone()\n",
    "        neg_scores.scatter_(1, pos_col, -1e9)\n",
    "        _, neg_idx = torch.topk(neg_scores, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    else:\n",
    "        rnd = torch.rand_like(feats[\"dot_uv\"])\n",
    "        rnd.scatter_(1, pos_col, 1e9)\n",
    "        _, neg_idx = torch.topk(-rnd, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    keep_cols = torch.cat([pos_col, neg_idx], dim=1)                # [B,1+N]\n",
    "    for k in [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\",\"text_sim\",\"label\",\"item_idx\"]:\n",
    "        feats[k] = torch.gather(feats[k], 1, keep_cols)\n",
    "    return feats\n",
    "\n",
    "def build_feats_gpu_sharded(cand_df, split_name):\n",
    "    N = len(cand_df)\n",
    "    L = CFG[\"hist_max\"]\n",
    "    bq = CFG[\"feat_batch_q\"]\n",
    "    out_dir = OUT_DIR / f\"ranker_feats_{split_name}_shards\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def write_shard(idx, feats_dict):\n",
    "        cpu = {k: v.detach().float().view(-1).to('cpu').numpy() for k,v in feats_dict.items()}\n",
    "        df = pd.DataFrame(cpu); df['item_idx'] = df['item_idx'].astype(np.int32)\n",
    "        df.to_parquet(out_dir / f\"part_{idx:03d}.parquet\", **WRITE_KW)\n",
    "\n",
    "    hist_series = cand_df['history_idx'].astype(str).tolist()\n",
    "    pos_list = cand_df['pos_item_idx'].astype(int).tolist()\n",
    "    cands_series = cand_df['cands'].astype(str).tolist()\n",
    "\n",
    "    rows_written = 0; shard_idx = 0; buf = None\n",
    "    for i in range(0, N, bq):\n",
    "        H = build_hist_tensor(hist_series[i:i+bq], L).to(device, non_blocking=True)\n",
    "        P = torch.tensor(pos_list[i:i+bq], dtype=torch.long, device=device)\n",
    "        C = torch.tensor([[int(x) for x in s.split()] for s in cands_series[i:i+bq]],\n",
    "                         dtype=torch.long, device=device)\n",
    "        U = user_vecs_from_hist_batch(H)\n",
    "        feats = _pack_batch_features(U, H, C, P)\n",
    "        feats = _select_negatives(feats)\n",
    "\n",
    "        if buf is None:\n",
    "            buf = {k: v.detach().clone() for k,v in feats.items()}\n",
    "        else:\n",
    "            for k in buf.keys():\n",
    "                buf[k] = torch.cat([buf[k], feats[k]], dim=0)\n",
    "\n",
    "        rows_in_buf = int(buf[\"label\"].numel())\n",
    "        if rows_in_buf >= CFG[\"shard_rows\"]:\n",
    "            write_shard(shard_idx, buf); shard_idx += 1\n",
    "            for k in list(buf.keys()): del buf[k]\n",
    "            buf = None; torch.cuda.empty_cache()\n",
    "\n",
    "        rows_written += int(feats[\"label\"].numel())\n",
    "        if (i//bq) % 20 == 0:\n",
    "            print(f\"[{split_name}] Built ~{rows_written/1e6:.2f}M rows...\")\n",
    "\n",
    "    if buf is not None:\n",
    "        write_shard(shard_idx, buf); shard_idx += 1\n",
    "        for k in list(buf.keys()): del buf[k]\n",
    "        buf = None; torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"[{split_name}] Done. Rows ~{rows_written:,}. Shards -> {out_dir}\")\n",
    "    return out_dir\n",
    "\n",
    "# Build shards if missing\n",
    "train_shards_dir = OUT_DIR / \"ranker_feats_train_shards\"\n",
    "val_shards_dir   = OUT_DIR / \"ranker_feats_val_shards\"\n",
    "test_shards_dir  = OUT_DIR / \"ranker_feats_test_shards\"\n",
    "\n",
    "if not train_shards_dir.exists():\n",
    "    train_shards_dir = build_feats_gpu_sharded(cand_train, \"train\")\n",
    "if not val_shards_dir.exists():\n",
    "    val_shards_dir = build_feats_gpu_sharded(cand_val, \"val\")\n",
    "if not test_shards_dir.exists():\n",
    "    test_shards_dir = build_feats_gpu_sharded(cand_test, \"test\")\n",
    "\n",
    "print(\"Shard dirs:\", train_shards_dir, val_shards_dir, test_shards_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec932e",
   "metadata": {},
   "source": [
    "## 4) Ranker (per‑shard training, AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb56e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANKER_COLS = [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\",\"text_sim\"]\n",
    "\n",
    "def shard_batches(files, batch_size):\n",
    "    for f in sorted(glob.glob(str(Path(files)/\"part_*.parquet\")) if isinstance(files, (str, Path)) else files):\n",
    "        df = pd.read_parquet(f, engine=\"fastparquet\", columns=RANKER_COLS+[\"label\"])\n",
    "\n",
    "        # ✅ build tensors without non_blocking, then .to(device, non_blocking=True)\n",
    "        X_np = df[RANKER_COLS].to_numpy(dtype='float32', copy=False)\n",
    "        y_np = df[\"label\"].to_numpy(dtype='float32', copy=False)\n",
    "        X = torch.from_numpy(X_np).to(device, non_blocking=True)\n",
    "        y = torch.from_numpy(y_np).to(device, non_blocking=True)\n",
    "\n",
    "        perm = torch.randperm(X.size(0), device=device)\n",
    "        for i in range(0, X.size(0), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            yield X[idx], y[idx]\n",
    "\n",
    "        del X, y, df\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class RankerMLP(nn.Module):\n",
    "    def __init__(self, d_in, hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "model = RankerMLP(len(RANKER_COLS), hidden=CFG[\"hidden\"], dropout=CFG[\"dropout\"]).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "use_amp = (device.type == \"cuda\")\n",
    "amp_device = \"cuda\" if use_amp else \"cpu\"\n",
    "\n",
    "# New GradScaler (old: torch.cuda.amp.GradScaler)\n",
    "scaler = torch.amp.GradScaler(amp_device, enabled=use_amp)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76054b",
   "metadata": {},
   "source": [
    "## 5) Train & Eval (end‑to‑end Recall@10 / NDCG@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b67739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train BCE 0.0258 | val Recall@10 0.7633 | val NDCG@10 0.7210\n",
      "Epoch 02 | train BCE 0.0253 | val Recall@10 0.7628 | val NDCG@10 0.7213\n",
      "Epoch 03 | train BCE 0.0252 | val Recall@10 0.7482 | val NDCG@10 0.7064\n",
      "Epoch 04 | train BCE 0.0252 | val Recall@10 0.7508 | val NDCG@10 0.7099\n",
      "Early stopping on Recall@10.\n",
      "Best val Recall@10 = 0.7633\n",
      "TEST — Recall@10: 0.7978, NDCG@10: 0.7563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def recall_at_k(preds, truth): return float(truth in preds)\n",
    "def ndcg_at_k(preds, truth):\n",
    "    try:\n",
    "        r = preds.index(truth) + 1; return 1.0 / math.log2(r + 1.0)\n",
    "    except ValueError: return 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_one_batch(cand_df_slice):\n",
    "    H = build_hist_tensor(cand_df_slice['history_idx'].astype(str).tolist(), CFG[\"hist_max\"]).to(device)\n",
    "    P = torch.tensor(cand_df_slice['pos_item_idx'].astype(int).tolist(), dtype=torch.long, device=device)\n",
    "    C = torch.tensor([[int(x) for x in s.split()] for s in cand_df_slice['cands'].astype(str).tolist()],\n",
    "                     dtype=torch.long, device=device)\n",
    "    U = user_vecs_from_hist_batch(H)\n",
    "    feats = _pack_batch_features(U, H, C, P)\n",
    "    # No negative subsampling at inference: rank all provided candidates\n",
    "    X = torch.stack([feats[c] for c in RANKER_COLS], dim=-1) # [B,K,6]\n",
    "    scores = model(X.view(-1, len(RANKER_COLS))).view(X.size(0), X.size(1))\n",
    "    topk = min(CFG[\"eval_topk\"], C.size(1))\n",
    "    vals, idx = torch.topk(scores, k=topk, dim=1)\n",
    "    reranked = [C[i][idx[i]].tolist() for i in range(C.size(0))]\n",
    "    return reranked\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reranked(cand_df, split=\"val\"):\n",
    "    model.eval()\n",
    "    hits = 0; ndcgs = 0.0; tot = 0\n",
    "    B = 2048\n",
    "    for i in range(0, len(cand_df), B):\n",
    "        batch_df = cand_df.iloc[i:i+B]\n",
    "        reranked = rerank_one_batch(batch_df)\n",
    "        for pos, rr in zip(batch_df['pos_item_idx'].tolist(), reranked):\n",
    "            pos = int(pos)\n",
    "            hits += float(pos in rr)\n",
    "            if pos in rr:\n",
    "                r = rr.index(pos) + 1\n",
    "                ndcgs += 1.0 / math.log2(r + 1.0)\n",
    "            tot += 1\n",
    "    return hits/max(1,tot), ndcgs/max(1,tot)\n",
    "\n",
    "\n",
    "def train_ranker():\n",
    "    best_recall = -1.0; bad = 0\n",
    "    for ep in range(1, CFG[\"epochs\"]+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0; nobs = 0\n",
    "        for Xb, yb in shard_batches(train_shards_dir, CFG[\"batch_size\"]):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(amp_device, enabled=use_amp, dtype=torch.float16):\n",
    "                logits = model(Xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            total_loss += float(loss.item()) * yb.numel()\n",
    "            nobs += yb.numel()\n",
    "\n",
    "        # Evaluate every epoch (can change to every 2 for even faster)\n",
    "        val_recall, val_ndcg = eval_reranked(cand_val, split=\"val\")\n",
    "        print(f\"Epoch {ep:02d} | train BCE {total_loss/max(1,nobs):.4f} | \"\n",
    "              f\"val Recall@{CFG['eval_topk']} {val_recall:.4f} | val NDCG@{CFG['eval_topk']} {val_ndcg:.4f}\")\n",
    "        if val_recall > best_recall + 1e-4:\n",
    "            best_recall, bad = val_recall, 0\n",
    "            torch.save(model.state_dict(), OUT_DIR/'ranker_best.pt')\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= CFG[\"patience\"]:\n",
    "                print(\"Early stopping on Recall@10.\"); break\n",
    "    print(\"Best val Recall@{} = {:.4f}\".format(CFG[\"eval_topk\"], best_recall))\n",
    "\n",
    "train_ranker()\n",
    "model.load_state_dict(torch.load(OUT_DIR/'ranker_best.pt', map_location=device, weights_only=True))\n",
    "test_recall, test_ndcg = eval_reranked(cand_test, split=\"test\")\n",
    "print(\"TEST — Recall@{}: {:.4f}, NDCG@{}: {:.4f}\".format(CFG[\"eval_topk\"], test_recall, CFG[\"eval_topk\"], test_ndcg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
