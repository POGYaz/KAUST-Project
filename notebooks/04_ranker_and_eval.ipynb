{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1433b1d-84c7-41c1-9815-a0d00d3de4c0",
   "metadata": {},
   "source": [
    "# 04: Two-Stage Ranker Notebook\n",
    "\n",
    "This notebook trains and evaluates an MLP ranker on two-stage candidate lists for Jarir:\n",
    "\n",
    "- **Config & Imports**  \n",
    "- **Load Sequences & Maps**  \n",
    "- **Load Embeddings & Feature Tensors**  \n",
    "- **Candidate Generation**  \n",
    "- **Feature Engineering (sharded)**  \n",
    "- **Ranker Model Definition**  \n",
    "- **Training & Evaluation**  \n",
    "- **Comparison with Baselines**  \n",
    "- **Save Results**  \n",
    "- **Final Summary**  \n",
    "- **Feature Importance Analysis**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e818ed-68c6-498a-9bc6-ad58ecacd0d4",
   "metadata": {},
   "source": [
    "## 0. Config & Imports\n",
    "\n",
    "- Import standard libraries and PyTorch  \n",
    "- Set up data paths and I/O settings  \n",
    "- Define configuration dictionary `CFG`  \n",
    "- Initialize device and random seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea847173-dfef-4bfb-bdea-811588eb6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: ../data/processed/jarir\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 0) Config & Imports --------------------------------------------------------\n",
    "import os, json, math, time, gc, glob, bisect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Data paths\n",
    "OUT_DIR = Path('../data/processed/jarir/')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "READ_KW  = dict(engine=\"fastparquet\")\n",
    "WRITE_KW = dict(engine=\"fastparquet\", index=False)\n",
    "\n",
    "CFG = {\n",
    "    # Retrieval\n",
    "    \"cand_topk\": 100,\n",
    "    \"cand_batch\": 4096,          # Candidate generation batch size\n",
    "    # Histories & features\n",
    "    \"hist_max\": 15,               # Reduced for Jarir (smaller sequences)\n",
    "    # Feature building\n",
    "    \"feat_batch_q\": 1024,        # queries per GPU batch when building features\n",
    "    \"shard_rows\": 1_000_000,     # approx rows per Parquet shard (features)\n",
    "    \"neg_per_query\": 20,         # keep 1 pos + N hard negatives per query\n",
    "    \"hard_negatives\": True,      # choose hardest by dot_uv; False=random\n",
    "    # Ranker\n",
    "    \"batch_size\": 2048,          # larger thanks to AMP\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 5,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"hidden\": 256,               # Smaller for Jarir\n",
    "    \"eval_topk\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"use_text\": False,           # No text embeddings for Jarir\n",
    "    # Two-Tower embeddings\n",
    "    \"embedding_dim\": 256,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd643b2b-fb2f-45c9-a2c8-1182363988c0",
   "metadata": {},
   "source": [
    "## 1. Load Sequences & Maps\n",
    "- Load precomputed train/val/test sequence tables\n",
    "- Load item and customer ID maps\n",
    "- Compute popularity counts for baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f3f171-948e-434b-bd27-658102e18855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: (1108, 6) (169, 6) (160, 6)\n",
      "Items: 1735 Customers: 929\n"
     ]
    }
   ],
   "source": [
    "# Load sequences\n",
    "seq_train = pd.read_parquet(OUT_DIR/'sequences_train.parquet', **READ_KW)\n",
    "seq_val   = pd.read_parquet(OUT_DIR/'sequences_val.parquet', **READ_KW)\n",
    "seq_test  = pd.read_parquet(OUT_DIR/'sequences_test.parquet', **READ_KW)\n",
    "print(\"Train/Val/Test:\", seq_train.shape, seq_val.shape, seq_test.shape)\n",
    "\n",
    "# Load item and customer maps\n",
    "item_map     = pd.read_parquet(OUT_DIR/'item_id_map.parquet', **READ_KW)\n",
    "customer_map = pd.read_parquet(OUT_DIR/'customer_id_map.parquet', **READ_KW)\n",
    "print(\"Items:\", len(item_map), \"Customers:\", len(customer_map))\n",
    "\n",
    "# Popularity for features/fallback\n",
    "pop_counts = seq_train['pos_item_idx'].value_counts()\n",
    "pop_norm   = (pop_counts - pop_counts.min()) / (pop_counts.max() - pop_counts.min() + 1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fba0ca-e062-4c94-961e-49cc5ed6e1c1",
   "metadata": {},
   "source": [
    "## 2. Load Two-Tower Embeddings & Feature Tensors\n",
    "- Load or generate user/item embeddings from two‐tower model\n",
    "- Move embeddings to GPU tensors\n",
    "- Build popularity and price z‐score feature tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963a4dba-aefc-4ebc-a251-f9a47c60e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Tower embeddings loaded: (929, 256) (1735, 256)\n",
      "Price features loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Two-Tower embeddings\n",
    "USER_EMB_PATH = OUT_DIR/'user_embeddings.npy'\n",
    "ITEM_EMB_PATH = OUT_DIR/'item_embeddings.npy'\n",
    "\n",
    "if USER_EMB_PATH.exists() and ITEM_EMB_PATH.exists():\n",
    "    USER_EMB = np.load(USER_EMB_PATH).astype('float32')\n",
    "    ITEM_EMB = np.load(ITEM_EMB_PATH).astype('float32')\n",
    "    print(\"Two-Tower embeddings loaded:\", USER_EMB.shape, ITEM_EMB.shape)\n",
    "else:\n",
    "    print(\"❌ Missing Two-Tower embeddings. Please run notebook 03 first.\")\n",
    "    n_users = len(customer_map)\n",
    "    n_items = len(item_map)\n",
    "    USER_EMB = np.random.randn(n_users, CFG[\"embedding_dim\"]).astype('float32')\n",
    "    ITEM_EMB = np.random.randn(n_items, CFG[\"embedding_dim\"]).astype('float32')\n",
    "    USER_EMB /= np.linalg.norm(USER_EMB, axis=1, keepdims=True)\n",
    "    ITEM_EMB /= np.linalg.norm(ITEM_EMB, axis=1, keepdims=True)\n",
    "    print(\"Using dummy embeddings for testing\")\n",
    "\n",
    "# GPU tensors for embeddings\n",
    "USER_EMB_T = torch.from_numpy(USER_EMB).to(device, non_blocking=True)\n",
    "ITEM_EMB_T = torch.from_numpy(ITEM_EMB).to(device, non_blocking=True)\n",
    "\n",
    "# Popularity tensor\n",
    "pop_vec = torch.zeros(len(item_map), dtype=torch.float32, device=device)\n",
    "pop_idx = torch.tensor(pop_counts.index.values, dtype=torch.long, device=device)\n",
    "pop_val = torch.tensor(pop_norm.loc[pop_counts.index].values, dtype=torch.float32, device=device)\n",
    "pop_vec[pop_idx] = pop_val\n",
    "\n",
    "# Load item metadata for additional features\n",
    "items_clean_path = OUT_DIR/'items_clean.parquet'\n",
    "price_z = None\n",
    "if items_clean_path.exists():\n",
    "    items_clean = pd.read_parquet(items_clean_path, **READ_KW)\n",
    "    if 'price_median' in items_clean.columns:\n",
    "        m = items_clean[['stock_code','price_median']].dropna()\n",
    "        m = m.merge(item_map, on='stock_code', how='inner')\n",
    "        if len(m) > 0:\n",
    "            mu, sigma = m['price_median'].mean(), m['price_median'].std() + 1e-6\n",
    "            z = ((m['price_median'] - mu) / sigma).astype(float)\n",
    "            price_z = torch.zeros(len(item_map), dtype=torch.float32, device=device)\n",
    "            ii = torch.tensor(m['item_idx'].astype(int).values, dtype=torch.long, device=device)\n",
    "            price_z[ii] = torch.tensor(z.values, dtype=torch.float32, device=device)\n",
    "            print(\"Price features loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395ea4b-af43-42c2-9035-0b76af8bce0e",
   "metadata": {},
   "source": [
    "## 3. Candidate Generation\n",
    " \n",
    "- Define helpers to parse history strings and compute user vectors\n",
    "- Build tensor of history indices\n",
    "- Generate top‐K candidates per user from two‐tower embeddings\n",
    "- Save and reload candidate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b814f49f-b74a-4ee8-812e-41f2f0d732bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training candidates (GPU)...\n",
      "Generating validation candidates (GPU)...\n",
      "Generating test candidates (GPU)...\n",
      "Candidates: (1108, 4) (169, 4) (160, 4)\n"
     ]
    }
   ],
   "source": [
    "def parse_hist(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    return [int(x) for x in s.strip().split()]\n",
    "\n",
    "@torch.no_grad()\n",
    "def user_vecs_from_hist_batch(hist_tensor):\n",
    "    \"\"\"Compute user vectors from history using Two-Tower item embeddings\"\"\"\n",
    "    B, L = hist_tensor.shape\n",
    "    safe_idx = hist_tensor.clamp(min=0)\n",
    "    H = ITEM_EMB_T.index_select(0, safe_idx.view(-1)).view(B, L, -1)\n",
    "    mask = (hist_tensor >= 0).float().unsqueeze(-1)\n",
    "    U = (H * mask).sum(1) / mask.sum(1).clamp_min(1e-6)\n",
    "    return F.normalize(U, dim=-1)\n",
    "\n",
    "def build_hist_tensor(series, L):\n",
    "    \"\"\"Build history tensor from string series\"\"\"\n",
    "    B = len(series)\n",
    "    H = torch.full((B, L), -1, dtype=torch.long)\n",
    "    for i, s in enumerate(series):\n",
    "        h = parse_hist(s)\n",
    "        if len(h) > L: h = h[-L:]\n",
    "        if h:\n",
    "            H[i, -len(h):] = torch.tensor(h, dtype=torch.long)\n",
    "    return H\n",
    "\n",
    "def gen_candidates_gpu(df, topk=100, batch_q=4096):\n",
    "    \"\"\"Generate candidates using Two-Tower embeddings\"\"\"\n",
    "    hist_series = df['history_idx'].astype(str).tolist()\n",
    "    H = build_hist_tensor(hist_series, CFG[\"hist_max\"])\n",
    "    U_chunks = []\n",
    "    for i in range(0, H.size(0), batch_q):\n",
    "        Ub = user_vecs_from_hist_batch(H[i:i+batch_q].to(device))\n",
    "        U_chunks.append(Ub.cpu())\n",
    "    U = torch.cat(U_chunks, 0).numpy().astype('float32')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        U_t = torch.from_numpy(U).to(device)\n",
    "        sims = U_t @ ITEM_EMB_T.t()\n",
    "        _, top_k_indices = torch.topk(sims, k=topk, dim=1)\n",
    "        I_full = top_k_indices.cpu().numpy().astype('int32')\n",
    "\n",
    "    pos_list = df['pos_item_idx'].astype(int).tolist()\n",
    "    ts_list  = df['ts'].astype(str).tolist() if 'ts' in df.columns else ['']*len(pos_list)\n",
    "    rows = []\n",
    "    for pos, cand_idx, ts, h_s in zip(pos_list, I_full, ts_list, hist_series):\n",
    "        cand = cand_idx.tolist()\n",
    "        if pos not in cand:\n",
    "            cand[-1] = int(pos)\n",
    "        rows.append((h_s, int(pos), \" \".join(map(str,cand)), ts))\n",
    "    return pd.DataFrame(rows, columns=['history_idx','pos_item_idx','cands','ts'])\n",
    "\n",
    "# Build & save candidates if missing\n",
    "CAND_TRAIN_PATH = OUT_DIR/'candidates_train.parquet'\n",
    "CAND_VAL_PATH   = OUT_DIR/'candidates_val.parquet'\n",
    "CAND_TEST_PATH  = OUT_DIR/'candidates_test.parquet'\n",
    "\n",
    "if not (CAND_TRAIN_PATH.exists() and CAND_VAL_PATH.exists() and CAND_TEST_PATH.exists()):\n",
    "    print(\"Generating training candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_train, topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TRAIN_PATH, **WRITE_KW)\n",
    "    print(\"Generating validation candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_val,   topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_VAL_PATH,   **WRITE_KW)\n",
    "    print(\"Generating test candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_test,  topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TEST_PATH,  **WRITE_KW)\n",
    "\n",
    "cand_train = pd.read_parquet(CAND_TRAIN_PATH, **READ_KW)\n",
    "cand_val   = pd.read_parquet(CAND_VAL_PATH,   **READ_KW)\n",
    "cand_test  = pd.read_parquet(CAND_TEST_PATH,  **READ_KW)\n",
    "print(\"Candidates:\", cand_train.shape, cand_val.shape, cand_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8539d4-863a-42ce-9bf5-d0d248deaeb2",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering on GPU (Sharded)\n",
    "- Pack features (dot product, max-sim, popularity, hist-length, price z)\n",
    "- Subsample hard negatives per query\n",
    "- Shard features into Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0952832-47d6-4020-a5bc-f6029d3330ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Built ~0.02M rows...\n",
      "[train] Done. Rows ~23,268. Shards -> ../data/processed/jarir/ranker_feats_train_shards\n",
      "[val] Built ~0.00M rows...\n",
      "[val] Done. Rows ~3,549. Shards -> ../data/processed/jarir/ranker_feats_val_shards\n",
      "[test] Built ~0.00M rows...\n",
      "[test] Done. Rows ~3,360. Shards -> ../data/processed/jarir/ranker_feats_test_shards\n",
      "Shard dirs: ../data/processed/jarir/ranker_feats_train_shards ../data/processed/jarir/ranker_feats_val_shards ../data/processed/jarir/ranker_feats_test_shards\n"
     ]
    }
   ],
   "source": [
    "def _pack_batch_features(Ub, Hb, Cb, Pb):\n",
    "    \"\"\"Pack features for a batch of candidates\"\"\"\n",
    "    B, d = Ub.shape\n",
    "    K = Cb.size(1)\n",
    "    L = Hb.size(1)\n",
    "\n",
    "    Vc = ITEM_EMB_T.index_select(0, Cb.view(-1)).view(B, K, d)\n",
    "    dot_uv = (Ub.unsqueeze(1) * Vc).sum(-1)\n",
    "\n",
    "    safe_hist = Hb.clamp(min=0)\n",
    "    Hvec = ITEM_EMB_T.index_select(0, safe_hist.view(-1)).view(B, L, d)\n",
    "    Hvec = F.normalize(Hvec, dim=-1); Vc_n = F.normalize(Vc, dim=-1)\n",
    "    sims = torch.matmul(Hvec, Vc_n.transpose(1,2))\n",
    "    maskL = (Hb >= 0).unsqueeze(-1).float()\n",
    "    sims = sims + (maskL - 1.0) * 1e9\n",
    "    max_sim_recent = sims.max(dim=1).values\n",
    "\n",
    "    pop    = pop_vec.index_select(0, Cb.view(-1)).view(B, K)\n",
    "    hlen   = (Hb >= 0).float().sum(1)/float(CFG[\"hist_max\"])\n",
    "    hlen   = hlen.unsqueeze(1).expand(B, K)\n",
    "    price  = price_z.index_select(0, Cb.view(-1)).view(B, K) if isinstance(price_z, torch.Tensor) else torch.zeros((B,K), device=Ub.device)\n",
    "    labels = (Cb == Pb.view(-1,1)).float()\n",
    "\n",
    "    return {\"dot_uv\": dot_uv, \"max_sim_recent\": max_sim_recent, \"pop\": pop,\n",
    "            \"hist_len\": hlen, \"price_z\": price, \"label\": labels, \"item_idx\": Cb.float()}\n",
    "\n",
    "def _select_negatives(feats):\n",
    "    \"\"\"Keep 1 positive + N negatives per query if configured\"\"\"\n",
    "    if CFG[\"neg_per_query\"] is None:\n",
    "        return feats\n",
    "    B, K = feats[\"label\"].shape\n",
    "    pos_col = torch.argmax(feats[\"label\"], dim=1, keepdim=True)\n",
    "    if CFG[\"hard_negatives\"]:\n",
    "        neg_scores = feats[\"dot_uv\"].clone()\n",
    "        neg_scores.scatter_(1, pos_col, -1e9)\n",
    "        _, neg_idx = torch.topk(neg_scores, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    else:\n",
    "        rnd = torch.rand_like(feats[\"dot_uv\"])\n",
    "        rnd.scatter_(1, pos_col, 1e9)\n",
    "        _, neg_idx = torch.topk(-rnd, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    keep_cols = torch.cat([pos_col, neg_idx], dim=1)\n",
    "    for k in [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\",\"label\",\"item_idx\"]:\n",
    "        feats[k] = torch.gather(feats[k], 1, keep_cols)\n",
    "    return feats\n",
    "\n",
    "def build_feats_gpu_sharded(cand_df, split_name):\n",
    "    \"\"\"Build features on GPU and save to sharded Parquet files\"\"\"\n",
    "    N = len(cand_df)\n",
    "    L = CFG[\"hist_max\"]\n",
    "    bq = CFG[\"feat_batch_q\"]\n",
    "    out_dir = OUT_DIR / f\"ranker_feats_{split_name}_shards\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def write_shard(idx, feats_dict):\n",
    "        cpu = {k: v.detach().float().view(-1).cpu().numpy() for k,v in feats_dict.items()}\n",
    "        df = pd.DataFrame(cpu); df['item_idx'] = df['item_idx'].astype(np.int32)\n",
    "        df.to_parquet(out_dir / f\"part_{idx:03d}.parquet\", **WRITE_KW)\n",
    "\n",
    "    hist_series  = cand_df['history_idx'].astype(str).tolist()\n",
    "    pos_list     = cand_df['pos_item_idx'].astype(int).tolist()\n",
    "    cands_series = cand_df['cands'].astype(str).tolist()\n",
    "\n",
    "    rows_written = 0; shard_idx = 0; buf = None\n",
    "    for i in range(0, N, bq):\n",
    "        H = build_hist_tensor(hist_series[i:i+bq], L).to(device, non_blocking=True)\n",
    "        P = torch.tensor(pos_list[i:i+bq], dtype=torch.long, device=device)\n",
    "        C = torch.tensor([[int(x) for x in s.split()] for s in cands_series[i:i+bq]],\n",
    "                         dtype=torch.long, device=device)\n",
    "        U = user_vecs_from_hist_batch(H)\n",
    "        feats = _pack_batch_features(U, H, C, P)\n",
    "        feats = _select_negatives(feats)\n",
    "\n",
    "        if buf is None:\n",
    "            buf = {k: v.detach().clone() for k,v in feats.items()}\n",
    "        else:\n",
    "            for k in buf:\n",
    "                buf[k] = torch.cat([buf[k], feats[k]], dim=0)\n",
    "\n",
    "        rows_in_buf = int(buf[\"label\"].numel())\n",
    "        if rows_in_buf >= CFG[\"shard_rows\"]:\n",
    "            write_shard(shard_idx, buf); shard_idx += 1\n",
    "            buf = None; torch.cuda.empty_cache()\n",
    "\n",
    "        rows_written += int(feats[\"label\"].numel())\n",
    "        if (i//bq) % 10 == 0:\n",
    "            print(f\"[{split_name}] Built ~{rows_written/1e6:.2f}M rows...\")\n",
    "\n",
    "    if buf is not None:\n",
    "        write_shard(shard_idx, buf)\n",
    "\n",
    "    print(f\"[{split_name}] Done. Rows ~{rows_written:,}. Shards -> {out_dir}\")\n",
    "    return out_dir\n",
    "\n",
    "# Build shards if missing\n",
    "train_shards_dir = OUT_DIR/\"ranker_feats_train_shards\"\n",
    "val_shards_dir   = OUT_DIR/\"ranker_feats_val_shards\"\n",
    "test_shards_dir  = OUT_DIR/\"ranker_feats_test_shards\"\n",
    "\n",
    "if not train_shards_dir.exists():\n",
    "    train_shards_dir = build_feats_gpu_sharded(cand_train, \"train\")\n",
    "if not val_shards_dir.exists():\n",
    "    val_shards_dir   = build_feats_gpu_sharded(cand_val,   \"val\")\n",
    "if not test_shards_dir.exists():\n",
    "    test_shards_dir  = build_feats_gpu_sharded(cand_test,  \"test\")\n",
    "\n",
    "print(\"Shard dirs:\", train_shards_dir, val_shards_dir, test_shards_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead27b8f-be16-4b1c-968c-3ca7a4e3baf7",
   "metadata": {},
   "source": [
    "## 5. Ranker Model Definition\n",
    "- Define feature columns and data loader for shard batches\n",
    "- Implement RankerMLP neural network class\n",
    "- Initialize model, optimizer, loss, and mixed‐precision scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c51b8a-95bf-4286-bf90-eee46c3275d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKER_COLS = [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\"]\n",
    "\n",
    "def shard_batches(files, batch_size):\n",
    "    \"\"\"Generate batches from sharded Parquet files\"\"\"\n",
    "    for f in sorted(glob.glob(str(Path(files)/\"part_*.parquet\"))):\n",
    "        df = pd.read_parquet(f, engine=\"fastparquet\", columns=RANKER_COLS+[\"label\"])\n",
    "        X_np = df[RANKER_COLS].to_numpy(dtype='float32', copy=False)\n",
    "        y_np = df[\"label\"].to_numpy(dtype='float32', copy=False)\n",
    "        X = torch.from_numpy(X_np).to(device, non_blocking=True)\n",
    "        y = torch.from_numpy(y_np).to(device, non_blocking=True)\n",
    "        perm = torch.randperm(X.size(0), device=device)\n",
    "        for i in range(0, X.size(0), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            yield X[idx], y[idx]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class RankerMLP(nn.Module):\n",
    "    \"\"\"MLP ranker for candidate re-ranking\"\"\"\n",
    "    def __init__(self, d_in, hidden=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "model   = RankerMLP(len(RANKER_COLS), hidden=CFG[\"hidden\"], dropout=CFG[\"dropout\"]).to(device)\n",
    "opt     = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "use_amp = (device.type == \"cuda\")\n",
    "amp_device = \"cuda\" if use_amp else \"cpu\"\n",
    "scaler  = torch.amp.GradScaler(amp_device, enabled=use_amp)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e915284-45a9-4d28-97f5-fb6f7ffb69da",
   "metadata": {},
   "source": [
    "## 6. Training & Evaluation\n",
    "- Define train_ranker() to run epoch loops with mixed‐precision\n",
    "- Evaluate on validation shards each epoch\n",
    "- Implement rerank_one_batch and eval_reranked for final metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3a3617-7cb5-4291-a4fd-81c4698c13d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING RANKER\n",
      "==================================================\n",
      "Epoch 01 | train BCE 0.4653 | val Recall@10 0.4911 | val NDCG@10 0.3792\n",
      "Epoch 02 | train BCE 0.2149 | val Recall@10 0.3787 | val NDCG@10 0.2760\n",
      "Epoch 03 | train BCE 0.1918 | val Recall@10 0.4852 | val NDCG@10 0.3701\n",
      "Epoch 04 | train BCE 0.1868 | val Recall@10 0.7337 | val NDCG@10 0.6177\n",
      "Epoch 05 | train BCE 0.1819 | val Recall@10 0.7870 | val NDCG@10 0.7043\n",
      "Epoch 06 | train BCE 0.1772 | val Recall@10 0.7337 | val NDCG@10 0.6327\n",
      "Epoch 07 | train BCE 0.1730 | val Recall@10 0.6982 | val NDCG@10 0.5967\n",
      "Epoch 08 | train BCE 0.1679 | val Recall@10 0.6627 | val NDCG@10 0.5524\n",
      "Epoch 09 | train BCE 0.1634 | val Recall@10 0.6272 | val NDCG@10 0.5363\n",
      "Epoch 10 | train BCE 0.1590 | val Recall@10 0.6568 | val NDCG@10 0.5492\n",
      "Early stopping on Recall@10.\n",
      "Best val Recall@10 = 0.7870\n",
      "\n",
      "==================================================\n",
      "FINAL EVALUATION\n",
      "==================================================\n",
      "TEST — Recall@10: 0.8063, NDCG@10: 0.7193\n"
     ]
    }
   ],
   "source": [
    "def rerank_one_batch(cand_df_slice):\n",
    "    \"\"\"Re-rank candidates for one batch\"\"\"\n",
    "    H = build_hist_tensor(cand_df_slice['history_idx'].astype(str).tolist(), CFG[\"hist_max\"]).to(device)\n",
    "    P = torch.tensor(cand_df_slice['pos_item_idx'].astype(int).tolist(), dtype=torch.long, device=device)\n",
    "    C = torch.tensor([[int(x) for x in s.split()] for s in cand_df_slice['cands'].astype(str).tolist()],\n",
    "                     dtype=torch.long, device=device)\n",
    "    U = user_vecs_from_hist_batch(H)\n",
    "    feats = _pack_batch_features(U, H, C, P)\n",
    "    X = torch.stack([feats[c] for c in RANKER_COLS], dim=-1).view(-1, len(RANKER_COLS))\n",
    "    scores = model(X).view(len(feats[\"label\"]), -1)\n",
    "    topk = min(CFG[\"eval_topk\"], C.size(1))\n",
    "    _, idx = torch.topk(scores, k=topk, dim=1)\n",
    "    return [C[i][idx[i]].tolist() for i in range(C.size(0))]\n",
    "\n",
    "def eval_reranked(cand_df, split=\"val\"):\n",
    "    \"\"\"Evaluate re-ranked candidates\"\"\"\n",
    "    model.eval()\n",
    "    hits = 0; ndcgs = 0.0; tot = 0\n",
    "    B = 1024\n",
    "    for i in range(0, len(cand_df), B):\n",
    "        batch_df = cand_df.iloc[i:i+B]\n",
    "        reranked = rerank_one_batch(batch_df)\n",
    "        for pos, rr in zip(batch_df['pos_item_idx'].tolist(), reranked):\n",
    "            hits += float(int(pos) in rr)\n",
    "            if int(pos) in rr:\n",
    "                rank = rr.index(int(pos)) + 1\n",
    "                ndcgs += 1.0 / math.log2(rank + 1.0)\n",
    "            tot += 1\n",
    "    return hits/max(1,tot), ndcgs/max(1,tot)\n",
    "\n",
    "def train_ranker():\n",
    "    \"\"\"Train the ranker model\"\"\"\n",
    "    best_recall = -1.0; bad = 0\n",
    "    for ep in range(1, CFG[\"epochs\"]+1):\n",
    "        model.train(); total_loss=0.0; nobs=0\n",
    "        for Xb, yb in shard_batches(train_shards_dir, CFG[\"batch_size\"]):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(amp_device, enabled=use_amp, dtype=torch.float16):\n",
    "                logits = model(Xb)\n",
    "                loss   = loss_fn(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            total_loss += float(loss.item()) * yb.numel()\n",
    "            nobs       += yb.numel()\n",
    "\n",
    "        val_recall, val_ndcg = eval_reranked(cand_val, split=\"val\")\n",
    "        print(f\"Epoch {ep:02d} | train BCE {total_loss/max(1,nobs):.4f} | \"\n",
    "              f\"val Recall@{CFG['eval_topk']} {val_recall:.4f} | val NDCG@{CFG['eval_topk']} {val_ndcg:.4f}\")\n",
    "        if val_recall > best_recall + 1e-4:\n",
    "            best_recall, bad = val_recall, 0\n",
    "            torch.save(model.state_dict(), OUT_DIR/'ranker_best.pt')\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= CFG[\"patience\"]:\n",
    "                print(\"Early stopping on Recall@10.\"); break\n",
    "    print(\"Best val Recall@{} = {:.4f}\".format(CFG[\"eval_topk\"], best_recall))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING RANKER\")\n",
    "print(\"=\"*50)\n",
    "train_ranker()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "model.load_state_dict(torch.load(OUT_DIR/'ranker_best.pt', map_location=device, weights_only=True))\n",
    "test_recall, test_ndcg = eval_reranked(cand_test, split=\"test\")\n",
    "print(f\"TEST — Recall@{CFG['eval_topk']}: {test_recall:.4f}, NDCG@{CFG['eval_topk']}: {test_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4839258-9000-4f96-9f17-bdd590c75882",
   "metadata": {},
   "source": [
    "## 7. Comparison with Baselines\n",
    "- Load precomputed baseline results\n",
    "- Print Recall@K for each baseline and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6401ed5-96bb-46b5-8c89-afe745558d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPARISON WITH BASELINES\n",
      "==================================================\n",
      "Baseline Results (Held-out Interactions):\n",
      "  Popularity: {'Recall@5': 0.0650887573964497, 'Recall@10': 0.09467455621301775, 'Recall@20': 0.1242603550295858}\n",
      "  ItemKNN: {'Recall@5': 0.0, 'Recall@10': 0.0, 'Recall@20': 0.011834319526627219}\n",
      "  UserKNN: {'Recall@5': 0.029585798816568046, 'Recall@10': 0.029585798816568046, 'Recall@20': 0.047337278106508875}\n",
      "  MatrixFactorization: {'Recall@5': 0.0, 'Recall@10': 0.005917159763313609, 'Recall@20': 0.005917159763313609}\n",
      "\n",
      "Best baseline (Popularity): Recall@10 = 0.0947\n",
      "Two-Stage Ranker: Recall@10 = 0.8063\n",
      "Improvement: +751.60%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH BASELINES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "baseline_results_path = OUT_DIR/'baseline_results.json'\n",
    "if baseline_results_path.exists():\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    print(\"Baseline Results (Held-out Interactions):\")\n",
    "    for model_name, results in baseline_results['held_out_interactions'].items():\n",
    "        print(f\"  {model_name}: {results}\")\n",
    "    best_baseline = max(\n",
    "        [(m, r[f\"Recall@{CFG['eval_topk']}\"]) for m,r in baseline_results['held_out_interactions'].items()],\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    print(f\"\\nBest baseline ({best_baseline[0]}): Recall@{CFG['eval_topk']} = {best_baseline[1]:.4f}\")\n",
    "    print(f\"Two-Stage Ranker: Recall@{CFG['eval_topk']} = {test_recall:.4f}\")\n",
    "    improvement = ((test_recall - best_baseline[1]) / best_baseline[1]) * 100 if best_baseline[1]>0 else 0\n",
    "    print(f\"Improvement: {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(\"No baseline results found for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efeabc9-cae8-4594-8f9a-95f6324ba9cb",
   "metadata": {},
   "source": [
    "## 9. Results Saving\n",
    "\n",
    "- Persist final test metrics and model configuration to JSON  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c73083c-539d-4579-9ba7-558da7f6721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results to ../data/processed/jarir/ranker_results.json\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'model_config': CFG,\n",
    "    'evaluation_results': {\n",
    "        'test_recall': test_recall,\n",
    "        'test_ndcg': test_ndcg,\n",
    "        'eval_topk': CFG['eval_topk']\n",
    "    },\n",
    "    'data_stats': {\n",
    "        'n_users': len(customer_map),\n",
    "        'n_items': len(item_map),\n",
    "        'train_sequences': len(seq_train),\n",
    "        'val_sequences': len(seq_val),\n",
    "        'test_sequences': len(seq_test)\n",
    "    }\n",
    "}\n",
    "with open(OUT_DIR/'ranker_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\nSaved results to {OUT_DIR/'ranker_results.json'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
