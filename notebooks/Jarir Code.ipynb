{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab5614-6240-456f-9496-af8736d95636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8b660-1090-4984-9f93-82489c0eba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata, re, json\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_PATH = Path('../data/raw/jarir.xlsx')   # Jarir dataset\n",
    "OUT_DIR = Path('../data/processed/jarir/')  # Jarir output directory\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cleaning toggles / thresholds\n",
    "KEEP_ONLY_POSITIVE_QTY = True\n",
    "KEEP_ONLY_POSITIVE_PRICE = True\n",
    "DROP_RETURNS_BY_INVOICE_PREFIX = True   # transactions starting with 'C'\n",
    "DROP_DUPLICATE_ROWS = True\n",
    "\n",
    "# Non-product removal rules (description patterns; case-insensitive)\n",
    "NON_PRODUCT_PATTERNS = [\n",
    "    r'POSTAGE', r'SHIPPING', r'CARRIAGE', r'DELIVERY',\n",
    "    r'BANK CHARGES', r'AMAZON', r'DOTCOM', r'PACKING', r'ADJUST', r'DISCOUNT',\n",
    "    r'SAMPLE', r'SAMPLES', r'CHECK', r'TEST', r'MANUAL', r'FEE', r'CHARGE'\n",
    "]\n",
    "\n",
    "# Outlier handling (IQR on log values)\n",
    "HANDLE_OUTLIERS = True\n",
    "WINSORIZE_INSTEAD_OF_DROP = False   # if False, drop; if True, cap to bounds\n",
    "LOG_EPS = 1e-6\n",
    "IQR_MULT = 3.0     # 3*IQR is conservative on log scale\n",
    "\n",
    "# Coverage filters (optional; applied after core cleaning)\n",
    "MIN_EVENTS_PER_USER = 1       # set 1 to keep all users\n",
    "MIN_PURCHASES_PER_ITEM = 1    # set 1 to keep all items\n",
    "\n",
    "# Randomness\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Load Excel file\n",
    "print(\"Loading Jarir dataset...\")\n",
    "raw = pd.read_excel(DATA_PATH, engine='openpyxl')\n",
    "\n",
    "print(\"Original columns:\", raw.columns.tolist())\n",
    "print(\"Original data shape:\", raw.shape)\n",
    "\n",
    "# --- Date parsing function for Jarir format ---\n",
    "def parse_jarir_dates(date_series):\n",
    "    \"\"\"\n",
    "    Parse Jarir dates like 'Jan-1', 'Jan-2', etc.\n",
    "    Returns datetime series with 2024 as the year.\n",
    "    \"\"\"\n",
    "    def parse_single_date(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return pd.NaT\n",
    "        \n",
    "        try:\n",
    "            # Handle formats like 'Jan-1', 'Jan-2', etc.\n",
    "            if isinstance(date_str, str) and '-' in date_str:\n",
    "                month_day = date_str.split('-')\n",
    "                if len(month_day) == 2:\n",
    "                    month_str = month_day[0].strip()\n",
    "                    day_str = month_day[1].strip()\n",
    "                    \n",
    "                    # Month mapping\n",
    "                    month_map = {\n",
    "                        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,\n",
    "                        'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,\n",
    "                        'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "                    }\n",
    "                    \n",
    "                    if month_str in month_map and day_str.isdigit():\n",
    "                        month = month_map[month_str]\n",
    "                        day = int(day_str)\n",
    "                        # Use 2024 as the year\n",
    "                        return pd.Timestamp(2024, month, day)\n",
    "            return pd.NaT\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    \n",
    "    return pd.to_datetime(date_series.apply(parse_single_date), errors='coerce')\n",
    "\n",
    "# --- Standardize column names for Jarir data ---\n",
    "# Map actual Jarir columns to standard names\n",
    "rename_map = {\n",
    "    'ItemNumber': 'stock_code',\n",
    "    'ItemDescription': 'description',\n",
    "    'CustomerId': 'customer_id',\n",
    "    'Sales Quantity 2024': 'quantity',\n",
    "    'Sales Amount 2024': 'line_amount',\n",
    "    'Date': 'invoice_date',\n",
    "    'Showroom': 'country',  # Use showroom as country equivalent\n",
    "    'Brand': 'brand',\n",
    "    'GL Class': 'category',\n",
    "    'Vendor Prefix': 'vendor',\n",
    "    'Model': 'model',\n",
    "    'ShortItemNo': 'short_item_no',\n",
    "    'SalesChannel': 'sales_channel',\n",
    "    'Customer_VAT_Status': 'vat_status',\n",
    "    'UNQTRN': 'unique_transaction',\n",
    "    'No of Trans 2024': 'num_transactions'\n",
    "}\n",
    "\n",
    "# Apply renaming for columns that exist\n",
    "for old_col, new_col in rename_map.items():\n",
    "    if old_col in raw.columns:\n",
    "        raw.rename(columns={old_col: new_col}, inplace=True)\n",
    "\n",
    "print(\"Columns after renaming:\", raw.columns.tolist())\n",
    "\n",
    "# --- Fix date parsing for Jarir ---\n",
    "print(\"Fixing date parsing for Jarir...\")\n",
    "print(\"Original date sample:\", raw['invoice_date'].head())\n",
    "print(\"Original date dtype:\", raw['invoice_date'].dtype)\n",
    "\n",
    "# Use our custom date parsing function\n",
    "raw['invoice_date'] = parse_jarir_dates(raw['invoice_date'])\n",
    "\n",
    "print(\"Parsed dates sample:\", raw['invoice_date'].head())\n",
    "print(\"Valid dates:\", raw['invoice_date'].notna().sum())\n",
    "\n",
    "# If all dates are invalid, create synthetic dates\n",
    "if raw['invoice_date'].isna().all():\n",
    "    print(\"\\n❌ All dates are invalid. Creating synthetic dates...\")\n",
    "    raw['invoice_date'] = pd.date_range(start='2024-01-01', periods=len(raw), freq='D')\n",
    "    print(\"Created synthetic dates\")\n",
    "\n",
    "# --- Add price column (line_amount / quantity) ---\n",
    "raw['price'] = raw['line_amount'] / raw['quantity']\n",
    "\n",
    "print('Loaded rows:', len(raw))\n",
    "raw.head(3)\n",
    "\n",
    "## Step 1 — Basic validity filters\n",
    "\n",
    "quality = {}\n",
    "df = raw.copy()\n",
    "\n",
    "# Missing critical fields\n",
    "before = len(df)\n",
    "df = df[df['invoice_date'].notna() & df['stock_code'].notna() & df['description'].notna() & df['country'].notna() & df['customer_id'].notna()]\n",
    "quality['drop_missing_core'] = before - len(df)\n",
    "\n",
    "# Returns by invoice prefix 'C' (if applicable)\n",
    "if DROP_RETURNS_BY_INVOICE_PREFIX and 'unique_transaction' in df.columns:\n",
    "    before = len(df)\n",
    "    mask = df['unique_transaction'].astype(str).str.startswith('C', na=False)\n",
    "    df = df[~mask]\n",
    "    quality['drop_returns_invoice_prefix'] = before - len(df)\n",
    "\n",
    "# Positive quantity/price\n",
    "if KEEP_ONLY_POSITIVE_QTY:\n",
    "    before = len(df)\n",
    "    df = df[df['quantity'] > 0]\n",
    "    quality['drop_nonpositive_qty'] = before - len(df)\n",
    "\n",
    "if KEEP_ONLY_POSITIVE_PRICE:\n",
    "    before = len(df)\n",
    "    df = df[df['price'] > 0]\n",
    "    quality['drop_nonpositive_price'] = before - len(df)\n",
    "\n",
    "# Drop duplicate rows (exact duplicates)\n",
    "if DROP_DUPLICATE_ROWS:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    quality['drop_exact_duplicates'] = before - len(df)\n",
    "\n",
    "len(df), quality\n",
    "\n",
    "## Step 2 — Text normalization & non‑product removal\n",
    "\n",
    "# Text normalization helpers\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = '' if pd.isna(s) else str(s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)               # collapse whitespace\n",
    "    s = unicodedata.normalize('NFKC', s)      # unicode normalization\n",
    "    return s.upper()                           # uppercase for matching\n",
    "\n",
    "df['description'] = df['description'].astype(str).map(normalize_text)\n",
    "df['stock_code'] = df['stock_code'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Remove non-product lines by description patterns\n",
    "pattern = re.compile('|'.join(NON_PRODUCT_PATTERNS), flags=re.IGNORECASE)\n",
    "before = len(df)\n",
    "df = df[~df['description'].str.contains(pattern)]\n",
    "quality['drop_non_product_patterns'] = before - len(df)\n",
    "\n",
    "len(df), df['description'].head(5).tolist()[:3]\n",
    "\n",
    "## Step 3 — De‑duplicate & aggregate duplicate transaction lines\n",
    "\n",
    "# Some transactions may repeat the same stock_code line; aggregate quantities & compute line_amount\n",
    "df['line_amount'] = df['quantity'] * df['price']\n",
    "\n",
    "# Aggregate per (customer_id, stock_code, invoice_date) for Jarir\n",
    "agg_cols = {\n",
    "    'quantity':'sum',\n",
    "    'price':'mean',          # average price across repeats\n",
    "    'line_amount':'sum',\n",
    "    'description':'first',   # keep first normalized description\n",
    "    'country':'first',\n",
    "    'brand':'first',\n",
    "    'category':'first',\n",
    "    'vendor':'first',\n",
    "    'invoice_date':'first',\n",
    "}\n",
    "before = len(df)\n",
    "df = df.groupby(['customer_id','stock_code','invoice_date'], as_index=False).agg(agg_cols)\n",
    "quality['aggregated_duplicate_lines'] = before - len(df)\n",
    "\n",
    "len(df), df.head(3)\n",
    "\n",
    "## Step 4 — Outlier detection & handling (IQR on log scale)\n",
    "\n",
    "def iqr_bounds_log(series, mult=IQR_MULT, eps=LOG_EPS):\n",
    "    s = np.log(series.clip(lower=eps))\n",
    "    # Handle NaN values properly\n",
    "    s_clean = s.dropna()\n",
    "    if len(s_clean) == 0:\n",
    "        return 0, 0\n",
    "    q1, q3 = np.percentile(s_clean, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - mult*iqr, q3 + mult*iqr\n",
    "    return np.exp(lo), np.exp(hi)\n",
    "\n",
    "if HANDLE_OUTLIERS:\n",
    "    # Bounds for unit price, quantity, and line_amount\n",
    "    price_lo, price_hi = iqr_bounds_log(df['price'])\n",
    "    qty_lo, qty_hi = iqr_bounds_log(df['quantity'])\n",
    "    amt_lo, amt_hi = iqr_bounds_log(df['line_amount'])\n",
    "\n",
    "    # Apply handling\n",
    "    def handle(series, lo, hi, name):\n",
    "        before = len(series)\n",
    "        if WINSORIZE_INSTEAD_OF_DROP:\n",
    "            clipped = series.clip(lower=lo, upper=hi)\n",
    "            removed = 0\n",
    "            return clipped, removed\n",
    "        else:\n",
    "            mask = (series >= lo) & (series <= hi)\n",
    "            removed = int((~mask).sum())\n",
    "            return series[mask], removed\n",
    "\n",
    "    # Because handling may change row counts, operate via mask\n",
    "    mask_price = (df['price'] >= price_lo) & (df['price'] <= price_hi)\n",
    "    mask_qty = (df['quantity'] >= qty_lo) & (df['quantity'] <= qty_hi)\n",
    "    mask_amt = (df['line_amount'] >= amt_lo) & (df['line_amount'] <= amt_hi)\n",
    "    combined_mask = mask_price & mask_qty & mask_amt\n",
    "\n",
    "    before = len(df)\n",
    "    df = df[combined_mask].copy()\n",
    "    quality['drop_outliers'] = before - len(df)\n",
    "\n",
    "# Recompute line_amount after any clipping (if winsorized, we would recompute here)\n",
    "df['line_amount'] = df['quantity'] * df['price']\n",
    "\n",
    "len(df)\n",
    "\n",
    "## Step 5 — Optional coverage filters (rare users/items)\n",
    "\n",
    "# These are optional; useful for sequential models to ensure enough history.\n",
    "# Set thresholds to 1 above if you want to skip dropping rare entities.\n",
    "\n",
    "# Filter items with too few purchases\n",
    "if MIN_PURCHASES_PER_ITEM > 1:\n",
    "    item_counts = df.groupby('stock_code').size()\n",
    "    keep_items = set(item_counts[item_counts >= MIN_PURCHASES_PER_ITEM].index)\n",
    "    before = len(df)\n",
    "    df = df[df['stock_code'].isin(keep_items)]\n",
    "    quality['drop_rare_items'] = before - len(df)\n",
    "\n",
    "# Filter users with too few events\n",
    "if MIN_EVENTS_PER_USER > 1:\n",
    "    user_counts = df.groupby('customer_id').size()\n",
    "    keep_users = set(user_counts[user_counts >= MIN_EVENTS_PER_USER].index)\n",
    "    before = len(df)\n",
    "    df = df[df['customer_id'].isin(keep_users)]\n",
    "    quality['drop_rare_users'] = before - len(df)\n",
    "\n",
    "len(df)\n",
    "\n",
    "## Step 6 — Final tidy tables & save\n",
    "\n",
    "interactions = (\n",
    "    df[[\n",
    "        'customer_id','invoice_date',\n",
    "        'stock_code','description','quantity',\n",
    "        'price','line_amount','country','brand','category','vendor'\n",
    "    ]].copy()\n",
    "    .sort_values(['customer_id','invoice_date'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Normalize dtypes explicitly (avoid pandas/pyarrow extension dtypes)\n",
    "interactions['stock_code']   = interactions['stock_code'].astype(str)\n",
    "interactions['description']  = interactions['description'].astype(str)\n",
    "interactions['country']      = interactions['country'].astype(str)\n",
    "interactions['brand']        = interactions['brand'].astype(str)\n",
    "interactions['category']     = interactions['category'].astype(str)\n",
    "interactions['vendor']       = interactions['vendor'].astype(str)\n",
    "\n",
    "interactions['invoice_date'] = pd.to_datetime(interactions['invoice_date'], errors='coerce').dt.tz_localize(None)\n",
    "interactions['customer_id']  = pd.to_numeric(interactions['customer_id'], errors='coerce').astype('Int64').astype('int64')\n",
    "interactions['quantity']     = pd.to_numeric(interactions['quantity'], errors='coerce').astype('float64')\n",
    "interactions['price']        = pd.to_numeric(interactions['price'], errors='coerce').astype('float64')\n",
    "interactions['line_amount']  = pd.to_numeric(interactions['line_amount'], errors='coerce').astype('float64')\n",
    "\n",
    "# Drop any rows that became NA in critical fields after coercion\n",
    "interactions = interactions.dropna(subset=['invoice_date','customer_id','stock_code'])\n",
    "\n",
    "# Defensive: convert any stray Period/Interval columns if they slipped in\n",
    "for col in interactions.columns:\n",
    "    dt = interactions[col].dtype\n",
    "    if str(dt).startswith('period'):   # PeriodDtype -> timestamp\n",
    "        interactions[col] = interactions[col].astype('datetime64[ns]')\n",
    "    if str(dt).startswith('interval'): # IntervalDtype -> string\n",
    "        interactions[col] = interactions[col].astype(str)\n",
    "\n",
    "# --- Item catalog ------------------------------------------------------------\n",
    "item_catalog = (\n",
    "    interactions.groupby('stock_code', as_index=False)\n",
    "    .agg(\n",
    "        description=('description', lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0]),\n",
    "        price_median=('price', 'median'),\n",
    "        price_mean=('price', 'mean'),\n",
    "        pop=('stock_code', 'size'),\n",
    "        brand=('brand', lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0]),\n",
    "        category=('category', lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0]),\n",
    "        vendor=('vendor', lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0])\n",
    "    )\n",
    "    .sort_values('pop', ascending=False)\n",
    ")\n",
    "\n",
    "# --- Customer table ----------------------------------------------------------\n",
    "customer_table = (\n",
    "    interactions.groupby('customer_id', as_index=False)\n",
    "    .agg(\n",
    "        first_date=('invoice_date','min'),\n",
    "        last_date =('invoice_date','max'),\n",
    "        n_events  =('invoice_date','nunique'),\n",
    "        n_lines   =('stock_code','size'),\n",
    "        country_mode=('country', lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0]),\n",
    "        total_spent=('line_amount','sum')\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Write to Parquet (no registry hacking) ---------------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "OUT_DIR = Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('./out')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_parquet_safe(df, path):\n",
    "    # Try pyarrow first\n",
    "    try:\n",
    "        df.to_parquet(path, index=False, engine='pyarrow')\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] pyarrow failed on {path.name}: {e}\\nFalling back to fastparquet...\")\n",
    "        # Fallback: fastparquet (no Arrow extension registry involved)\n",
    "        import fastparquet  # ensure installed: pip install fastparquet\n",
    "        df.to_parquet(path, index=False, engine='fastparquet')\n",
    "\n",
    "write_parquet_safe(interactions,  OUT_DIR / 'interactions_clean.parquet')\n",
    "write_parquet_safe(item_catalog,  OUT_DIR / 'items_clean.parquet')\n",
    "write_parquet_safe(customer_table, OUT_DIR / 'customers_clean.parquet')\n",
    "\n",
    "# --- Quality report ----------------------------------------------------------\n",
    "if 'quality' not in globals():\n",
    "    quality = {}\n",
    "rows_initial = int(len(raw)) if 'raw' in globals() else None\n",
    "\n",
    "report = {\n",
    "    'counts': {'rows_initial': rows_initial, 'rows_final': int(len(interactions))},\n",
    "    'quality': quality\n",
    "}\n",
    "with open(OUT_DIR / 'quality_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('✅ Saved cleaned tables to', OUT_DIR)\n",
    "report\n",
    "\n",
    "## Optional: Quick visual checks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interactions['price'].plot(kind='hist', bins=50, title='Unit Price (clean)')\n",
    "plt.xlabel('price'); plt.ylabel('count'); plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interactions['quantity'].plot(kind='hist', bins=50, title='Quantity (clean)')\n",
    "plt.xlabel('quantity'); plt.ylabel('count'); plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "(interactions['line_amount']).plot(kind='hist', bins=50, title='Line Amount (clean)')\n",
    "plt.xlabel('line_amount'); plt.ylabel('count'); plt.show()\n",
    "\n",
    "## Step 7 — Build user sequences & time-based split (→ train/val/test)\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Inputs from previous steps (or reload if running standalone)\n",
    "OUT_DIR = Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('./out')\n",
    "interactions_path = OUT_DIR / 'interactions_clean.parquet'\n",
    "if 'interactions' not in globals():\n",
    "    interactions = pd.read_parquet(interactions_path)\n",
    "\n",
    "# --- Config ---\n",
    "HIST_MAX      = 15     # cap on history length (reduced for Jarir)\n",
    "MIN_HISTORY   = 2      # require at least this many past items to form a sample\n",
    "SPLIT_QS      = (0.80, 0.90)  # 80%/10%/10% time split by global timestamps\n",
    "SAVE_NEGATIVES = False # set True to also save random negatives for quick tests\n",
    "N_NEG_TRAIN   = 50\n",
    "N_NEG_VALTEST = 100\n",
    "\n",
    "# --- Build stable ID maps (will be reused later) ---\n",
    "items = interactions['stock_code'].astype(str).unique()\n",
    "item_id_map = pd.DataFrame({'stock_code': items}).sort_values('stock_code').reset_index(drop=True)\n",
    "item_id_map['item_idx'] = np.arange(len(item_id_map), dtype=np.int64)\n",
    "\n",
    "customers = interactions['customer_id'].astype('int64').unique()\n",
    "customer_id_map = pd.DataFrame({'customer_id': customers}).sort_values('customer_id').reset_index(drop=True)\n",
    "customer_id_map['user_idx'] = np.arange(len(customer_id_map), dtype=np.int64)\n",
    "\n",
    "# Join indices into interactions (no exotic dtypes)\n",
    "interactions_idx = (\n",
    "    interactions\n",
    "    .merge(item_id_map, on='stock_code', how='left')\n",
    "    .merge(customer_id_map, on='customer_id', how='left')\n",
    "    .sort_values(['customer_id','invoice_date'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Compute global time cutoffs ---\n",
    "t1 = interactions_idx['invoice_date'].quantile(SPLIT_QS[0])\n",
    "t2 = interactions_idx['invoice_date'].quantile(SPLIT_QS[1])\n",
    "\n",
    "def build_sequences(df_user_sorted, t_start, t_end, hist_max=HIST_MAX, min_hist=MIN_HISTORY):\n",
    "    \"\"\"Create (history -> next item) samples whose TARGET time is within [t_start, t_end).\n",
    "       History can include events before t_start to avoid cold history.\"\"\"\n",
    "    rows = []\n",
    "    item_pool = item_id_map['item_idx'].to_numpy()\n",
    "    for uid, g in df_user_sorted.groupby('customer_id', sort=False):\n",
    "        items_idx = g['item_idx'].to_numpy()\n",
    "        times     = g['invoice_date'].to_numpy()\n",
    "        country   = g['country'].iloc[-1]  # last known country\n",
    "        for i in range(1, len(items_idx)):\n",
    "            ts = times[i]\n",
    "            if not (t_start <= ts < t_end):\n",
    "                continue\n",
    "            hist = items_idx[max(0, i - hist_max):i]\n",
    "            if len(hist) < min_hist:\n",
    "                continue\n",
    "            pos  = items_idx[i]\n",
    "            # store as plain strings to keep Parquet simple/robust\n",
    "            rows.append((\n",
    "                uid,\n",
    "                int(df_user_sorted['user_idx'].iloc[i]),\n",
    "                ts,\n",
    "                ' '.join(map(str, hist.tolist())),\n",
    "                int(pos),\n",
    "                str(country)\n",
    "            ))\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['customer_id','user_idx','ts','history_idx','pos_item_idx','country'])\n",
    "    out = pd.DataFrame(rows, columns=['customer_id','user_idx','ts','history_idx','pos_item_idx','country'])\n",
    "    return out\n",
    "\n",
    "# Split windows (inclusive start, exclusive end)\n",
    "t_min = interactions_idx['invoice_date'].min()\n",
    "t_max = interactions_idx['invoice_date'].max()\n",
    "\n",
    "seq_train = build_sequences(interactions_idx, t_min, t1)\n",
    "seq_val   = build_sequences(interactions_idx, t1,   t2)\n",
    "seq_test  = build_sequences(interactions_idx, t2,   t_max + pd.Timedelta(seconds=1))\n",
    "\n",
    "# --- (Optional) simple random negatives for quick testing ---\n",
    "rng = np.random.default_rng(42)\n",
    "def add_random_negs(df_seq, n_neg):\n",
    "    if df_seq.empty:\n",
    "        df_seq['neg_idx'] = ''\n",
    "        return df_seq\n",
    "    item_pool = item_id_map['item_idx'].to_numpy()\n",
    "    negs = []\n",
    "    for h, pos in zip(df_seq['history_idx'].values, df_seq['pos_item_idx'].values):\n",
    "        hist_set = set(map(int, h.split())) if h else set()\n",
    "        forbid   = hist_set | {int(pos)}\n",
    "        # sample without replacement until we have n_neg or exhaust\n",
    "        choices = item_pool[~np.isin(item_pool, list(forbid))]\n",
    "        if len(choices) == 0:\n",
    "            negs.append('')\n",
    "            continue\n",
    "        take = choices[rng.choice(len(choices), size=min(n_neg, len(choices)), replace=False)]\n",
    "        negs.append(' '.join(map(str, take.tolist())))\n",
    "    df_seq = df_seq.copy()\n",
    "    df_seq['neg_idx'] = negs\n",
    "    return df_seq\n",
    "\n",
    "if SAVE_NEGATIVES:\n",
    "    seq_train = add_random_negs(seq_train, N_NEG_TRAIN)\n",
    "    seq_val   = add_random_negs(seq_val,   N_NEG_VALTEST)\n",
    "    seq_test  = add_random_negs(seq_test,  N_NEG_VALTEST)\n",
    "\n",
    "# --- Save artifacts (plain types; Parquet-friendly) ---\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_parquet_safe(df, path):\n",
    "    try:\n",
    "        df.to_parquet(path, index=False, engine='pyarrow')\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] pyarrow failed on {Path(path).name}: {e}\\nFalling back to fastparquet...\")\n",
    "        import fastparquet\n",
    "        df.to_parquet(path, index=False, engine='fastparquet')\n",
    "\n",
    "write_parquet_safe(item_id_map,     OUT_DIR / 'item_id_map.parquet')\n",
    "write_parquet_safe(customer_id_map, OUT_DIR / 'customer_id_map.parquet')\n",
    "write_parquet_safe(seq_train,       OUT_DIR / 'sequences_train.parquet')\n",
    "write_parquet_safe(seq_val,         OUT_DIR / 'sequences_val.parquet')\n",
    "write_parquet_safe(seq_test,        OUT_DIR / 'sequences_test.parquet')\n",
    "\n",
    "print(\"✅ Sequences saved:\",\n",
    "      len(seq_train), \"train |\",\n",
    "      len(seq_val),   \"val |\",\n",
    "      len(seq_test),  \"test\")\n",
    "print(\"Time cuts:\", t_min, \"→\", t1, \"→\", t2, \"→\", t_max)\n",
    "display(seq_train.head(3))\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = Path(OUT_DIR)  # if OUT_DIR is a string\n",
    "files = [\n",
    "    'item_id_map.parquet',\n",
    "    'customer_id_map.parquet',\n",
    "    'sequences_train.parquet',\n",
    "    'sequences_val.parquet',\n",
    "    'sequences_test.parquet',\n",
    "]\n",
    "for f in files:\n",
    "    p = OUT_DIR / f\n",
    "    print(f\"{f} -> exists={p.exists()} size={p.stat().st_size if p.exists() else 'NA'}\")\n",
    "\n",
    "# Read a small sample using fastparquet to avoid pyarrow registry issues\n",
    "df_train = pd.read_parquet(OUT_DIR/'sequences_train.parquet', engine='fastparquet')\n",
    "print(df_train.shape)\n",
    "print(df_train.head(3))\n",
    "\n",
    "df[\"stock_code\"].nunique()  # Check unique stock codes in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23cb6f-f5a4-4dcc-b116-67dfdda4c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA EMB AND BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50418bc-903b-4d2e-b1fc-2a49d26404ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config & Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data paths\n",
    "OUT_DIR = Path('../data/processed/jarir/')\n",
    "INTERACTIONS_TRAIN_PATH = OUT_DIR / 'interactions_clean.parquet'\n",
    "CUSTOMER_MAP_PATH = OUT_DIR / 'customer_id_map.parquet'\n",
    "ITEM_MAP_PATH = OUT_DIR / 'item_id_map.parquet'\n",
    "\n",
    "# Evaluation config\n",
    "K_VALUES = [5, 10, 20]  # Recall@K values to evaluate\n",
    "EVAL_SAMPLE_SIZE = 1000  # Number of users to sample for evaluation\n",
    "\n",
    "print(\"Loading Jarir interaction data...\")\n",
    "\n",
    "# --- Load Processed Data ---\n",
    "# Load sequences\n",
    "seq_train = pd.read_parquet(OUT_DIR / 'sequences_train.parquet', engine=\"fastparquet\")\n",
    "seq_val = pd.read_parquet(OUT_DIR / 'sequences_val.parquet', engine=\"fastparquet\")\n",
    "seq_test = pd.read_parquet(OUT_DIR / 'sequences_test.parquet', engine=\"fastparquet\")\n",
    "\n",
    "# Load maps\n",
    "item_map = pd.read_parquet(OUT_DIR / 'item_id_map.parquet', engine=\"fastparquet\")\n",
    "customer_map = pd.read_parquet(OUT_DIR / 'customer_id_map.parquet', engine=\"fastparquet\")\n",
    "\n",
    "print(f\"Sequences: Train={len(seq_train)}, Val={len(seq_val)}, Test={len(seq_test)}\")\n",
    "print(f\"Items: {len(item_map)}, Customers: {len(customer_map)}\")\n",
    "\n",
    "print(f\"Customers: {len(customer_map)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample interactions:\")\n",
    "print(interactions.head())\n",
    "\n",
    "print(\"\\nSample customer map:\")\n",
    "print(customer_map.head())\n",
    "\n",
    "print(\"\\nSample item map:\")\n",
    "print(item_map.head())\n",
    "\n",
    "# --- Create User and Item Mappings ---\n",
    "# Create lookup dictionaries\n",
    "customer_to_idx = dict(zip(customer_map['customer_id'], customer_map['user_idx']))\n",
    "item_to_idx = dict(zip(item_map['stock_code'], item_map['item_idx']))\n",
    "\n",
    "# Add indices to interactions\n",
    "interactions['user_idx'] = interactions['customer_id'].map(customer_to_idx)\n",
    "interactions['item_idx'] = interactions['stock_code'].map(item_to_idx)\n",
    "\n",
    "# Remove any rows with missing indices\n",
    "before = len(interactions)\n",
    "interactions = interactions.dropna(subset=['user_idx', 'item_idx'])\n",
    "print(f\"Removed {before - len(interactions)} rows with missing indices\")\n",
    "\n",
    "# Convert to integers\n",
    "interactions['user_idx'] = interactions['user_idx'].astype(int)\n",
    "interactions['item_idx'] = interactions['item_idx'].astype(int)\n",
    "\n",
    "print(f\"Final interactions shape: {interactions.shape}\")\n",
    "print(f\"Unique users: {interactions['user_idx'].nunique()}\")\n",
    "print(f\"Unique items: {interactions['item_idx'].nunique()}\")\n",
    "\n",
    "# --- Create Sparse Interaction Matrix ---\n",
    "def create_sparse_matrix_from_sequences(seq_df, n_users, n_items):\n",
    "    \"\"\"Create sparse matrix from sequences for training\"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    values = []\n",
    "    \n",
    "    for _, row in seq_df.iterrows():\n",
    "        user_idx = row['user_idx']\n",
    "        pos_item_idx = row['pos_item_idx']\n",
    "        \n",
    "        # Add positive interaction\n",
    "        rows.append(user_idx)\n",
    "        cols.append(pos_item_idx)\n",
    "        values.append(1.0)\n",
    "        \n",
    "        # Add history interactions\n",
    "        if pd.notna(row['history_idx']) and row['history_idx']:\n",
    "            hist_items = [int(x) for x in row['history_idx'].split()]\n",
    "            for hist_item in hist_items:\n",
    "                rows.append(user_idx)\n",
    "                cols.append(hist_item)\n",
    "                values.append(0.5)  # Lower weight for history\n",
    "    \n",
    "    matrix = csr_matrix((values, (rows, cols)), shape=(n_users, n_items))\n",
    "    return matrix\n",
    "\n",
    "# Create training matrix from sequences\n",
    "train_matrix = create_sparse_matrix_from_sequences(seq_train, len(customer_map), len(item_map))\n",
    "\n",
    "n_users = len(customer_map)\n",
    "n_items = len(item_map)\n",
    "\n",
    "print(f\"Creating sparse matrix: {n_users} users × {n_items} items\")\n",
    "\n",
    "full_matrix = create_sparse_matrix(interactions, n_users, n_items)\n",
    "\n",
    "print(f\"Sparse matrix shape: {full_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {full_matrix.nnz}\")\n",
    "print(f\"Sparsity: {1 - full_matrix.nnz / (n_users * n_items):.4f}\")\n",
    "\n",
    "# --- Create Train/Test Split for Proper Evaluation ---\n",
    "print(\"\\nCreating train/test split for evaluation...\")\n",
    "np.random.seed(42)\n",
    "test_interactions = []\n",
    "\n",
    "# Use validation sequences for evaluation\n",
    "test_interactions = []\n",
    "for _, row in seq_val.iterrows():\n",
    "    user_idx = row['user_idx']\n",
    "    test_item = row['pos_item_idx']\n",
    "    test_interactions.append((user_idx, test_item, 1.0))\n",
    "    \n",
    "    # Remove test interaction from training matrix\n",
    "    train_matrix[user_idx, test_item] = 0\n",
    "\n",
    "print(f\"Created {len(test_interactions)} test interactions from validation sequences\")\n",
    "\n",
    "\n",
    "# Create train matrix (with test interactions removed)\n",
    "train_matrix = full_matrix.copy()\n",
    "\n",
    "print(f\"Train matrix shape: {train_matrix.shape}\")\n",
    "print(f\"Train non-zero entries: {train_matrix.nnz}\")\n",
    "\n",
    "# Convert to dense if matrix is small enough\n",
    "if n_users * n_items < 1e7:  # 10M elements threshold\n",
    "    print(\"Converting to dense matrix for faster computation...\")\n",
    "    train_matrix_dense = train_matrix.toarray()\n",
    "    use_dense = True\n",
    "else:\n",
    "    train_matrix_dense = None\n",
    "    use_dense = False\n",
    "\n",
    "# --- Baseline Models ---\n",
    "\n",
    "class SimpleItemKNN:\n",
    "    \"\"\"Simple ItemKNN baseline using cosine similarity\"\"\"\n",
    "    def __init__(self, k=50):\n",
    "        self.k = k\n",
    "        self.item_similarities = None\n",
    "        self.train_matrix = None\n",
    "    \n",
    "    def fit(self, matrix):\n",
    "        self.train_matrix = matrix\n",
    "        # Compute item-item similarities\n",
    "        if use_dense:\n",
    "            # Use dense matrix for small datasets\n",
    "            item_similarities = cosine_similarity(matrix.T)\n",
    "        else:\n",
    "            # Use sparse matrix for large datasets\n",
    "            item_similarities = cosine_similarity(matrix.T.toarray())\n",
    "        \n",
    "        self.item_similarities = item_similarities\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, n_recommendations=10):\n",
    "        if user_idx >= self.train_matrix.shape[0]:\n",
    "            return []\n",
    "        \n",
    "        # Get user's interactions - FIXED FOR SPARSE MATRICES\n",
    "        if hasattr(self.train_matrix, 'toarray'):\n",
    "            # Sparse matrix\n",
    "            user_interactions = self.train_matrix[user_idx].toarray().flatten()\n",
    "        else:\n",
    "            # Dense matrix\n",
    "            user_interactions = self.train_matrix[user_idx]\n",
    "        \n",
    "        # Find items user has interacted with - NOW SAFE\n",
    "        interacted_items = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        if len(interacted_items) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Compute scores for all items - SIMPLIFIED APPROACH\n",
    "        scores = np.zeros(self.train_matrix.shape[1])\n",
    "        \n",
    "        for item in interacted_items:\n",
    "            # Get top-k similar items\n",
    "            similar_items = np.argsort(self.item_similarities[item])[::-1][:self.k]\n",
    "            for similar_item in similar_items:\n",
    "                if similar_item not in interacted_items:  # Don't recommend already interacted items\n",
    "                    # Just add similarity score, don't multiply by interaction strength\n",
    "                    scores[similar_item] += self.item_similarities[item][similar_item]\n",
    "        \n",
    "        # Return top recommendations - REMOVE THE > 0 FILTER\n",
    "        top_items = np.argsort(scores)[::-1][:n_recommendations]\n",
    "        return [item for item in top_items if scores[item] > 0]\n",
    "\n",
    "class SimpleUserKNN:\n",
    "    \"\"\"Simple UserKNN baseline using cosine similarity\"\"\"\n",
    "    def __init__(self, k=50):\n",
    "        self.k = k\n",
    "        self.user_similarities = None\n",
    "        self.train_matrix = None\n",
    "    \n",
    "    def fit(self, matrix):\n",
    "        self.train_matrix = matrix\n",
    "        # Compute user-user similarities\n",
    "        if use_dense:\n",
    "            user_similarities = cosine_similarity(matrix)\n",
    "        else:\n",
    "            user_similarities = cosine_similarity(matrix.toarray())\n",
    "        \n",
    "        self.user_similarities = user_similarities\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, n_recommendations=10):\n",
    "        if user_idx >= self.train_matrix.shape[0]:\n",
    "            return []\n",
    "        \n",
    "        # Get similar users - INCLUDE SELF TOO\n",
    "        similar_users = np.argsort(self.user_similarities[user_idx])[::-1][:self.k+1]  # Include self\n",
    "        \n",
    "        # Aggregate recommendations from similar users\n",
    "        scores = np.zeros(self.train_matrix.shape[1])\n",
    "        \n",
    "        for similar_user in similar_users:\n",
    "            # Get similar user's interactions - FIXED FOR SPARSE MATRICES\n",
    "            if hasattr(self.train_matrix, 'toarray'):\n",
    "                # Sparse matrix\n",
    "                similar_user_interactions = self.train_matrix[similar_user].toarray().flatten()\n",
    "            else:\n",
    "                # Dense matrix\n",
    "                similar_user_interactions = self.train_matrix[similar_user]\n",
    "            \n",
    "            # Add weighted scores - REMOVE SIMILARITY WEIGHT FOR NOW\n",
    "            scores += similar_user_interactions  # Just sum interactions\n",
    "        \n",
    "        # Get user's own interactions to exclude - FIXED FOR SPARSE MATRICES\n",
    "        if hasattr(self.train_matrix, 'toarray'):\n",
    "            # Sparse matrix\n",
    "            user_interactions = self.train_matrix[user_idx].toarray().flatten()\n",
    "        else:\n",
    "            # Dense matrix\n",
    "            user_interactions = self.train_matrix[user_idx]\n",
    "        \n",
    "        interacted_items = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        # Zero out already interacted items\n",
    "        scores[interacted_items] = -1\n",
    "        \n",
    "        # Return top recommendations - REMOVE THE > 0 FILTER\n",
    "        top_items = np.argsort(scores)[::-1][:n_recommendations]\n",
    "        return [item for item in top_items if scores[item] > -0.5]  # Allow small negative scores\n",
    "\n",
    "class SimpleMatrixFactorization:\n",
    "    \"\"\"Simple Matrix Factorization using NMF\"\"\"\n",
    "    def __init__(self, n_factors=50, max_iter=100):\n",
    "        self.n_factors = n_factors\n",
    "        self.max_iter = max_iter\n",
    "        self.model = None\n",
    "        self.train_matrix = None\n",
    "    \n",
    "    def fit(self, matrix):\n",
    "        self.train_matrix = matrix\n",
    "        \n",
    "        # Convert to dense for NMF\n",
    "        if use_dense:\n",
    "            matrix_dense = matrix\n",
    "        else:\n",
    "            matrix_dense = matrix.toarray()\n",
    "        \n",
    "        # Apply NMF\n",
    "        self.model = NMF(n_components=self.n_factors, max_iter=self.max_iter, random_state=42)\n",
    "        self.model.fit(matrix_dense)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, n_recommendations=10):\n",
    "        if user_idx >= self.train_matrix.shape[0] or self.model is None:\n",
    "            return []\n",
    "        \n",
    "        # Get user's interactions to exclude - FIXED FOR SPARSE MATRICES\n",
    "        if hasattr(self.train_matrix, 'toarray'):\n",
    "            # Sparse matrix\n",
    "            user_interactions = self.train_matrix[user_idx].toarray().flatten()\n",
    "        else:\n",
    "            # Dense matrix\n",
    "            user_interactions = self.train_matrix[user_idx]\n",
    "        \n",
    "        interacted_items = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        # Get predicted scores\n",
    "        user_factors = self.model.transform(self.train_matrix[user_idx:user_idx+1])\n",
    "        item_factors = self.model.components_\n",
    "        \n",
    "        scores = np.dot(user_factors, item_factors).flatten()\n",
    "        \n",
    "        # Zero out already interacted items\n",
    "        scores[interacted_items] = -1\n",
    "        \n",
    "        # Return top recommendations - REMOVE THE > 0 FILTER\n",
    "        top_items = np.argsort(scores)[::-1][:n_recommendations]\n",
    "        return [item for item in top_items if scores[item] > -0.5]  # Allow small negative scores\n",
    "\n",
    "class PopularityBaseline:\n",
    "    \"\"\"Simple popularity-based baseline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.item_popularity = None\n",
    "        self.train_matrix = None\n",
    "    \n",
    "    def fit(self, matrix):\n",
    "        self.train_matrix = matrix\n",
    "        # Compute item popularity (sum of interactions)\n",
    "        self.item_popularity = np.array(matrix.sum(axis=0)).flatten()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, n_recommendations=10):\n",
    "        if user_idx >= self.train_matrix.shape[0]:\n",
    "            return []\n",
    "        \n",
    "        # Get user's interactions to exclude\n",
    "        if hasattr(self.train_matrix, 'toarray'):\n",
    "            user_interactions = self.train_matrix[user_idx].toarray().flatten()\n",
    "        else:\n",
    "            user_interactions = self.train_matrix[user_idx]\n",
    "        \n",
    "        interacted_items = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        # Use popularity scores\n",
    "        scores = self.item_popularity.copy()\n",
    "        scores[interacted_items] = -1  # Exclude already interacted items\n",
    "        \n",
    "        # Return top recommendations\n",
    "        top_items = np.argsort(scores)[::-1][:n_recommendations]\n",
    "        return [item for item in top_items if scores[item] > 0]\n",
    "\n",
    "# --- Evaluation Functions ---\n",
    "\n",
    "def evaluate_with_held_out_interactions(model, train_matrix, test_interactions, k_values=[5, 10, 20]):\n",
    "    \"\"\"Evaluate model using held-out interactions\"\"\"\n",
    "    results = {k: [] for k in k_values}\n",
    "    \n",
    "    for user_idx, test_item, test_value in test_interactions:\n",
    "        # Get recommendations\n",
    "        recommendations = model.predict(user_idx, n_recommendations=max(k_values))\n",
    "        \n",
    "        # Compute Recall@K for each K\n",
    "        for k in k_values:\n",
    "            if len(recommendations) >= k:\n",
    "                recommended_items = set(recommendations[:k])\n",
    "                # Check if test item is in recommendations\n",
    "                if test_item in recommended_items:\n",
    "                    results[k].append(1.0)\n",
    "                else:\n",
    "                    results[k].append(0.0)\n",
    "            else:\n",
    "                results[k].append(0.0)\n",
    "    \n",
    "    # Compute average Recall@K\n",
    "    avg_results = {}\n",
    "    for k in k_values:\n",
    "        if results[k]:\n",
    "            avg_results[f'Recall@{k}'] = np.mean(results[k])\n",
    "        else:\n",
    "            avg_results[f'Recall@{k}'] = 0.0\n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "# --- Train and Evaluate Baselines ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING BASELINE MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train ItemKNN\n",
    "print(\"\\nTraining ItemKNN...\")\n",
    "itemknn = SimpleItemKNN(k=50)\n",
    "itemknn.fit(train_matrix)\n",
    "\n",
    "# Train UserKNN\n",
    "print(\"Training UserKNN...\")\n",
    "userknn = SimpleUserKNN(k=50)\n",
    "userknn.fit(train_matrix)\n",
    "\n",
    "# Train Matrix Factorization\n",
    "print(\"Training Matrix Factorization...\")\n",
    "mf = SimpleMatrixFactorization(n_factors=50, max_iter=100)\n",
    "mf.fit(train_matrix)\n",
    "\n",
    "# Train Popularity Baseline\n",
    "print(\"Training Popularity Baseline...\")\n",
    "pop_baseline = PopularityBaseline()\n",
    "pop_baseline.fit(train_matrix)\n",
    "\n",
    "# --- Debug: Check one user's recommendations ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEBUGGING RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_user = 0\n",
    "print(f\"Debugging user {test_user}:\")\n",
    "user_interactions = train_matrix[test_user].toarray().flatten()\n",
    "interacted_items = np.where(user_interactions > 0)[0]\n",
    "print(f\"User {test_user} train interactions: {interacted_items}\")\n",
    "\n",
    "# Find test interaction for this user\n",
    "test_item_for_user = None\n",
    "for user_idx, test_item, test_value in test_interactions:\n",
    "    if user_idx == test_user:\n",
    "        test_item_for_user = test_item\n",
    "        break\n",
    "\n",
    "if test_item_for_user is not None:\n",
    "    print(f\"User {test_user} test item: {test_item_for_user}\")\n",
    "    \n",
    "    # Test ItemKNN\n",
    "    itemknn_recs = itemknn.predict(test_user, 10)\n",
    "    print(f\"ItemKNN recommendations: {itemknn_recs}\")\n",
    "    print(f\"ItemKNN count: {len(itemknn_recs)}\")\n",
    "    print(f\"Test item in ItemKNN recs: {test_item_for_user in itemknn_recs}\")\n",
    "    \n",
    "    # Test UserKNN  \n",
    "    userknn_recs = userknn.predict(test_user, 10)\n",
    "    print(f\"UserKNN recommendations: {userknn_recs}\")\n",
    "    print(f\"UserKNN count: {len(userknn_recs)}\")\n",
    "    print(f\"Test item in UserKNN recs: {test_item_for_user in userknn_recs}\")\n",
    "    \n",
    "    # Test MF\n",
    "    mf_recs = mf.predict(test_user, 10)\n",
    "    print(f\"MF recommendations: {mf_recs}\")\n",
    "    print(f\"MF count: {len(mf_recs)}\")\n",
    "    print(f\"Test item in MF recs: {test_item_for_user in mf_recs}\")\n",
    "    \n",
    "    # Test Popularity\n",
    "    pop_recs = pop_baseline.predict(test_user, 10)\n",
    "    print(f\"Popularity recommendations: {pop_recs}\")\n",
    "    print(f\"Popularity count: {len(pop_recs)}\")\n",
    "    print(f\"Test item in Popularity recs: {test_item_for_user in pop_recs}\")\n",
    "else:\n",
    "    print(f\"No test interaction found for user {test_user}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING BASELINES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate with held-out interactions\n",
    "print(\"\\nEvaluating with held-out interactions...\")\n",
    "itemknn_results = evaluate_with_held_out_interactions(itemknn, train_matrix, test_interactions, k_values=K_VALUES)\n",
    "userknn_results = evaluate_with_held_out_interactions(userknn, train_matrix, test_interactions, k_values=K_VALUES)\n",
    "mf_results = evaluate_with_held_out_interactions(mf, train_matrix, test_interactions, k_values=K_VALUES)\n",
    "pop_results = evaluate_with_held_out_interactions(pop_baseline, train_matrix, test_interactions, k_values=K_VALUES)\n",
    "\n",
    "print(\"Held-out Interaction Results:\")\n",
    "print(f\"ItemKNN: {itemknn_results}\")\n",
    "print(f\"UserKNN: {userknn_results}\")\n",
    "print(f\"Matrix Factorization: {mf_results}\")\n",
    "print(f\"Popularity: {pop_results}\")\n",
    "\n",
    "# --- Data Quality Check ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count users and items with interactions\n",
    "n_users_with_interactions = (train_matrix.sum(axis=1) > 0).sum()\n",
    "n_items_with_interactions = (train_matrix.sum(axis=0) > 0).sum()\n",
    "\n",
    "print(f\"Users with interactions: {n_users_with_interactions}/{n_users} ({n_users_with_interactions/n_users:.2%})\")\n",
    "print(f\"Items with interactions: {n_items_with_interactions}/{n_items} ({n_items_with_interactions/n_items:.2%})\")\n",
    "\n",
    "# Check user overlap\n",
    "print(f\"Total interactions: {train_matrix.nnz}\")\n",
    "print(f\"Average interactions per user: {train_matrix.nnz / n_users:.2f}\")\n",
    "print(f\"Average interactions per item: {train_matrix.nnz / n_items:.2f}\")\n",
    "\n",
    "# --- Prepare Data for Two-Tower ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPARING DATA FOR TWO-TOWER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create training data from sequences\n",
    "interactions_for_twotower = []\n",
    "for _, row in seq_train.iterrows():\n",
    "    user_idx = row['user_idx']\n",
    "    pos_item_idx = row['pos_item_idx']\n",
    "    \n",
    "    # Add positive interaction\n",
    "    interactions_for_twotower.append({\n",
    "        'user_idx': user_idx,\n",
    "        'item_idx': pos_item_idx,\n",
    "        'interaction_strength': 1.0\n",
    "    })\n",
    "    \n",
    "    # Add negative samples\n",
    "    if pd.notna(row['history_idx']) and row['history_idx']:\n",
    "        hist_items = [int(x) for x in row['history_idx'].split()]\n",
    "        for hist_item in hist_items:\n",
    "            interactions_for_twotower.append({\n",
    "                'user_idx': user_idx,\n",
    "                'item_idx': hist_item,\n",
    "                'interaction_strength': 0.5\n",
    "            })\n",
    "\n",
    "interactions_for_twotower = pd.DataFrame(interactions_for_twotower)\n",
    "interactions_for_twotower['interaction_strength'] = interactions_for_twotower['quantity'] * interactions_for_twotower['price']\n",
    "\n",
    "# Normalize interaction strength\n",
    "interaction_strength_mean = interactions_for_twotower['interaction_strength'].mean()\n",
    "interaction_strength_std = interactions_for_twotower['interaction_strength'].std()\n",
    "interactions_for_twotower['interaction_strength_norm'] = (\n",
    "    (interactions_for_twotower['interaction_strength'] - interaction_strength_mean) / interaction_strength_std\n",
    ")\n",
    "\n",
    "# Save processed data\n",
    "interactions_for_twotower.to_parquet(OUT_DIR / 'interactions_twotower.parquet', index=False)\n",
    "\n",
    "print(f\"Saved {len(interactions_for_twotower)} interactions for Two-Tower training\")\n",
    "print(f\"Interaction strength - Mean: {interaction_strength_mean:.2f}, Std: {interaction_strength_std:.2f}\")\n",
    "\n",
    "# --- Save Baseline Results ---\n",
    "\n",
    "baseline_results = {\n",
    "    'held_out_interactions': {\n",
    "        'ItemKNN': itemknn_results,\n",
    "        'UserKNN': userknn_results,\n",
    "        'MatrixFactorization': mf_results,\n",
    "        'Popularity': pop_results\n",
    "    },\n",
    "    'data_stats': {\n",
    "        'n_users': n_users,\n",
    "        'n_items': n_items,\n",
    "        'n_interactions': train_matrix.nnz,\n",
    "        'sparsity': 1 - train_matrix.nnz / (n_users * n_items),\n",
    "        'n_test_interactions': len(test_interactions)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / 'baseline_results.json', 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved baseline results to {OUT_DIR / 'baseline_results.json'}\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Dataset: Jarir Retail\")\n",
    "print(f\"Users: {n_users}\")\n",
    "print(f\"Items: {n_items}\")\n",
    "print(f\"Interactions: {train_matrix.nnz}\")\n",
    "print(f\"Sparsity: {1 - train_matrix.nnz / (n_users * n_items):.4f}\")\n",
    "print(f\"Test interactions: {len(test_interactions)}\")\n",
    "\n",
    "print(f\"\\nBest baseline results (Held-out Interactions):\")\n",
    "best_model = max([\n",
    "    ('ItemKNN', itemknn_results[f'Recall@{K_VALUES[-1]}']),\n",
    "    ('UserKNN', userknn_results[f'Recall@{K_VALUES[-1]}']),\n",
    "    ('MatrixFactorization', mf_results[f'Recall@{K_VALUES[-1]}']),\n",
    "    ('Popularity', pop_results[f'Recall@{K_VALUES[-1]}'])\n",
    "], key=lambda x: x[1])\n",
    "print(f\"Best: {best_model[0]} with Recall@{K_VALUES[-1]}: {best_model[1]:.4f}\")\n",
    "\n",
    "print(f\"\\nData ready for Two-Tower training!\")\n",
    "print(f\"Files saved in: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdca01-8db5-4fe6-966f-85bc12c1f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780a61d-450d-4455-8b88-ceb6d4e36060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config & Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data paths\n",
    "OUT_DIR = Path('../data/processed/jarir/')\n",
    "INTERACTIONS_PATH = OUT_DIR / 'interactions_twotower.parquet'\n",
    "CUSTOMER_MAP_PATH = OUT_DIR / 'customer_id_map.parquet'\n",
    "ITEM_MAP_PATH = OUT_DIR / 'item_id_map.parquet'\n",
    "BASELINE_RESULTS_PATH = OUT_DIR / 'baseline_results.json'\n",
    "\n",
    "# Model configuration\n",
    "CFG = {\n",
    "    \"d_model\": 256,  # embedding dim (shared space)\n",
    "    \"batch_size\": 512,  # per-step batch\n",
    "    \"accum_steps\": 2,  # gradient accumulation\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 6,  # early stop on Recall@K\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"eval_topk\": 10,  # Recall@10\n",
    "    \"eval_sample\": None,  # None = full validation\n",
    "    \"seed\": 42,\n",
    "    \"k_neg\": 50,  # negatives per example\n",
    "    \"fixed_logit_scale\": 10.0  # temperature scale for logits\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CFG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(CFG['seed'])\n",
    "np.random.seed(CFG['seed'])\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "# Load sequences\n",
    "seq_train = pd.read_parquet(OUT_DIR / 'sequences_train.parquet', engine=\"fastparquet\")\n",
    "seq_val = pd.read_parquet(OUT_DIR / 'sequences_val.parquet', engine=\"fastparquet\")\n",
    "\n",
    "# Load maps\n",
    "item_map = pd.read_parquet(OUT_DIR / 'item_id_map.parquet', engine=\"fastparquet\")\n",
    "customer_map = pd.read_parquet(OUT_DIR / 'customer_id_map.parquet', engine=\"fastparquet\")\n",
    "\n",
    "print(f\"Sequences: Train={len(seq_train)}, Val={len(seq_val)}\")\n",
    "print(f\"Items: {len(item_map)}, Customers: {len(customer_map)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample interactions:\")\n",
    "print(interactions.head())\n",
    "\n",
    "# Load baseline results\n",
    "with open(BASELINE_RESULTS_PATH, 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "print(\"\\nBaseline results:\")\n",
    "for model, results in baseline_results['held_out_interactions'].items():\n",
    "    print(f\"  {model}: {results}\")\n",
    "\n",
    "# --- Create Train/Validation Split ---\n",
    "print(\"\\nCreating train/validation split...\")\n",
    "\n",
    "# Create train/val split by user_idx (to avoid data leakage)\n",
    "np.random.seed(CFG['seed'])\n",
    "user_ids = interactions['user_idx'].unique()\n",
    "val_users = np.random.choice(user_ids, size=len(user_ids)//5, replace=False)\n",
    "\n",
    "# Use pre-split sequences\n",
    "train_interactions = seq_train.copy()\n",
    "val_interactions = seq_val.copy()\n",
    "\n",
    "print(f\"Train sequences: {len(train_interactions)}\")\n",
    "print(f\"Val sequences: {len(val_interactions)}\")\n",
    "\n",
    "# --- Two-Tower Model Architecture ---\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"Two-Tower model for interaction-based recommendations\"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, d_model=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # User tower\n",
    "        self.user_embedding = nn.Embedding(n_users, d_model)\n",
    "        self.user_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Item tower\n",
    "        self.item_embedding = nn.Embedding(n_items, d_model)\n",
    "        self.item_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.user_embedding.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, mean=0.0, std=0.1)\n",
    "    \n",
    "    def user_vec(self, user_ids):\n",
    "        \"\"\"Get user embeddings\"\"\"\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        user_vec = self.user_mlp(user_emb)\n",
    "        return F.normalize(user_vec, p=2, dim=1)\n",
    "    \n",
    "    def item_vec(self, item_ids):\n",
    "        \"\"\"Get item embeddings\"\"\"\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        item_vec = self.item_mlp(item_emb)\n",
    "        return F.normalize(item_vec, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, user_ids, pos_item_ids, neg_item_ids=None):\n",
    "        \"\"\"Forward pass with optional negative sampling\"\"\"\n",
    "        user_vec = self.user_vec(user_ids)\n",
    "        pos_item_vec = self.item_vec(pos_item_ids)\n",
    "        \n",
    "        # Positive scores\n",
    "        pos_scores = torch.sum(user_vec * pos_item_vec, dim=1) * CFG['fixed_logit_scale']\n",
    "        \n",
    "        if neg_item_ids is not None:\n",
    "            neg_item_vec = self.item_vec(neg_item_ids)\n",
    "            neg_scores = torch.sum(user_vec.unsqueeze(1) * neg_item_vec, dim=2) * CFG['fixed_logit_scale']\n",
    "            return pos_scores, neg_scores\n",
    "        else:\n",
    "            return pos_scores\n",
    "\n",
    "# --- Dataset and DataLoader ---\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, n_items, k_neg=50):\n",
    "        self.sequences = sequences\n",
    "        self.n_items = n_items\n",
    "        self.k_neg = k_neg\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.sequences.iloc[idx]\n",
    "        user_id = row['user_idx']\n",
    "        pos_item_id = row['pos_item_idx']\n",
    "        \n",
    "        # Get history items for hard negative sampling\n",
    "        hist_items = []\n",
    "        if pd.notna(row['history_idx']) and row['history_idx']:\n",
    "            hist_items = [int(x) for x in row['history_idx'].split()]\n",
    "        \n",
    "        # Sample negative items (avoid history items)\n",
    "        available_items = list(set(range(self.n_items)) - set(hist_items) - {pos_item_id})\n",
    "        if len(available_items) >= self.k_neg:\n",
    "            neg_item_ids = np.random.choice(available_items, size=self.k_neg, replace=False)\n",
    "        else:\n",
    "            neg_item_ids = np.random.choice(self.n_items, size=self.k_neg, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'user_id': torch.tensor(user_id, dtype=torch.long),\n",
    "            'pos_item_id': torch.tensor(pos_item_id, dtype=torch.long),\n",
    "            'neg_item_ids': torch.tensor(neg_item_ids, dtype=torch.long),\n",
    "            'history_items': torch.tensor(hist_items, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SequenceDataset(train_interactions, len(item_map), CFG['k_neg'])\n",
    "val_dataset = SequenceDataset(val_interactions, len(item_map), CFG['k_neg'])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CFG['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CFG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# --- Initialize Model ---\n",
    "print(\"\\nInitializing Two-Tower model...\")\n",
    "\n",
    "model = TwoTowerModel(\n",
    "    n_users=len(customer_map),\n",
    "    n_items=len(item_map),\n",
    "    d_model=CFG['d_model'],\n",
    "    dropout=CFG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Compile model for faster training (if available)\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# --- Training Setup ---\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CFG['lr'], \n",
    "    weight_decay=CFG['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max', \n",
    "    factor=0.5, \n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "def in_batch_negative_loss(pos_scores, neg_scores):\n",
    "    \"\"\"In-batch negative loss\"\"\"\n",
    "    # Combine positive and negative scores\n",
    "    all_scores = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    labels = torch.zeros(pos_scores.size(0), dtype=torch.long, device=device)\n",
    "    loss = F.cross_entropy(all_scores, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_recall_at_k(model, val_loader, k=10, n_samples=None):\n",
    "    \"\"\"Evaluate Recall@K on validation sequences\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_recalls = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if n_samples and sample_count >= n_samples:\n",
    "                break\n",
    "                \n",
    "            user_ids = batch['user_id'].to(device)\n",
    "            pos_item_ids = batch['pos_item_id'].to(device)\n",
    "            \n",
    "            # Get user and item embeddings\n",
    "            user_vec = model.user_vec(user_ids)\n",
    "            \n",
    "            # Get all item embeddings\n",
    "            all_item_ids = torch.arange(len(item_map), device=device)\n",
    "            all_item_vec = model.item_vec(all_item_ids)\n",
    "            \n",
    "            # Compute scores for all items\n",
    "            scores = torch.mm(user_vec, all_item_vec.T)\n",
    "            \n",
    "            # Get top-k recommendations\n",
    "            _, top_k_indices = torch.topk(scores, k=k, dim=1)\n",
    "            \n",
    "            # Check if positive items are in top-k\n",
    "            hits = (top_k_indices == pos_item_ids.unsqueeze(1)).any(dim=1)\n",
    "            recalls = hits.float().mean().item()\n",
    "            \n",
    "            all_recalls.append(recalls)\n",
    "            sample_count += len(user_ids)\n",
    "    \n",
    "    return np.mean(all_recalls) if all_recalls else 0.0\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING TWO-TOWER MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_recall = 0.0\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_recalls = []\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        pos_item_ids = batch['pos_item_id'].to(device)\n",
    "        neg_item_ids = batch['neg_item_ids'].to(device)\n",
    "        history_items = batch['history_items'].to(device)  # New field\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_scores, neg_scores = model(user_ids, pos_item_ids, neg_item_ids)\n",
    "        \n",
    "        # Compute loss (same as before)\n",
    "        loss = in_batch_negative_loss(pos_scores, neg_scores)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (batch_count + 1) % CFG['accum_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    # Average loss\n",
    "    avg_loss = epoch_loss / batch_count\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Evaluation\n",
    "    val_recall = evaluate_recall_at_k(\n",
    "        model, \n",
    "        val_loader, \n",
    "        k=CFG['eval_topk'], \n",
    "        n_samples=CFG['eval_sample']\n",
    "    )\n",
    "    val_recalls.append(val_recall)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{CFG['epochs']}: Loss={avg_loss:.4f}, Recall@{CFG['eval_topk']}={val_recall:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_recall)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_recall > best_recall:\n",
    "        best_recall = val_recall\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), OUT_DIR / 'best_twotower_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CFG['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nBest Recall@{CFG['eval_topk']}: {best_recall:.4f}\")\n",
    "\n",
    "# --- Load Best Model and Final Evaluation ---\n",
    "print(\"\\nLoading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(OUT_DIR / 'best_twotower_model.pth'))\n",
    "\n",
    "# Final evaluation\n",
    "final_recall = evaluate_recall_at_k(model, val_loader, k=CFG['eval_topk'])\n",
    "print(f\"Final Recall@{CFG['eval_topk']}: {final_recall:.4f}\")\n",
    "\n",
    "# --- Save Embeddings ---\n",
    "print(\"\\nSaving embeddings...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Save user embeddings\n",
    "    all_user_ids = torch.arange(len(customer_map), device=device)\n",
    "    user_embeddings = model.user_vec(all_user_ids).cpu().numpy()\n",
    "    np.save(OUT_DIR / 'user_embeddings.npy', user_embeddings)\n",
    "    \n",
    "    # Save item embeddings\n",
    "    all_item_ids = torch.arange(len(item_map), device=device)\n",
    "    item_embeddings = model.item_vec(all_item_ids).cpu().numpy()\n",
    "    np.save(OUT_DIR / 'item_embeddings.npy', item_embeddings)\n",
    "\n",
    "print(f\"Saved user embeddings: {user_embeddings.shape}\")\n",
    "print(f\"Saved item embeddings: {item_embeddings.shape}\")\n",
    "\n",
    "# --- Compare with Baselines ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH BASELINES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get best baseline result\n",
    "best_baseline = max([\n",
    "    (model_name, results[f'Recall@{CFG[\"eval_topk\"]}'])\n",
    "    for model_name, results in baseline_results['held_out_interactions'].items()\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"Best baseline ({best_baseline[0]}): Recall@{CFG['eval_topk']} = {best_baseline[1]:.4f}\")\n",
    "print(f\"Two-Tower model: Recall@{CFG['eval_topk']} = {final_recall:.4f}\")\n",
    "\n",
    "improvement = ((final_recall - best_baseline[1]) / best_baseline[1]) * 100 if best_baseline[1] > 0 else 0\n",
    "print(f\"Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "# --- Save Results ---\n",
    "results = {\n",
    "    'model_config': CFG,\n",
    "    'training_results': {\n",
    "        'best_recall': best_recall,\n",
    "        'final_recall': final_recall,\n",
    "        'train_losses': train_losses,\n",
    "        'val_recalls': val_recalls\n",
    "    },\n",
    "    'comparison': {\n",
    "        'best_baseline': {\n",
    "            'model': best_baseline[0],\n",
    "            'recall': best_baseline[1]\n",
    "        },\n",
    "        'two_tower_recall': final_recall,\n",
    "        'improvement_percent': improvement\n",
    "    },\n",
    "    'data_stats': baseline_results['data_stats']\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / 'twotower_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved results to {OUT_DIR / 'twotower_results.json'}\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Dataset: Jarir Retail\")\n",
    "print(f\"Model: Two-Tower (Interaction-based)\")\n",
    "print(f\"Embedding dimension: {CFG['d_model']}\")\n",
    "print(f\"Training epochs: {len(train_losses)}\")\n",
    "print(f\"Best Recall@{CFG['eval_topk']}: {best_recall:.4f}\")\n",
    "print(f\"Final Recall@{CFG['eval_topk']}: {final_recall:.4f}\")\n",
    "print(f\"Improvement over best baseline: {improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - best_twotower_model.pth\")\n",
    "print(f\"  - user_embeddings.npy\")\n",
    "print(f\"  - item_embeddings.npy\")\n",
    "print(f\"  - twotower_results.json\")\n",
    "\n",
    "print(f\"\\nModel ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614a24b-8a74-401e-87fc-7ae2c0ab115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b23cc8-b8a7-421a-97d5-ed3e44fa8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Config & Imports --------------------------------------------------------\n",
    "import os, json, math, time, gc, glob, bisect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Data paths\n",
    "OUT_DIR = Path('../data/processed/jarir/')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "READ_KW  = dict(engine=\"fastparquet\")\n",
    "WRITE_KW = dict(engine=\"fastparquet\", index=False)\n",
    "\n",
    "CFG = {\n",
    "    # Retrieval\n",
    "    \"cand_topk\": 100,\n",
    "    \"cand_batch\": 4096,          # Candidate generation batch size\n",
    "    # Histories & features\n",
    "    \"hist_max\": 15,               # Reduced for Jarir (smaller sequences)\n",
    "    # Feature building\n",
    "    \"feat_batch_q\": 1024,        # queries per GPU batch when building features\n",
    "    \"shard_rows\": 1_000_000,     # approx rows per Parquet shard (features)\n",
    "    \"neg_per_query\": 20,         # keep 1 pos + N hard negatives per query\n",
    "    \"hard_negatives\": True,      # choose hardest by dot_uv; False=random\n",
    "    # Ranker\n",
    "    \"batch_size\": 2048,          # larger thanks to AMP\n",
    "    \"epochs\": 20,\n",
    "    \"patience\": 5,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"hidden\": 256,               # Smaller for Jarir\n",
    "    \"eval_topk\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"use_text\": False,           # No text embeddings for Jarir\n",
    "    # Two-Tower embeddings\n",
    "    \"embedding_dim\": 256,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "# Load sequences\n",
    "seq_train = pd.read_parquet(OUT_DIR/'sequences_train.parquet', **READ_KW)\n",
    "seq_val   = pd.read_parquet(OUT_DIR/'sequences_val.parquet', **READ_KW)\n",
    "seq_test  = pd.read_parquet(OUT_DIR/'sequences_test.parquet', **READ_KW)\n",
    "print(\"Train/Val/Test:\", seq_train.shape, seq_val.shape, seq_test.shape)\n",
    "\n",
    "# Load item and customer maps\n",
    "item_map = pd.read_parquet(OUT_DIR/'item_id_map.parquet', **READ_KW)\n",
    "customer_map = pd.read_parquet(OUT_DIR/'customer_id_map.parquet', **READ_KW)\n",
    "print(\"Items:\", len(item_map), \"Customers:\", len(customer_map))\n",
    "\n",
    "# Popularity for features/fallback\n",
    "pop_counts = seq_train['pos_item_idx'].value_counts()\n",
    "pop_norm = (pop_counts - pop_counts.min()) / (pop_counts.max() - pop_counts.min() + 1e-9)\n",
    "\n",
    "# Load Two-Tower embeddings\n",
    "USER_EMB_PATH = OUT_DIR/'user_embeddings.npy'\n",
    "ITEM_EMB_PATH = OUT_DIR/'item_embeddings.npy'\n",
    "\n",
    "if USER_EMB_PATH.exists() and ITEM_EMB_PATH.exists():\n",
    "    USER_EMB = np.load(USER_EMB_PATH).astype('float32')\n",
    "    ITEM_EMB = np.load(ITEM_EMB_PATH).astype('float32')\n",
    "    print(\"Two-Tower embeddings loaded:\", USER_EMB.shape, ITEM_EMB.shape)\n",
    "else:\n",
    "    print(\"❌ Missing Two-Tower embeddings. Please run notebook 03 first.\")\n",
    "    # Create dummy embeddings for testing\n",
    "    n_users = len(customer_map)\n",
    "    n_items = len(item_map)\n",
    "    USER_EMB = np.random.randn(n_users, CFG[\"embedding_dim\"]).astype('float32')\n",
    "    ITEM_EMB = np.random.randn(n_items, CFG[\"embedding_dim\"]).astype('float32')\n",
    "    # Normalize\n",
    "    USER_EMB /= np.linalg.norm(USER_EMB, axis=1, keepdims=True)\n",
    "    ITEM_EMB /= np.linalg.norm(ITEM_EMB, axis=1, keepdims=True)\n",
    "    print(\"Using dummy embeddings for testing\")\n",
    "\n",
    "# GPU tensors for embeddings\n",
    "USER_EMB_T = torch.from_numpy(USER_EMB).to(device, non_blocking=True)\n",
    "ITEM_EMB_T = torch.from_numpy(ITEM_EMB).to(device, non_blocking=True)\n",
    "\n",
    "# Popularity tensor\n",
    "pop_vec = torch.zeros(len(item_map), dtype=torch.float32, device=device)\n",
    "pop_idx = torch.tensor(pop_counts.index.values, dtype=torch.long, device=device)\n",
    "pop_val = torch.tensor(pop_norm.loc[pop_counts.index].values, dtype=torch.float32, device=device)\n",
    "pop_vec[pop_idx] = pop_val\n",
    "\n",
    "# Load item metadata for additional features\n",
    "items_clean_path = OUT_DIR/'items_clean.parquet'\n",
    "price_z = None\n",
    "if items_clean_path.exists():\n",
    "    items_clean = pd.read_parquet(items_clean_path, **READ_KW)\n",
    "    if 'price_median' in items_clean.columns:\n",
    "        m = items_clean[['stock_code','price_median']].dropna()\n",
    "        # Map stock_code to item_idx\n",
    "        m = m.merge(item_map, on='stock_code', how='inner')\n",
    "        if len(m) > 0:\n",
    "            mu, sigma = m['price_median'].mean(), m['price_median'].std() + 1e-6\n",
    "            z = ((m['price_median'] - mu) / sigma).astype(float)\n",
    "            price_z = torch.zeros(len(item_map), dtype=torch.float32, device=device)\n",
    "            ii = torch.tensor(m['item_idx'].astype(int).values, dtype=torch.long, device=device)\n",
    "            price_z[ii] = torch.tensor(z.values, dtype=torch.float32, device=device)\n",
    "            print(\"Price features loaded\")\n",
    "\n",
    "def parse_hist(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    return [int(x) for x in s.strip().split()]\n",
    "\n",
    "# --- 1) Candidate Generation using Two-Tower Embeddings ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def user_vecs_from_hist_batch(hist_tensor):\n",
    "    \"\"\"Compute user vectors from history using Two-Tower item embeddings\"\"\"\n",
    "    B, L = hist_tensor.shape\n",
    "    safe_idx = hist_tensor.clamp(min=0)  # replace -1 with 0 for gather\n",
    "    H = ITEM_EMB_T.index_select(0, safe_idx.view(-1)).view(B, L, -1)  # [B,L,d]\n",
    "    mask = (hist_tensor >= 0).float().unsqueeze(-1)                    # [B,L,1]\n",
    "    U = (H * mask).sum(1) / mask.sum(1).clamp_min(1e-6)                # [B,d]\n",
    "    U = F.normalize(U, dim=-1)\n",
    "    return U\n",
    "\n",
    "def build_hist_tensor(series, L):\n",
    "    \"\"\"Build history tensor from string series\"\"\"\n",
    "    B = len(series)\n",
    "    H = torch.full((B, L), -1, dtype=torch.long)\n",
    "    for i, s in enumerate(series):\n",
    "        h = parse_hist(s)\n",
    "        if len(h) > L: h = h[-L:]\n",
    "        if h:\n",
    "            H[i, -len(h):] = torch.tensor(h, dtype=torch.long)\n",
    "    return H\n",
    "\n",
    "def gen_candidates_gpu(df, topk=100, batch_q=4096):\n",
    "    \"\"\"Generate candidates using Two-Tower embeddings\"\"\"\n",
    "    hist_series = df['history_idx'].astype(str).tolist()\n",
    "    H = build_hist_tensor(hist_series, CFG[\"hist_max\"])  # CPU\n",
    "    U_chunks = []\n",
    "    for i in range(0, H.size(0), batch_q):\n",
    "        Ub = user_vecs_from_hist_batch(H[i:i+batch_q].to(device))\n",
    "        U_chunks.append(Ub.detach().cpu())\n",
    "    U = torch.cat(U_chunks, 0).numpy().astype('float32')\n",
    "\n",
    "    # Compute similarities with all items\n",
    "    with torch.no_grad():\n",
    "        U_tensor = torch.from_numpy(U).to(device)\n",
    "        sims = U_tensor @ ITEM_EMB_T.t()\n",
    "        _, top_k_indices = torch.topk(sims, k=topk, dim=1)\n",
    "        I_full = top_k_indices.cpu().numpy().astype('int32')\n",
    "\n",
    "    pos_list = df['pos_item_idx'].astype(int).tolist()\n",
    "    ts_list  = df['ts'].astype(str).tolist() if 'ts' in df.columns else ['']*len(pos_list)\n",
    "    rows = []\n",
    "    for pos, cand_idx, ts, h_s in zip(pos_list, I_full, ts_list, hist_series):\n",
    "        cand = cand_idx.tolist()\n",
    "        if pos not in cand:\n",
    "            cand[-1] = int(pos)\n",
    "        rows.append((h_s, int(pos), \" \".join(map(str,cand)), ts))\n",
    "    return pd.DataFrame(rows, columns=['history_idx','pos_item_idx','cands','ts'])\n",
    "\n",
    "# Build & save candidates if missing\n",
    "CAND_TRAIN_PATH = OUT_DIR/'candidates_train.parquet'\n",
    "CAND_VAL_PATH   = OUT_DIR/'candidates_val.parquet'\n",
    "CAND_TEST_PATH  = OUT_DIR/'candidates_test.parquet'\n",
    "\n",
    "if not (CAND_TRAIN_PATH.exists() and CAND_VAL_PATH.exists() and CAND_TEST_PATH.exists()):\n",
    "    print(\"Generating training candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_train, topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TRAIN_PATH, **WRITE_KW)\n",
    "    print(\"Generating validation candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_val,   topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_VAL_PATH, **WRITE_KW)\n",
    "    print(\"Generating test candidates (GPU)...\")\n",
    "    gen_candidates_gpu(seq_test,  topk=CFG[\"cand_topk\"], batch_q=CFG[\"cand_batch\"]).to_parquet(CAND_TEST_PATH, **WRITE_KW)\n",
    "\n",
    "cand_train = pd.read_parquet(CAND_TRAIN_PATH, **READ_KW)\n",
    "cand_val   = pd.read_parquet(CAND_VAL_PATH, **READ_KW)\n",
    "cand_test  = pd.read_parquet(CAND_TEST_PATH, **READ_KW)\n",
    "print(\"Candidates:\", cand_train.shape, cand_val.shape, cand_test.shape)\n",
    "\n",
    "# --- 2) Feature Engineering on GPU (sharded) ---\n",
    "\n",
    "def _pack_batch_features(Ub, Hb, Cb, Pb):\n",
    "    \"\"\"Pack features for a batch of candidates\"\"\"\n",
    "    B, d = Ub.shape\n",
    "    K = Cb.size(1)\n",
    "    L = Hb.size(1)\n",
    "    \n",
    "    # Candidate item vectors\n",
    "    Vc = ITEM_EMB_T.index_select(0, Cb.view(-1)).view(B, K, d)\n",
    "    \n",
    "    # dot(u,v) - similarity between user and candidate\n",
    "    dot_uv = (Ub.unsqueeze(1) * Vc).sum(-1)                        # [B,K]\n",
    "    \n",
    "    # Max sim to recent history\n",
    "    safe_hist = Hb.clamp(min=0)\n",
    "    Hvec = ITEM_EMB_T.index_select(0, safe_hist.view(-1)).view(B, L, d)\n",
    "    Hvec = F.normalize(Hvec, dim=-1); Vc_n = F.normalize(Vc, dim=-1)\n",
    "    sims = torch.matmul(Hvec, Vc_n.transpose(1,2))                  # [B,L,K]\n",
    "    maskL = (Hb >= 0).unsqueeze(-1).float()\n",
    "    sims = sims + (maskL - 1.0) * 1e9\n",
    "    max_sim_recent = sims.max(dim=1).values                         # [B,K]\n",
    "    \n",
    "    # popularity & history length\n",
    "    pop = pop_vec.index_select(0, Cb.view(-1)).view(B, K)\n",
    "    hlen = (Hb >= 0).float().sum(1) / float(CFG[\"hist_max\"])\n",
    "    hlen = hlen.unsqueeze(1).expand(B, K)\n",
    "    \n",
    "    # price_z (if available)\n",
    "    if isinstance(price_z, torch.Tensor):\n",
    "        price = price_z.index_select(0, Cb.view(-1)).view(B, K)\n",
    "    else:\n",
    "        price = torch.zeros((B, K), device=Ub.device)\n",
    "    \n",
    "    # labels\n",
    "    labels = (Cb == Pb.view(-1,1)).float()\n",
    "    \n",
    "    return {\"dot_uv\": dot_uv, \"max_sim_recent\": max_sim_recent, \"pop\": pop,\n",
    "            \"hist_len\": hlen, \"price_z\": price, \"label\": labels, \"item_idx\": Cb.float()}\n",
    "\n",
    "def _select_negatives(feats):\n",
    "    \"\"\"Keep 1 positive + N negatives per query if configured\"\"\"\n",
    "    if CFG[\"neg_per_query\"] is None:\n",
    "        return feats\n",
    "    B, K = feats[\"label\"].shape\n",
    "    pos_col = torch.argmax(feats[\"label\"], dim=1, keepdim=True)     # [B,1]\n",
    "    if CFG[\"hard_negatives\"]:\n",
    "        neg_scores = feats[\"dot_uv\"].clone()\n",
    "        neg_scores.scatter_(1, pos_col, -1e9)\n",
    "        _, neg_idx = torch.topk(neg_scores, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    else:\n",
    "        rnd = torch.rand_like(feats[\"dot_uv\"])\n",
    "        rnd.scatter_(1, pos_col, 1e9)\n",
    "        _, neg_idx = torch.topk(-rnd, k=min(CFG[\"neg_per_query\"], K-1), dim=1)\n",
    "    keep_cols = torch.cat([pos_col, neg_idx], dim=1)                # [B,1+N]\n",
    "    for k in [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\",\"label\",\"item_idx\"]:\n",
    "        feats[k] = torch.gather(feats[k], 1, keep_cols)\n",
    "    return feats\n",
    "\n",
    "def build_feats_gpu_sharded(cand_df, split_name):\n",
    "    \"\"\"Build features on GPU and save to sharded Parquet files\"\"\"\n",
    "    N = len(cand_df)\n",
    "    L = CFG[\"hist_max\"]\n",
    "    bq = CFG[\"feat_batch_q\"]\n",
    "    out_dir = OUT_DIR / f\"ranker_feats_{split_name}_shards\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def write_shard(idx, feats_dict):\n",
    "        cpu = {k: v.detach().float().view(-1).to('cpu').numpy() for k,v in feats_dict.items()}\n",
    "        df = pd.DataFrame(cpu); df['item_idx'] = df['item_idx'].astype(np.int32)\n",
    "        df.to_parquet(out_dir / f\"part_{idx:03d}.parquet\", **WRITE_KW)\n",
    "\n",
    "    hist_series = cand_df['history_idx'].astype(str).tolist()\n",
    "    pos_list = cand_df['pos_item_idx'].astype(int).tolist()\n",
    "    cands_series = cand_df['cands'].astype(str).tolist()\n",
    "\n",
    "    rows_written = 0; shard_idx = 0; buf = None\n",
    "    for i in range(0, N, bq):\n",
    "        H = build_hist_tensor(hist_series[i:i+bq], L).to(device, non_blocking=True)\n",
    "        P = torch.tensor(pos_list[i:i+bq], dtype=torch.long, device=device)\n",
    "        C = torch.tensor([[int(x) for x in s.split()] for s in cands_series[i:i+bq]],\n",
    "                         dtype=torch.long, device=device)\n",
    "        U = user_vecs_from_hist_batch(H)\n",
    "        feats = _pack_batch_features(U, H, C, P)\n",
    "        feats = _select_negatives(feats)\n",
    "\n",
    "        if buf is None:\n",
    "            buf = {k: v.detach().clone() for k,v in feats.items()}\n",
    "        else:\n",
    "            for k in buf.keys():\n",
    "                buf[k] = torch.cat([buf[k], feats[k]], dim=0)\n",
    "\n",
    "        rows_in_buf = int(buf[\"label\"].numel())\n",
    "        if rows_in_buf >= CFG[\"shard_rows\"]:\n",
    "            write_shard(shard_idx, buf); shard_idx += 1\n",
    "            for k in list(buf.keys()): del buf[k]\n",
    "            buf = None; torch.cuda.empty_cache()\n",
    "\n",
    "        rows_written += int(feats[\"label\"].numel())\n",
    "        if (i//bq) % 10 == 0:\n",
    "            print(f\"[{split_name}] Built ~{rows_written/1e6:.2f}M rows...\")\n",
    "\n",
    "    if buf is not None:\n",
    "        write_shard(shard_idx, buf); shard_idx += 1\n",
    "        for k in list(buf.keys()): del buf[k]\n",
    "        buf = None; torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"[{split_name}] Done. Rows ~{rows_written:,}. Shards -> {out_dir}\")\n",
    "    return out_dir\n",
    "\n",
    "# Build shards if missing\n",
    "train_shards_dir = OUT_DIR / \"ranker_feats_train_shards\"\n",
    "val_shards_dir   = OUT_DIR / \"ranker_feats_val_shards\"\n",
    "test_shards_dir  = OUT_DIR / \"ranker_feats_test_shards\"\n",
    "\n",
    "if not train_shards_dir.exists():\n",
    "    train_shards_dir = build_feats_gpu_sharded(cand_train, \"train\")\n",
    "if not val_shards_dir.exists():\n",
    "    val_shards_dir = build_feats_gpu_sharded(cand_val, \"val\")\n",
    "if not test_shards_dir.exists():\n",
    "    test_shards_dir = build_feats_gpu_sharded(cand_test, \"test\")\n",
    "\n",
    "print(\"Shard dirs:\", train_shards_dir, val_shards_dir, test_shards_dir)\n",
    "\n",
    "# --- 3) Ranker Model ---\n",
    "\n",
    "RANKER_COLS = [\"dot_uv\",\"max_sim_recent\",\"pop\",\"hist_len\",\"price_z\"]\n",
    "\n",
    "def shard_batches(files, batch_size):\n",
    "    \"\"\"Generate batches from sharded Parquet files\"\"\"\n",
    "    for f in sorted(glob.glob(str(Path(files)/\"part_*.parquet\")) if isinstance(files, (str, Path)) else files):\n",
    "        df = pd.read_parquet(f, engine=\"fastparquet\", columns=RANKER_COLS+[\"label\"])\n",
    "\n",
    "        # Build tensors without non_blocking, then .to(device, non_blocking=True)\n",
    "        X_np = df[RANKER_COLS].to_numpy(dtype='float32', copy=False)\n",
    "        y_np = df[\"label\"].to_numpy(dtype='float32', copy=False)\n",
    "        X = torch.from_numpy(X_np).to(device, non_blocking=True)\n",
    "        y = torch.from_numpy(y_np).to(device, non_blocking=True)\n",
    "\n",
    "        perm = torch.randperm(X.size(0), device=device)\n",
    "        for i in range(0, X.size(0), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            yield X[idx], y[idx]\n",
    "\n",
    "        del X, y, df\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class RankerMLP(nn.Module):\n",
    "    \"\"\"MLP ranker for candidate re-ranking\"\"\"\n",
    "    def __init__(self, d_in, hidden=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "model = RankerMLP(len(RANKER_COLS), hidden=CFG[\"hidden\"], dropout=CFG[\"dropout\"]).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "use_amp = (device.type == \"cuda\")\n",
    "amp_device = \"cuda\" if use_amp else \"cpu\"\n",
    "\n",
    "# GradScaler for mixed precision\n",
    "scaler = torch.amp.GradScaler(amp_device, enabled=use_amp)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- 4) Training & Evaluation ---\n",
    "\n",
    "def recall_at_k(preds, truth): \n",
    "    return float(truth in preds)\n",
    "\n",
    "def ndcg_at_k(preds, truth):\n",
    "    try:\n",
    "        r = preds.index(truth) + 1; return 1.0 / math.log2(r + 1.0)\n",
    "    except ValueError: \n",
    "        return 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_one_batch(cand_df_slice):\n",
    "    \"\"\"Re-rank candidates for one batch\"\"\"\n",
    "    H = build_hist_tensor(cand_df_slice['history_idx'].astype(str).tolist(), CFG[\"hist_max\"]).to(device)\n",
    "    P = torch.tensor(cand_df_slice['pos_item_idx'].astype(int).tolist(), dtype=torch.long, device=device)\n",
    "    C = torch.tensor([[int(x) for x in s.split()] for s in cand_df_slice['cands'].astype(str).tolist()],\n",
    "                     dtype=torch.long, device=device)\n",
    "    U = user_vecs_from_hist_batch(H)\n",
    "    feats = _pack_batch_features(U, H, C, P)\n",
    "    # No negative subsampling at inference: rank all provided candidates\n",
    "    X = torch.stack([feats[c] for c in RANKER_COLS], dim=-1) # [B,K,5]\n",
    "    scores = model(X.view(-1, len(RANKER_COLS))).view(X.size(0), X.size(1))\n",
    "    topk = min(CFG[\"eval_topk\"], C.size(1))\n",
    "    vals, idx = torch.topk(scores, k=topk, dim=1)\n",
    "    reranked = [C[i][idx[i]].tolist() for i in range(C.size(0))]\n",
    "    return reranked\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reranked(cand_df, split=\"val\"):\n",
    "    \"\"\"Evaluate re-ranked candidates\"\"\"\n",
    "    model.eval()\n",
    "    hits = 0; ndcgs = 0.0; tot = 0\n",
    "    B = 1024\n",
    "    for i in range(0, len(cand_df), B):\n",
    "        batch_df = cand_df.iloc[i:i+B]\n",
    "        reranked = rerank_one_batch(batch_df)\n",
    "        for pos, rr in zip(batch_df['pos_item_idx'].tolist(), reranked):\n",
    "            pos = int(pos)\n",
    "            hits += float(pos in rr)\n",
    "            if pos in rr:\n",
    "                r = rr.index(pos) + 1\n",
    "                ndcgs += 1.0 / math.log2(r + 1.0)\n",
    "            tot += 1\n",
    "    return hits/max(1,tot), ndcgs/max(1,tot)\n",
    "\n",
    "def train_ranker():\n",
    "    \"\"\"Train the ranker model\"\"\"\n",
    "    best_recall = -1.0; bad = 0\n",
    "    for ep in range(1, CFG[\"epochs\"]+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0; nobs = 0\n",
    "        for Xb, yb in shard_batches(train_shards_dir, CFG[\"batch_size\"]):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(amp_device, enabled=use_amp, dtype=torch.float16):\n",
    "                logits = model(Xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            total_loss += float(loss.item()) * yb.numel()\n",
    "            nobs += yb.numel()\n",
    "\n",
    "        # Evaluate every epoch\n",
    "        val_recall, val_ndcg = eval_reranked(cand_val, split=\"val\")\n",
    "        print(f\"Epoch {ep:02d} | train BCE {total_loss/max(1,nobs):.4f} | \"\n",
    "              f\"val Recall@{CFG['eval_topk']} {val_recall:.4f} | val NDCG@{CFG['eval_topk']} {val_ndcg:.4f}\")\n",
    "        if val_recall > best_recall + 1e-4:\n",
    "            best_recall, bad = val_recall, 0\n",
    "            torch.save(model.state_dict(), OUT_DIR/'ranker_best.pt')\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= CFG[\"patience\"]:\n",
    "                print(\"Early stopping on Recall@10.\"); break\n",
    "    print(\"Best val Recall@{} = {:.4f}\".format(CFG[\"eval_topk\"], best_recall))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING RANKER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_ranker()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(OUT_DIR/'ranker_best.pt', map_location=device, weights_only=True))\n",
    "test_recall, test_ndcg = eval_reranked(cand_test, split=\"test\")\n",
    "print(\"TEST — Recall@{}: {:.4f}, NDCG@{}: {:.4f}\".format(CFG[\"eval_topk\"], test_recall, CFG[\"eval_topk\"], test_ndcg))\n",
    "\n",
    "# --- 5) Comparison with Baselines ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH BASELINES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load baseline results if available\n",
    "baseline_results_path = OUT_DIR / 'baseline_results.json'\n",
    "if baseline_results_path.exists():\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    \n",
    "    print(\"Baseline Results (Held-out Interactions):\")\n",
    "    for model_name, results in baseline_results['held_out_interactions'].items():\n",
    "        print(f\"  {model_name}: {results}\")\n",
    "    \n",
    "    # Get best baseline\n",
    "    best_baseline = max([\n",
    "        (model_name, results[f'Recall@{CFG[\"eval_topk\"]}'])\n",
    "        for model_name, results in baseline_results['held_out_interactions'].items()\n",
    "    ], key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nBest baseline ({best_baseline[0]}): Recall@{CFG['eval_topk']} = {best_baseline[1]:.4f}\")\n",
    "    print(f\"Two-Stage Ranker: Recall@{CFG['eval_topk']} = {test_recall:.4f}\")\n",
    "    \n",
    "    improvement = ((test_recall - best_baseline[1]) / best_baseline[1]) * 100 if best_baseline[1] > 0 else 0\n",
    "    print(f\"Improvement: {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(\"No baseline results found for comparison\")\n",
    "\n",
    "# --- 6) Save Results ---\n",
    "\n",
    "results = {\n",
    "    'model_config': CFG,\n",
    "    'evaluation_results': {\n",
    "        'test_recall': test_recall,\n",
    "        'test_ndcg': test_ndcg,\n",
    "        'eval_topk': CFG['eval_topk']\n",
    "    },\n",
    "    'data_stats': {\n",
    "        'n_users': len(customer_map),\n",
    "        'n_items': len(item_map),\n",
    "        'train_sequences': len(seq_train),\n",
    "        'val_sequences': len(seq_val),\n",
    "        'test_sequences': len(seq_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / 'ranker_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved results to {OUT_DIR / 'ranker_results.json'}\")\n",
    "\n",
    "# --- 7) Final Summary ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Dataset: Jarir Retail\")\n",
    "print(f\"Model: Two-Stage (Two-Tower + MLP Ranker)\")\n",
    "print(f\"Embedding dimension: {CFG['embedding_dim']}\")\n",
    "print(f\"Ranker hidden size: {CFG['hidden']}\")\n",
    "print(f\"Test Recall@{CFG['eval_topk']}: {test_recall:.4f}\")\n",
    "print(f\"Test NDCG@{CFG['eval_topk']}: {test_ndcg:.4f}\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - ranker_best.pt\")\n",
    "print(f\"  - ranker_results.json\")\n",
    "\n",
    "print(f\"\\nModel ready for inference!\")\n",
    "\n",
    "# --- 8) Optional: Feature Importance Analysis ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze feature importance by computing gradients\n",
    "model.eval()\n",
    "feature_importance = torch.zeros(len(RANKER_COLS), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample a batch from validation\n",
    "    for Xb, yb in shard_batches(val_shards_dir, 1024):\n",
    "        Xb.requires_grad_(True)\n",
    "        logits = model(Xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradient magnitudes\n",
    "        feature_importance += Xb.grad.abs().mean(dim=0)\n",
    "        break\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_names = RANKER_COLS\n",
    "\n",
    "print(\"Feature Importance (gradient magnitude):\")\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"  {name}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_names, feature_importance)\n",
    "plt.title('Feature Importance in Ranker')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
