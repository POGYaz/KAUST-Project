{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12570662,"sourceType":"datasetVersion","datasetId":7938464}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Full Class","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport logging\nimport datetime\nimport zoneinfo\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s — %(levelname)s — %(message)s\",\n    handlers=[\n        logging.FileHandler(f\"etl_pipeline_{datetime.datetime.now(tz=zoneinfo.ZoneInfo('Asia/Riyadh')).strftime('%Y%m%d_%H%M%S')}.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\nclass ETLPipeline:\n    def __init__(self, customers_path, products_catalog_path, transactions_log_path, output_path):\n        self.customers_path = customers_path\n        self.products_catalog_path = products_catalog_path\n        self.transactions_log_path = transactions_log_path\n        self.output_path = output_path\n\n    def run_pipeline(self, threshold=2):\n        \"\"\"Handles full ETL: logging, loading, type casting, merging, saving\"\"\"\n        \n        # Load CSVs\n        customers = pd.read_csv(self.customers_path)\n        products = pd.read_csv(self.products_catalog_path)\n        transactions = pd.read_csv(self.transactions_log_path)\n        logger.info(\"Loaded dataframes from CSVs\")\n\n        logger.info(f\"* Before Merging *\\ncustomers: {customers.shape}\\nproducts: {products.shape}\\ntransactions: {transactions.shape}\")\n\n        if 'Date' in transactions.columns and 'Timestamp' not in transactions.columns:\n            transactions.rename(columns={'Date': 'Timestamp'}, inplace=True)\n\n        # Lightweight type casting\n        customers['CustomerID'] = customers['CustomerID'].astype(str)\n        customers['Business_Category'] = customers['Business_Category'].astype(str)\n        customers['Business_Size'] = customers['Business_Size'].astype(str)\n        customers['Customer_Since'] = pd.to_datetime(customers['Customer_Since'])\n\n        products['SKU'] = products['SKU'].astype(str)\n        products['Rev_GL_Class'] = products['Rev_GL_Class'].astype(str)\n        products['Sub_Category'] = products['Sub_Category'].astype(str)\n        products['Item_Description'] = products['Item_Description'].astype(str)\n        products['Brand'] = products['Brand'].astype(str)\n        products['Unit_Price'] = pd.to_numeric(products['Unit_Price'], errors='coerce')\n        products['Attributes'] = products['Attributes'].astype(str)\n\n        transactions['TransactionID'] = transactions['TransactionID'].astype(str)\n        transactions['CustomerID'] = transactions['CustomerID'].astype(str)\n        transactions['Timestamp'] = pd.to_datetime(transactions['Timestamp'])\n        transactions['SKU'] = transactions['SKU'].astype(str)\n        transactions['Quantity'] = pd.to_numeric(transactions['Quantity'], downcast='integer', errors='coerce')\n\n        logger.info(\"Lightweight type casting completed\")\n\n        # Check and handle missing CustomerID keys\n        valid_cust_mask = transactions['CustomerID'].isin(customers['CustomerID'])\n        invalid_cust_count = (~valid_cust_mask).sum()\n        invalid_cust_percent = invalid_cust_count / len(transactions) * 100\n        if invalid_cust_count > 0:\n            if invalid_cust_percent < threshold:\n                logger.warning(f\"Dropping {invalid_cust_count} rows with invalid CustomerIDs (< {threshold}%)\")\n                transactions = transactions[valid_cust_mask]\n            else:\n                logger.warning(f\"Too many invalid CustomerID rows ({invalid_cust_percent:.2f}%) — no rows dropped\")\n        else:\n            logger.info(\"All CustomerIDs are valid.\")\n\n        # Check and handle missing SKU keys\n        valid_sku_mask = transactions['SKU'].isin(products['SKU'])\n        invalid_sku_count = (~valid_sku_mask).sum()\n        invalid_sku_percent = invalid_sku_count / len(transactions) * 100\n        if invalid_sku_count > 0:\n            if invalid_sku_percent < threshold:\n                logger.warning(f\"Dropping {invalid_sku_count} rows with invalid SKUs (< {threshold}%)\")\n                transactions = transactions[valid_sku_mask]\n            else:\n                logger.warning(f\"Too many invalid SKU rows ({invalid_sku_percent:.2f}%) — no rows dropped\")\n        else:\n            logger.info(\"All SKUs are valid.\")\n\n        # Merging\n        merged_df = pd.merge(transactions, customers, on='CustomerID', how='left')\n        merged_df = pd.merge(merged_df, products, on='SKU', how='left')\n        merged_df = merged_df.sort_values(by=['CustomerID', 'Timestamp', 'SKU'])\n\n        logger.info(f\"* After Merging *\\nmerged shape: {merged_df.shape}\")\n\n        # Saving\n        mem_MB = merged_df.memory_usage(deep=True).sum() / 1_048_576\n        logger.info(f\"Estimated in-memory size: {mem_MB:.2f} MB\")\n\n        if mem_MB < 1000:\n            merged_df.to_csv(self.output_path + \".csv\", index=False)\n            logger.info(\"Saved dataset as CSV\")\n        else:\n            merged_df.to_parquet(self.output_path + \".parquet\", index=False)\n            logger.info(\"Saved dataset as Parquet\")\n\n\n        for handler in logger.handlers:\n            handler.flush()\n\n        \n        return merged_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T20:30:26.272174Z","iopub.execute_input":"2025-07-25T20:30:26.272788Z","iopub.status.idle":"2025-07-25T20:30:26.289879Z","shell.execute_reply.started":"2025-07-25T20:30:26.272756Z","shell.execute_reply":"2025-07-25T20:30:26.289090Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"customers = os.path.join(\"/kaggle\", \"input\", \"etl-csvs\", \"customers.csv\")\nproducts_catalog = os.path.join(\"/kaggle\", \"input\", \"etl-csvs\", \"products_catalog.csv\")\ntransactions_log = os.path.join(\"/kaggle\", \"input\", \"etl-csvs\", \"transactions_log.csv\")\noutput_path = os.path.join(\"/kaggle\", \"working\", \"merged_transactions\")\n\netl = ETLPipeline(customers, products_catalog, transactions_log, output_path)\nmerged_df = etl.run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T20:30:26.291169Z","iopub.execute_input":"2025-07-25T20:30:26.291394Z","iopub.status.idle":"2025-07-25T20:30:27.100558Z","shell.execute_reply.started":"2025-07-25T20:30:26.291375Z","shell.execute_reply":"2025-07-25T20:30:27.099669Z"}},"outputs":[],"execution_count":10}]}